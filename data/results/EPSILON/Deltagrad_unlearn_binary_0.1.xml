<data>
<generate>
</generate>
<training>
Train - Epoch 0, Batch: 0, Loss: 0.692563
Train - Epoch 0, Batch: 1, Loss: 0.692770
Train - Epoch 0, Batch: 2, Loss: 0.691673
Train - Epoch 0, Batch: 3, Loss: 0.692328
Train - Epoch 0, Batch: 4, Loss: 0.692554
Train - Epoch 0, Batch: 5, Loss: 0.691279
Train - Epoch 0, Batch: 6, Loss: 0.691022
Train - Epoch 0, Batch: 7, Loss: 0.691795
Train - Epoch 0, Batch: 8, Loss: 0.691016
Train - Epoch 0, Batch: 9, Loss: 0.691368
Train - Epoch 0, Batch: 10, Loss: 0.691868
Train - Epoch 0, Batch: 11, Loss: 0.689168
Train - Epoch 0, Batch: 12, Loss: 0.693172
Train - Epoch 0, Batch: 13, Loss: 0.689493
Train - Epoch 0, Batch: 14, Loss: 0.689119
Train - Epoch 0, Batch: 15, Loss: 0.691696
Train - Epoch 0, Batch: 16, Loss: 0.689283
Train - Epoch 0, Batch: 17, Loss: 0.689373
Train - Epoch 0, Batch: 18, Loss: 0.689483
Train - Epoch 0, Batch: 19, Loss: 0.690704
Train - Epoch 0, Batch: 20, Loss: 0.689203
Train - Epoch 0, Batch: 21, Loss: 0.688093
Train - Epoch 0, Batch: 22, Loss: 0.690528
Train - Epoch 0, Batch: 23, Loss: 0.688982
Train - Epoch 0, Batch: 24, Loss: 0.687941
Train - Epoch 0, Batch: 25, Loss: 0.689674
Train - Epoch 0, Batch: 26, Loss: 0.687593
Train - Epoch 0, Batch: 27, Loss: 0.687331
Train - Epoch 0, Batch: 28, Loss: 0.687374
Train - Epoch 0, Batch: 29, Loss: 0.687707
Train - Epoch 0, Batch: 30, Loss: 0.686813
Train - Epoch 0, Batch: 31, Loss: 0.688130
Train - Epoch 0, Batch: 32, Loss: 0.686673
Train - Epoch 0, Batch: 33, Loss: 0.686001
Train - Epoch 0, Batch: 34, Loss: 0.684590
Train - Epoch 0, Batch: 35, Loss: 0.687582
Train - Epoch 0, Batch: 36, Loss: 0.686273
Train - Epoch 0, Batch: 37, Loss: 0.685520
Train - Epoch 0, Batch: 38, Loss: 0.685995
Train - Epoch 0, Batch: 39, Loss: 0.684392
Train - Epoch 0, Batch: 40, Loss: 0.685223
Train - Epoch 0, Batch: 41, Loss: 0.682832
Train - Epoch 0, Batch: 42, Loss: 0.685600
Train - Epoch 0, Batch: 43, Loss: 0.681866
Train - Epoch 0, Batch: 44, Loss: 0.681955
Train - Epoch 0, Batch: 45, Loss: 0.683341
Train - Epoch 0, Batch: 46, Loss: 0.688231
Train - Epoch 0, Batch: 47, Loss: 0.683333
Train - Epoch 0, Batch: 48, Loss: 0.683324
Train - Epoch 0, Batch: 49, Loss: 0.684806
Train - Epoch 0, Batch: 50, Loss: 0.683859
Train - Epoch 0, Batch: 51, Loss: 0.683575
Train - Epoch 0, Batch: 52, Loss: 0.682712
Train - Epoch 0, Batch: 53, Loss: 0.684193
Train - Epoch 0, Batch: 54, Loss: 0.682767
Train - Epoch 0, Batch: 55, Loss: 0.683735
Train - Epoch 0, Batch: 56, Loss: 0.682919
Train - Epoch 0, Batch: 57, Loss: 0.682438
Train - Epoch 0, Batch: 58, Loss: 0.679311
Train - Epoch 0, Batch: 59, Loss: 0.681953
Train - Epoch 0, Batch: 60, Loss: 0.681057
Train - Epoch 0, Batch: 61, Loss: 0.677960
Train - Epoch 0, Batch: 62, Loss: 0.684233
Train - Epoch 0, Batch: 63, Loss: 0.680707
Train - Epoch 0, Batch: 64, Loss: 0.682212
Train - Epoch 0, Batch: 65, Loss: 0.681537
Train - Epoch 0, Batch: 66, Loss: 0.679776
Train - Epoch 0, Batch: 67, Loss: 0.680816
Train - Epoch 0, Batch: 68, Loss: 0.678221
Train - Epoch 0, Batch: 69, Loss: 0.681903
Train - Epoch 0, Batch: 70, Loss: 0.678412
Train - Epoch 0, Batch: 71, Loss: 0.679311
Train - Epoch 0, Batch: 72, Loss: 0.678854
Train - Epoch 0, Batch: 73, Loss: 0.677346
Train - Epoch 0, Batch: 74, Loss: 0.677640
Train - Epoch 0, Batch: 75, Loss: 0.679052
Train - Epoch 0, Batch: 76, Loss: 0.675894
Train - Epoch 0, Batch: 77, Loss: 0.680583
Train - Epoch 0, Batch: 78, Loss: 0.679209
Train - Epoch 0, Batch: 79, Loss: 0.676665
Train - Epoch 0, Batch: 80, Loss: 0.679788
Train - Epoch 0, Batch: 81, Loss: 0.672962
Train - Epoch 0, Batch: 82, Loss: 0.677327
Train - Epoch 0, Batch: 83, Loss: 0.672844
Train - Epoch 0, Batch: 84, Loss: 0.679052
Train - Epoch 0, Batch: 85, Loss: 0.676491
Train - Epoch 0, Batch: 86, Loss: 0.678498
Train - Epoch 0, Batch: 87, Loss: 0.678050
Train - Epoch 0, Batch: 88, Loss: 0.678901
Train - Epoch 0, Batch: 89, Loss: 0.677943
Train - Epoch 0, Batch: 90, Loss: 0.679396
Train - Epoch 0, Batch: 91, Loss: 0.674506
Train - Epoch 0, Batch: 92, Loss: 0.673178
Train - Epoch 0, Batch: 93, Loss: 0.674967
Train - Epoch 0, Batch: 94, Loss: 0.671805
Train - Epoch 0, Batch: 95, Loss: 0.672104
Train - Epoch 0, Batch: 96, Loss: 0.673409
Train - Epoch 0, Batch: 97, Loss: 0.674501
Train - Epoch 0, Batch: 98, Loss: 0.673819
Train - Epoch 0, Batch: 99, Loss: 0.674300
Train - Epoch 0, Batch: 100, Loss: 0.674695
Train - Epoch 0, Batch: 101, Loss: 0.676478
Train - Epoch 0, Batch: 102, Loss: 0.673624
Train - Epoch 0, Batch: 103, Loss: 0.674529
Train - Epoch 0, Batch: 104, Loss: 0.673542
Train - Epoch 0, Batch: 105, Loss: 0.670516
Train - Epoch 0, Batch: 106, Loss: 0.677318
Train - Epoch 0, Batch: 107, Loss: 0.672228
Train - Epoch 0, Batch: 108, Loss: 0.673920
Train - Epoch 0, Batch: 109, Loss: 0.672528
Train - Epoch 0, Batch: 110, Loss: 0.674413
Train - Epoch 0, Batch: 111, Loss: 0.672695
Train - Epoch 0, Batch: 112, Loss: 0.672828
Train - Epoch 0, Batch: 113, Loss: 0.673146
Train - Epoch 0, Batch: 114, Loss: 0.672156
Train - Epoch 0, Batch: 115, Loss: 0.672108
Train - Epoch 0, Batch: 116, Loss: 0.674052
Train - Epoch 0, Batch: 117, Loss: 0.668748
Train - Epoch 0, Batch: 118, Loss: 0.667124
Train - Epoch 0, Batch: 119, Loss: 0.668391
Train - Epoch 0, Batch: 120, Loss: 0.673867
Train - Epoch 0, Batch: 121, Loss: 0.671233
Train - Epoch 0, Batch: 122, Loss: 0.667799
Train - Epoch 0, Batch: 123, Loss: 0.671247
Train - Epoch 0, Batch: 124, Loss: 0.669733
Train - Epoch 0, Batch: 125, Loss: 0.671134
Train - Epoch 0, Batch: 126, Loss: 0.666433
Train - Epoch 0, Batch: 127, Loss: 0.668504
Train - Epoch 0, Batch: 128, Loss: 0.666702
Train - Epoch 0, Batch: 129, Loss: 0.666808
Train - Epoch 0, Batch: 130, Loss: 0.666226
Train - Epoch 0, Batch: 131, Loss: 0.669677
Train - Epoch 0, Batch: 132, Loss: 0.671399
Train - Epoch 0, Batch: 133, Loss: 0.671894
Train - Epoch 0, Batch: 134, Loss: 0.664421
Train - Epoch 0, Batch: 135, Loss: 0.666009
Train - Epoch 0, Batch: 136, Loss: 0.671649
Train - Epoch 0, Batch: 137, Loss: 0.668613
Train - Epoch 0, Batch: 138, Loss: 0.671775
Train - Epoch 0, Batch: 139, Loss: 0.669376
Train - Epoch 0, Batch: 140, Loss: 0.668285
Train - Epoch 0, Batch: 141, Loss: 0.666758
Train - Epoch 0, Batch: 142, Loss: 0.668810
Train - Epoch 0, Batch: 143, Loss: 0.666363
Train - Epoch 0, Batch: 144, Loss: 0.665851
Train - Epoch 0, Batch: 145, Loss: 0.665872
Train - Epoch 0, Batch: 146, Loss: 0.667442
Train - Epoch 0, Batch: 147, Loss: 0.659910
Train - Epoch 0, Batch: 148, Loss: 0.665536
Train - Epoch 0, Batch: 149, Loss: 0.665886
Train - Epoch 0, Batch: 150, Loss: 0.665825
Train - Epoch 0, Batch: 151, Loss: 0.662606
Train - Epoch 0, Batch: 152, Loss: 0.668842
Train - Epoch 0, Batch: 153, Loss: 0.663403
Train - Epoch 0, Batch: 154, Loss: 0.664612
Train - Epoch 0, Batch: 155, Loss: 0.665745
Train - Epoch 0, Batch: 156, Loss: 0.665351
Train - Epoch 0, Batch: 157, Loss: 0.664123
Train - Epoch 0, Batch: 158, Loss: 0.667251
Train - Epoch 0, Batch: 159, Loss: 0.665169
Train - Epoch 0, Batch: 160, Loss: 0.662928
Train - Epoch 0, Batch: 161, Loss: 0.661980
Train - Epoch 0, Batch: 162, Loss: 0.663481
Train - Epoch 0, Batch: 163, Loss: 0.664587
Train - Epoch 0, Batch: 164, Loss: 0.664373
Train - Epoch 0, Batch: 165, Loss: 0.665076
Train - Epoch 0, Batch: 166, Loss: 0.663118
Train - Epoch 0, Batch: 167, Loss: 0.661406
Train - Epoch 0, Batch: 168, Loss: 0.660567
Train - Epoch 0, Batch: 169, Loss: 0.663332
Train - Epoch 0, Batch: 170, Loss: 0.663057
Train - Epoch 0, Batch: 171, Loss: 0.664265
Train - Epoch 0, Batch: 172, Loss: 0.661603
Train - Epoch 0, Batch: 173, Loss: 0.659027
Train - Epoch 0, Batch: 174, Loss: 0.659512
Train - Epoch 0, Batch: 175, Loss: 0.661689
Train - Epoch 0, Batch: 176, Loss: 0.659841
Train - Epoch 0, Batch: 177, Loss: 0.662025
Train - Epoch 0, Batch: 178, Loss: 0.656467
Train - Epoch 0, Batch: 179, Loss: 0.662839
Train - Epoch 0, Batch: 180, Loss: 0.661920
Train - Epoch 0, Batch: 181, Loss: 0.662944
Train - Epoch 0, Batch: 182, Loss: 0.657495
Train - Epoch 0, Batch: 183, Loss: 0.662207
Train - Epoch 0, Batch: 184, Loss: 0.658351
Train - Epoch 0, Batch: 185, Loss: 0.658159
Train - Epoch 0, Batch: 186, Loss: 0.652912
Train - Epoch 0, Batch: 187, Loss: 0.660743
Train - Epoch 0, Batch: 188, Loss: 0.660596
Train - Epoch 0, Batch: 189, Loss: 0.654837
Train - Epoch 0, Batch: 190, Loss: 0.661491
Train - Epoch 0, Batch: 191, Loss: 0.658478
Train - Epoch 0, Batch: 192, Loss: 0.660024
Train - Epoch 0, Batch: 193, Loss: 0.660339
Train - Epoch 0, Batch: 194, Loss: 0.659193
Train - Epoch 0, Batch: 195, Loss: 0.659260
Train - Epoch 0, Batch: 196, Loss: 0.655723
Train - Epoch 0, Batch: 197, Loss: 0.654715
Train - Epoch 0, Batch: 198, Loss: 0.657292
Train - Epoch 0, Batch: 199, Loss: 0.652133
Train - Epoch 0, Batch: 200, Loss: 0.651530
Train - Epoch 0, Batch: 201, Loss: 0.660658
Train - Epoch 0, Batch: 202, Loss: 0.659395
Train - Epoch 0, Batch: 203, Loss: 0.651614
Train - Epoch 0, Batch: 204, Loss: 0.654653
Train - Epoch 0, Batch: 205, Loss: 0.659043
Train - Epoch 0, Batch: 206, Loss: 0.656657
Train - Epoch 0, Batch: 207, Loss: 0.656019
Train - Epoch 0, Batch: 208, Loss: 0.655387
Train - Epoch 0, Batch: 209, Loss: 0.650771
Train - Epoch 0, Batch: 210, Loss: 0.653429
Train - Epoch 0, Batch: 211, Loss: 0.655242
Train - Epoch 0, Batch: 212, Loss: 0.665089
Train - Epoch 0, Batch: 213, Loss: 0.650319
Train - Epoch 0, Batch: 214, Loss: 0.660282
Train - Epoch 0, Batch: 215, Loss: 0.661889
Train - Epoch 0, Batch: 216, Loss: 0.653579
Train - Epoch 0, Batch: 217, Loss: 0.651639
Train - Epoch 0, Batch: 218, Loss: 0.653049
Train - Epoch 0, Batch: 219, Loss: 0.658140
Train - Epoch 0, Batch: 220, Loss: 0.656630
Train - Epoch 0, Batch: 221, Loss: 0.654855
Train - Epoch 0, Batch: 222, Loss: 0.657577
Train - Epoch 0, Batch: 223, Loss: 0.651606
Train - Epoch 0, Batch: 224, Loss: 0.657482
Train - Epoch 0, Batch: 225, Loss: 0.658453
Train - Epoch 0, Batch: 226, Loss: 0.656666
Train - Epoch 0, Batch: 227, Loss: 0.652491
Train - Epoch 0, Batch: 228, Loss: 0.651449
Train - Epoch 0, Batch: 229, Loss: 0.648341
Train - Epoch 0, Batch: 230, Loss: 0.657671
Train - Epoch 0, Batch: 231, Loss: 0.650056
Train - Epoch 0, Batch: 232, Loss: 0.646062
Train - Epoch 0, Batch: 233, Loss: 0.656152
Train - Epoch 0, Batch: 234, Loss: 0.650729
Train - Epoch 0, Batch: 235, Loss: 0.653509
Train - Epoch 0, Batch: 236, Loss: 0.650945
Train - Epoch 0, Batch: 237, Loss: 0.656788
Train - Epoch 0, Batch: 238, Loss: 0.657680
Train - Epoch 0, Batch: 239, Loss: 0.646888
Train - Epoch 0, Batch: 240, Loss: 0.654885
Train - Epoch 0, Batch: 241, Loss: 0.650947
Train - Epoch 0, Batch: 242, Loss: 0.649257
Train - Epoch 0, Batch: 243, Loss: 0.653343
Train - Epoch 0, Batch: 244, Loss: 0.651322
Train - Epoch 0, Batch: 245, Loss: 0.654823
Train - Epoch 0, Batch: 246, Loss: 0.650922
Train - Epoch 0, Batch: 247, Loss: 0.647605
Train - Epoch 0, Batch: 248, Loss: 0.650044
Train - Epoch 0, Batch: 249, Loss: 0.644917
Train - Epoch 0, Batch: 250, Loss: 0.645006
Train - Epoch 0, Batch: 251, Loss: 0.650680
Train - Epoch 0, Batch: 252, Loss: 0.648153
Train - Epoch 0, Batch: 253, Loss: 0.650085
Train - Epoch 0, Batch: 254, Loss: 0.647496
Train - Epoch 0, Batch: 255, Loss: 0.647815
Train - Epoch 0, Batch: 256, Loss: 0.650348
Train - Epoch 0, Batch: 257, Loss: 0.655109
Train - Epoch 0, Batch: 258, Loss: 0.644092
Train - Epoch 0, Batch: 259, Loss: 0.650533
Train - Epoch 0, Batch: 260, Loss: 0.651813
Train - Epoch 0, Batch: 261, Loss: 0.643810
Train - Epoch 0, Batch: 262, Loss: 0.648407
Train - Epoch 0, Batch: 263, Loss: 0.643204
Train - Epoch 0, Batch: 264, Loss: 0.642773
Train - Epoch 0, Batch: 265, Loss: 0.642809
Train - Epoch 0, Batch: 266, Loss: 0.650174
Train - Epoch 0, Batch: 267, Loss: 0.642801
Train - Epoch 0, Batch: 268, Loss: 0.648843
Train - Epoch 0, Batch: 269, Loss: 0.651768
Train - Epoch 0, Batch: 270, Loss: 0.649537
Train - Epoch 0, Batch: 271, Loss: 0.649008
Train - Epoch 0, Batch: 272, Loss: 0.648843
Train - Epoch 0, Batch: 273, Loss: 0.648589
Train - Epoch 0, Batch: 274, Loss: 0.647356
Train - Epoch 0, Batch: 275, Loss: 0.644078
Train - Epoch 0, Batch: 276, Loss: 0.647164
Train - Epoch 0, Batch: 277, Loss: 0.648434
Train - Epoch 0, Batch: 278, Loss: 0.644474
Train - Epoch 0, Batch: 279, Loss: 0.644130
Train - Epoch 0, Batch: 280, Loss: 0.645482
Train - Epoch 0, Batch: 281, Loss: 0.641831
Train - Epoch 0, Batch: 282, Loss: 0.652155
Train - Epoch 0, Batch: 283, Loss: 0.640212
Train - Epoch 0, Batch: 284, Loss: 0.644863
Train - Epoch 0, Batch: 285, Loss: 0.644276
Train - Epoch 0, Batch: 286, Loss: 0.642594
Train - Epoch 0, Batch: 287, Loss: 0.643514
Train - Epoch 0, Batch: 288, Loss: 0.641115
Train - Epoch 0, Batch: 289, Loss: 0.640757
Train - Epoch 0, Batch: 290, Loss: 0.641260
Train - Epoch 0, Batch: 291, Loss: 0.647001
Train - Epoch 0, Batch: 292, Loss: 0.651639
Train - Epoch 0, Batch: 293, Loss: 0.644109
Train - Epoch 0, Batch: 294, Loss: 0.640262
Train - Epoch 0, Batch: 295, Loss: 0.646088
Train - Epoch 0, Batch: 296, Loss: 0.647206
Train - Epoch 0, Batch: 297, Loss: 0.644837
Train - Epoch 0, Batch: 298, Loss: 0.645323
Train - Epoch 0, Batch: 299, Loss: 0.644767
Train - Epoch 0, Batch: 300, Loss: 0.650344
Train - Epoch 0, Batch: 301, Loss: 0.643473
Train - Epoch 0, Batch: 302, Loss: 0.650632
Train - Epoch 0, Batch: 303, Loss: 0.638935
Train - Epoch 0, Batch: 304, Loss: 0.636629
Train - Epoch 0, Batch: 305, Loss: 0.641913
Train - Epoch 0, Batch: 306, Loss: 0.641347
Train - Epoch 0, Batch: 307, Loss: 0.650145
Train - Epoch 0, Batch: 308, Loss: 0.640407
Train - Epoch 0, Batch: 309, Loss: 0.636937
Train - Epoch 0, Batch: 310, Loss: 0.637632
Train - Epoch 0, Batch: 311, Loss: 0.642784
Train - Epoch 0, Batch: 312, Loss: 0.642095
Train - Epoch 0, Batch: 313, Loss: 0.638764
Train - Epoch 0, Batch: 314, Loss: 0.635534
Train - Epoch 0, Batch: 315, Loss: 0.641408
Train - Epoch 0, Batch: 316, Loss: 0.647211
Train - Epoch 0, Batch: 317, Loss: 0.639228
Train - Epoch 0, Batch: 318, Loss: 0.641595
Train - Epoch 0, Batch: 319, Loss: 0.641361
Train - Epoch 0, Batch: 320, Loss: 0.637277
Train - Epoch 0, Batch: 321, Loss: 0.641100
Train - Epoch 0, Batch: 322, Loss: 0.647224
Train - Epoch 0, Batch: 323, Loss: 0.641606
Train - Epoch 0, Batch: 324, Loss: 0.641494
Train - Epoch 0, Batch: 325, Loss: 0.637532
Train - Epoch 0, Batch: 326, Loss: 0.638230
Train - Epoch 0, Batch: 327, Loss: 0.634417
Train - Epoch 0, Batch: 328, Loss: 0.636230
Train - Epoch 0, Batch: 329, Loss: 0.647688
Train - Epoch 0, Batch: 330, Loss: 0.639174
Train - Epoch 0, Batch: 331, Loss: 0.628815
Train - Epoch 0, Batch: 332, Loss: 0.640014
Train - Epoch 0, Batch: 333, Loss: 0.649003
Train - Epoch 0, Batch: 334, Loss: 0.642747
Train - Epoch 0, Batch: 335, Loss: 0.636095
Train - Epoch 0, Batch: 336, Loss: 0.634420
Train - Epoch 0, Batch: 337, Loss: 0.639852
Train - Epoch 0, Batch: 338, Loss: 0.631979
Train - Epoch 0, Batch: 339, Loss: 0.636140
Train - Epoch 0, Batch: 340, Loss: 0.643772
Train - Epoch 0, Batch: 341, Loss: 0.630311
Train - Epoch 0, Batch: 342, Loss: 0.632333
Train - Epoch 0, Batch: 343, Loss: 0.642106
Train - Epoch 0, Batch: 344, Loss: 0.644301
Train - Epoch 0, Batch: 345, Loss: 0.637133
Train - Epoch 0, Batch: 346, Loss: 0.630428
Train - Epoch 0, Batch: 347, Loss: 0.637991
Train - Epoch 0, Batch: 348, Loss: 0.631612
Train - Epoch 0, Batch: 349, Loss: 0.627737
Train - Epoch 0, Batch: 350, Loss: 0.637856
Train - Epoch 0, Batch: 351, Loss: 0.635134
Train - Epoch 0, Batch: 352, Loss: 0.630797
Train - Epoch 0, Batch: 353, Loss: 0.636992
Train - Epoch 0, Batch: 354, Loss: 0.635666
Train - Epoch 0, Batch: 355, Loss: 0.638890
Train - Epoch 0, Batch: 356, Loss: 0.633140
Train - Epoch 0, Batch: 357, Loss: 0.642882
Train - Epoch 0, Batch: 358, Loss: 0.634955
Train - Epoch 0, Batch: 359, Loss: 0.635905
Train - Epoch 0, Batch: 360, Loss: 0.644505
Train - Epoch 0, Batch: 361, Loss: 0.633165
Train - Epoch 0, Batch: 362, Loss: 0.641047
Train - Epoch 0, Batch: 363, Loss: 0.634694
Train - Epoch 0, Batch: 364, Loss: 0.636572
Train - Epoch 0, Batch: 365, Loss: 0.631519
Train - Epoch 0, Batch: 366, Loss: 0.634810
Train - Epoch 0, Batch: 367, Loss: 0.629565
Train - Epoch 0, Batch: 368, Loss: 0.637291
Train - Epoch 0, Batch: 369, Loss: 0.632404
Train - Epoch 0, Batch: 370, Loss: 0.644720
Train - Epoch 0, Batch: 371, Loss: 0.633077
Train - Epoch 0, Batch: 372, Loss: 0.630700
Train - Epoch 0, Batch: 373, Loss: 0.636045
Train - Epoch 0, Batch: 374, Loss: 0.636350
Train - Epoch 0, Batch: 375, Loss: 0.643298
Train - Epoch 0, Batch: 376, Loss: 0.632991
Train - Epoch 0, Batch: 377, Loss: 0.627795
Train - Epoch 0, Batch: 378, Loss: 0.635934
Train - Epoch 0, Batch: 379, Loss: 0.624484
Train - Epoch 0, Batch: 380, Loss: 0.637500
Train - Epoch 0, Batch: 381, Loss: 0.627859
Train - Epoch 0, Batch: 382, Loss: 0.633944
Train - Epoch 0, Batch: 383, Loss: 0.635250
Train - Epoch 0, Batch: 384, Loss: 0.626378
Train - Epoch 0, Batch: 385, Loss: 0.631261
Train - Epoch 0, Batch: 386, Loss: 0.628003
Train - Epoch 0, Batch: 387, Loss: 0.634285
Train - Epoch 0, Batch: 388, Loss: 0.635790
Train - Epoch 0, Batch: 389, Loss: 0.629388
Train - Epoch 0, Batch: 390, Loss: 0.634166
Train - Epoch 0, Batch: 391, Loss: 0.631792
Train - Epoch 0, Batch: 392, Loss: 0.631577
Train - Epoch 0, Batch: 393, Loss: 0.637054
Train - Epoch 0, Batch: 394, Loss: 0.631170
Train - Epoch 0, Batch: 395, Loss: 0.642789
Train - Epoch 0, Batch: 396, Loss: 0.632395
Train - Epoch 0, Batch: 397, Loss: 0.631566
Train - Epoch 0, Batch: 398, Loss: 0.628497
Train - Epoch 0, Batch: 399, Loss: 0.634927
Train - Epoch 0, Batch: 400, Loss: 0.627180
Train - Epoch 0, Batch: 401, Loss: 0.628821
Train - Epoch 0, Batch: 402, Loss: 0.632182
Train - Epoch 0, Batch: 403, Loss: 0.630464
Train - Epoch 0, Batch: 404, Loss: 0.627959
Train - Epoch 0, Batch: 405, Loss: 0.626692
Train - Epoch 0, Batch: 406, Loss: 0.631204
Train - Epoch 0, Batch: 407, Loss: 0.632028
Train - Epoch 0, Batch: 408, Loss: 0.629102
Train - Epoch 0, Batch: 409, Loss: 0.627759
Train - Epoch 0, Batch: 410, Loss: 0.628166
Train - Epoch 0, Batch: 411, Loss: 0.630380
Train - Epoch 0, Batch: 412, Loss: 0.632594
Train - Epoch 0, Batch: 413, Loss: 0.633597
Train - Epoch 0, Batch: 414, Loss: 0.627065
Train - Epoch 0, Batch: 415, Loss: 0.631702
Train - Epoch 0, Batch: 416, Loss: 0.622709
Train - Epoch 0, Batch: 417, Loss: 0.634020
Train - Epoch 0, Batch: 418, Loss: 0.634131
Train - Epoch 0, Batch: 419, Loss: 0.632027
Train - Epoch 0, Batch: 420, Loss: 0.629339
Train - Epoch 0, Batch: 421, Loss: 0.629111
Train - Epoch 0, Batch: 422, Loss: 0.622530
Train - Epoch 0, Batch: 423, Loss: 0.627201
Train - Epoch 0, Batch: 424, Loss: 0.629189
Train - Epoch 0, Batch: 425, Loss: 0.627176
Train - Epoch 0, Batch: 426, Loss: 0.626903
Train - Epoch 0, Batch: 427, Loss: 0.631733
Train - Epoch 0, Batch: 428, Loss: 0.627549
Train - Epoch 0, Batch: 429, Loss: 0.615747
Train - Epoch 0, Batch: 430, Loss: 0.629580
Train - Epoch 0, Batch: 431, Loss: 0.627995
Train - Epoch 0, Batch: 432, Loss: 0.624093
Train - Epoch 0, Batch: 433, Loss: 0.623696
Train - Epoch 0, Batch: 434, Loss: 0.622722
Train - Epoch 0, Batch: 435, Loss: 0.621751
Train - Epoch 0, Batch: 436, Loss: 0.631646
Train - Epoch 0, Batch: 437, Loss: 0.618723
Train - Epoch 0, Batch: 438, Loss: 0.630932
Train - Epoch 0, Batch: 439, Loss: 0.628485
Train - Epoch 0, Batch: 440, Loss: 0.625883
Train - Epoch 0, Batch: 441, Loss: 0.630807
Train - Epoch 0, Batch: 442, Loss: 0.623293
Train - Epoch 0, Batch: 443, Loss: 0.623517
Train - Epoch 0, Batch: 444, Loss: 0.621326
Train - Epoch 0, Batch: 445, Loss: 0.626087
Train - Epoch 0, Batch: 446, Loss: 0.624587
Train - Epoch 0, Batch: 447, Loss: 0.625770
Train - Epoch 0, Batch: 448, Loss: 0.630285
Train - Epoch 0, Batch: 449, Loss: 0.620665
Train - Epoch 0, Batch: 450, Loss: 0.621148
Train - Epoch 0, Batch: 451, Loss: 0.629077
Train - Epoch 0, Batch: 452, Loss: 0.631360
Train - Epoch 0, Batch: 453, Loss: 0.628641
Train - Epoch 0, Batch: 454, Loss: 0.617318
Train - Epoch 0, Batch: 455, Loss: 0.621072
Train - Epoch 0, Batch: 456, Loss: 0.615368
Train - Epoch 0, Batch: 457, Loss: 0.619325
Train - Epoch 0, Batch: 458, Loss: 0.614496
Train - Epoch 0, Batch: 459, Loss: 0.629359
Train - Epoch 0, Batch: 460, Loss: 0.616565
Train - Epoch 0, Batch: 461, Loss: 0.625672
Train - Epoch 0, Batch: 462, Loss: 0.625167
Train - Epoch 0, Batch: 463, Loss: 0.620069
Train - Epoch 0, Batch: 464, Loss: 0.624485
Train - Epoch 0, Batch: 465, Loss: 0.625956
Train - Epoch 0, Batch: 466, Loss: 0.625215
Train - Epoch 0, Batch: 467, Loss: 0.618552
Train - Epoch 0, Batch: 468, Loss: 0.629805
Train - Epoch 0, Batch: 469, Loss: 0.628093
Train - Epoch 0, Batch: 470, Loss: 0.624283
Train - Epoch 0, Batch: 471, Loss: 0.613314
Train - Epoch 0, Batch: 472, Loss: 0.628061
Train - Epoch 0, Batch: 473, Loss: 0.614066
Train - Epoch 0, Batch: 474, Loss: 0.608201
Train - Epoch 0, Batch: 475, Loss: 0.616270
Train - Epoch 0, Batch: 476, Loss: 0.620795
Train - Epoch 0, Batch: 477, Loss: 0.630634
Train - Epoch 0, Batch: 478, Loss: 0.618857
Train - Epoch 0, Batch: 479, Loss: 0.622531
Train - Epoch 0, Batch: 480, Loss: 0.617018
Train - Epoch 0, Batch: 481, Loss: 0.630876
Train - Epoch 0, Batch: 482, Loss: 0.612809
Train - Epoch 0, Batch: 483, Loss: 0.625881
Train - Epoch 0, Batch: 484, Loss: 0.628328
Train - Epoch 0, Batch: 485, Loss: 0.628110
Train - Epoch 0, Batch: 486, Loss: 0.624521
Train - Epoch 0, Batch: 487, Loss: 0.629009
Train - Epoch 0, Batch: 488, Loss: 0.620099
Train - Epoch 0, Batch: 489, Loss: 0.621638
Train - Epoch 0, Batch: 490, Loss: 0.615723
Train - Epoch 0, Batch: 491, Loss: 0.625028
Train - Epoch 0, Batch: 492, Loss: 0.623355
Train - Epoch 0, Batch: 493, Loss: 0.627115
Train - Epoch 0, Batch: 494, Loss: 0.618364
Train - Epoch 0, Batch: 495, Loss: 0.616148
Train - Epoch 0, Batch: 496, Loss: 0.622438
Train - Epoch 0, Batch: 497, Loss: 0.613274
Train - Epoch 0, Batch: 498, Loss: 0.623668
Train - Epoch 0, Batch: 499, Loss: 0.616481
Train - Epoch 0, Batch: 500, Loss: 0.618537
Train - Epoch 0, Batch: 501, Loss: 0.617002
Train - Epoch 0, Batch: 502, Loss: 0.620391
Train - Epoch 0, Batch: 503, Loss: 0.618180
Train - Epoch 0, Batch: 504, Loss: 0.619948
Train - Epoch 0, Batch: 505, Loss: 0.616743
Train - Epoch 0, Batch: 506, Loss: 0.613673
Train - Epoch 0, Batch: 507, Loss: 0.604817
Train - Epoch 0, Batch: 508, Loss: 0.622079
Train - Epoch 0, Batch: 509, Loss: 0.620065
Train - Epoch 0, Batch: 510, Loss: 0.627315
Train - Epoch 0, Batch: 511, Loss: 0.611298
Train - Epoch 0, Batch: 512, Loss: 0.624506
Train - Epoch 0, Batch: 513, Loss: 0.629685
Train - Epoch 0, Batch: 514, Loss: 0.611952
Train - Epoch 0, Batch: 515, Loss: 0.616021
Train - Epoch 0, Batch: 516, Loss: 0.627671
Train - Epoch 0, Batch: 517, Loss: 0.619580
Train - Epoch 0, Batch: 518, Loss: 0.614451
Train - Epoch 0, Batch: 519, Loss: 0.623078
Train - Epoch 0, Batch: 520, Loss: 0.615075
Train - Epoch 0, Batch: 521, Loss: 0.621491
Train - Epoch 0, Batch: 522, Loss: 0.618999
Train - Epoch 0, Batch: 523, Loss: 0.617018
Train - Epoch 0, Batch: 524, Loss: 0.620639
Train - Epoch 0, Batch: 525, Loss: 0.617131
Train - Epoch 0, Batch: 526, Loss: 0.609777
Train - Epoch 0, Batch: 527, Loss: 0.614669
Train - Epoch 0, Batch: 528, Loss: 0.613292
Train - Epoch 0, Batch: 529, Loss: 0.611874
Train - Epoch 0, Batch: 530, Loss: 0.612391
Train - Epoch 0, Batch: 531, Loss: 0.609883
Train - Epoch 0, Batch: 532, Loss: 0.608698
Train - Epoch 0, Batch: 533, Loss: 0.611266
Train - Epoch 0, Batch: 534, Loss: 0.622076
Train - Epoch 0, Batch: 535, Loss: 0.618024
Train - Epoch 0, Batch: 536, Loss: 0.623680
Train - Epoch 0, Batch: 537, Loss: 0.615359
Train - Epoch 0, Batch: 538, Loss: 0.616259
Train - Epoch 0, Batch: 539, Loss: 0.608905
Train - Epoch 0, Batch: 540, Loss: 0.617762
Train - Epoch 0, Batch: 541, Loss: 0.613685
Train - Epoch 0, Batch: 542, Loss: 0.622119
Train - Epoch 0, Batch: 543, Loss: 0.619952
Train - Epoch 0, Batch: 544, Loss: 0.605494
Train - Epoch 0, Batch: 545, Loss: 0.611430
Train - Epoch 0, Batch: 546, Loss: 0.619985
Train - Epoch 0, Batch: 547, Loss: 0.615277
Train - Epoch 0, Batch: 548, Loss: 0.611040
Train - Epoch 0, Batch: 549, Loss: 0.616041
Train - Epoch 0, Batch: 550, Loss: 0.608986
Train - Epoch 0, Batch: 551, Loss: 0.614265
Train - Epoch 0, Batch: 552, Loss: 0.615647
Train - Epoch 0, Batch: 553, Loss: 0.617190
Train - Epoch 0, Batch: 554, Loss: 0.617773
Train - Epoch 0, Batch: 555, Loss: 0.615218
Train - Epoch 0, Batch: 556, Loss: 0.608191
Train - Epoch 0, Batch: 557, Loss: 0.611199
Train - Epoch 0, Batch: 558, Loss: 0.608991
Train - Epoch 0, Batch: 559, Loss: 0.619003
Train - Epoch 0, Batch: 560, Loss: 0.610983
Train - Epoch 0, Batch: 561, Loss: 0.603938
Train - Epoch 0, Batch: 562, Loss: 0.609078
Train - Epoch 0, Batch: 563, Loss: 0.609169
Train - Epoch 0, Batch: 564, Loss: 0.625797
Train - Epoch 0, Batch: 565, Loss: 0.617781
Train - Epoch 0, Batch: 566, Loss: 0.610879
Train - Epoch 0, Batch: 567, Loss: 0.612027
Train - Epoch 0, Batch: 568, Loss: 0.600224
Train - Epoch 0, Batch: 569, Loss: 0.624165
Train - Epoch 0, Batch: 570, Loss: 0.610154
Train - Epoch 0, Batch: 571, Loss: 0.619606
Train - Epoch 0, Batch: 572, Loss: 0.611170
Train - Epoch 0, Batch: 573, Loss: 0.607888
Train - Epoch 0, Batch: 574, Loss: 0.615168
Train - Epoch 0, Batch: 575, Loss: 0.616204
Train - Epoch 0, Batch: 576, Loss: 0.614169
Train - Epoch 0, Batch: 577, Loss: 0.618093
Train - Epoch 0, Batch: 578, Loss: 0.600922
Train - Epoch 0, Batch: 579, Loss: 0.611767
Train - Epoch 0, Batch: 580, Loss: 0.616591
Train - Epoch 0, Batch: 581, Loss: 0.613437
Train - Epoch 0, Batch: 582, Loss: 0.614181
Train - Epoch 0, Batch: 583, Loss: 0.606924
Train - Epoch 0, Batch: 584, Loss: 0.603645
Train - Epoch 0, Batch: 585, Loss: 0.602132
Train - Epoch 0, Batch: 586, Loss: 0.611500
Train - Epoch 0, Batch: 587, Loss: 0.609210
Train - Epoch 0, Batch: 588, Loss: 0.610002
Train - Epoch 0, Batch: 589, Loss: 0.610216
Train - Epoch 0, Batch: 590, Loss: 0.606913
Train - Epoch 0, Batch: 591, Loss: 0.610922
Train - Epoch 0, Batch: 592, Loss: 0.619628
Train - Epoch 0, Batch: 593, Loss: 0.613122
Train - Epoch 0, Batch: 594, Loss: 0.614126
Train - Epoch 0, Batch: 595, Loss: 0.616118
Train - Epoch 0, Batch: 596, Loss: 0.617838
Train - Epoch 0, Batch: 597, Loss: 0.610315
Train - Epoch 0, Batch: 598, Loss: 0.605548
Train - Epoch 0, Batch: 599, Loss: 0.600161
Train - Epoch 0, Batch: 600, Loss: 0.605190
Train - Epoch 0, Batch: 601, Loss: 0.616903
Train - Epoch 0, Batch: 602, Loss: 0.608590
Train - Epoch 0, Batch: 603, Loss: 0.613040
Train - Epoch 0, Batch: 604, Loss: 0.612244
Train - Epoch 0, Batch: 605, Loss: 0.618115
Train - Epoch 0, Batch: 606, Loss: 0.608471
Train - Epoch 0, Batch: 607, Loss: 0.596175
Train - Epoch 0, Batch: 608, Loss: 0.606293
Train - Epoch 0, Batch: 609, Loss: 0.606546
Train - Epoch 0, Batch: 610, Loss: 0.604715
Train - Epoch 0, Batch: 611, Loss: 0.603523
Train - Epoch 0, Batch: 612, Loss: 0.610263
Train - Epoch 0, Batch: 613, Loss: 0.606948
Train - Epoch 0, Batch: 614, Loss: 0.597150
Train - Epoch 0, Batch: 615, Loss: 0.615245
Train - Epoch 0, Batch: 616, Loss: 0.609862
Train - Epoch 0, Batch: 617, Loss: 0.613470
Train - Epoch 0, Batch: 618, Loss: 0.609129
Train - Epoch 0, Batch: 619, Loss: 0.613582
Train - Epoch 0, Batch: 620, Loss: 0.611886
Train - Epoch 0, Batch: 621, Loss: 0.600411
Train - Epoch 0, Batch: 622, Loss: 0.609439
Train - Epoch 0, Batch: 623, Loss: 0.621307
Train - Epoch 0, Batch: 624, Loss: 0.614033
Train - Epoch 0, Batch: 625, Loss: 0.615665
Train - Epoch 0, Batch: 626, Loss: 0.610133
Train - Epoch 0, Batch: 627, Loss: 0.600223
Train - Epoch 0, Batch: 628, Loss: 0.609046
Train - Epoch 0, Batch: 629, Loss: 0.607789
Train - Epoch 0, Batch: 630, Loss: 0.607592
Train - Epoch 0, Batch: 631, Loss: 0.610211
Train - Epoch 0, Batch: 632, Loss: 0.607277
Train - Epoch 0, Batch: 633, Loss: 0.601286
Train - Epoch 0, Batch: 634, Loss: 0.610169
Train - Epoch 0, Batch: 635, Loss: 0.605979
Train - Epoch 0, Batch: 636, Loss: 0.604705
Train - Epoch 0, Batch: 637, Loss: 0.611239
Train - Epoch 0, Batch: 638, Loss: 0.608332
Train - Epoch 0, Batch: 639, Loss: 0.603854
Train - Epoch 0, Batch: 640, Loss: 0.600124
Train - Epoch 0, Batch: 641, Loss: 0.609249
Train - Epoch 0, Batch: 642, Loss: 0.600747
Train - Epoch 0, Batch: 643, Loss: 0.602806
Train - Epoch 0, Batch: 644, Loss: 0.601014
Train - Epoch 0, Batch: 645, Loss: 0.606098
Train - Epoch 0, Batch: 646, Loss: 0.605848
Train - Epoch 0, Batch: 647, Loss: 0.603292
Train - Epoch 0, Batch: 648, Loss: 0.602972
Train - Epoch 0, Batch: 649, Loss: 0.605896
Train - Epoch 0, Batch: 650, Loss: 0.611362
Train - Epoch 0, Batch: 651, Loss: 0.602397
Train - Epoch 0, Batch: 652, Loss: 0.610603
Train - Epoch 0, Batch: 653, Loss: 0.601502
Train - Epoch 0, Batch: 654, Loss: 0.601084
Train - Epoch 0, Batch: 655, Loss: 0.600621
Train - Epoch 0, Batch: 656, Loss: 0.604483
Train - Epoch 0, Batch: 657, Loss: 0.589803
Train - Epoch 0, Batch: 658, Loss: 0.606133
Train - Epoch 0, Batch: 659, Loss: 0.602896
Train - Epoch 0, Batch: 660, Loss: 0.606505
Train - Epoch 0, Batch: 661, Loss: 0.604083
Train - Epoch 0, Batch: 662, Loss: 0.603680
Train - Epoch 0, Batch: 663, Loss: 0.605313
Train - Epoch 0, Batch: 664, Loss: 0.606528
Train - Epoch 0, Batch: 665, Loss: 0.611081
Train - Epoch 0, Batch: 666, Loss: 0.599095
Train - Epoch 0, Batch: 667, Loss: 0.607306
Train - Epoch 0, Batch: 668, Loss: 0.601969
Train - Epoch 0, Batch: 669, Loss: 0.596310
Train - Epoch 0, Batch: 670, Loss: 0.600803
Train - Epoch 0, Batch: 671, Loss: 0.604416
Train - Epoch 0, Batch: 672, Loss: 0.602991
Train - Epoch 0, Batch: 673, Loss: 0.597158
Train - Epoch 0, Batch: 674, Loss: 0.606630
Train - Epoch 0, Batch: 675, Loss: 0.605988
Train - Epoch 0, Batch: 676, Loss: 0.605746
Train - Epoch 0, Batch: 677, Loss: 0.590429
Train - Epoch 0, Batch: 678, Loss: 0.595438
Train - Epoch 0, Batch: 679, Loss: 0.603351
Train - Epoch 0, Batch: 680, Loss: 0.605391
Train - Epoch 0, Batch: 681, Loss: 0.604382
Train - Epoch 0, Batch: 682, Loss: 0.610853
Train - Epoch 0, Batch: 683, Loss: 0.595634
Train - Epoch 0, Batch: 684, Loss: 0.599394
Train - Epoch 0, Batch: 685, Loss: 0.598705
Train - Epoch 0, Batch: 686, Loss: 0.606892
Train - Epoch 0, Batch: 687, Loss: 0.597191
Train - Epoch 0, Batch: 688, Loss: 0.596690
Train - Epoch 0, Batch: 689, Loss: 0.604348
Train - Epoch 0, Batch: 690, Loss: 0.599711
Train - Epoch 0, Batch: 691, Loss: 0.596031
Train - Epoch 0, Batch: 692, Loss: 0.593969
Train - Epoch 0, Batch: 693, Loss: 0.610628
Train - Epoch 0, Batch: 694, Loss: 0.598503
Train - Epoch 0, Batch: 695, Loss: 0.599605
Train - Epoch 0, Batch: 696, Loss: 0.602194
Train - Epoch 0, Batch: 697, Loss: 0.604679
Train - Epoch 0, Batch: 698, Loss: 0.603561
Train - Epoch 0, Batch: 699, Loss: 0.606325
Train - Epoch 0, Batch: 700, Loss: 0.600255
Train - Epoch 0, Batch: 701, Loss: 0.606300
Train - Epoch 0, Batch: 702, Loss: 0.600729
Train - Epoch 0, Batch: 703, Loss: 0.594562
Train - Epoch 0, Batch: 704, Loss: 0.607166
Train - Epoch 0, Batch: 705, Loss: 0.594899
Train - Epoch 0, Batch: 706, Loss: 0.591085
Train - Epoch 0, Batch: 707, Loss: 0.597315
Train - Epoch 0, Batch: 708, Loss: 0.592591
Train - Epoch 0, Batch: 709, Loss: 0.609903
Train - Epoch 0, Batch: 710, Loss: 0.588765
Train - Epoch 0, Batch: 711, Loss: 0.603399
Train - Epoch 0, Batch: 712, Loss: 0.603889
Train - Epoch 0, Batch: 713, Loss: 0.595722
Train - Epoch 0, Batch: 714, Loss: 0.587873
Train - Epoch 0, Batch: 715, Loss: 0.597687
Train - Epoch 0, Batch: 716, Loss: 0.591310
Train - Epoch 0, Batch: 717, Loss: 0.595123
Train - Epoch 0, Batch: 718, Loss: 0.598263
Train - Epoch 0, Batch: 719, Loss: 0.592809
Train - Epoch 0, Batch: 720, Loss: 0.602432
Train - Epoch 0, Batch: 721, Loss: 0.594973
Train - Epoch 0, Batch: 722, Loss: 0.596984
Train - Epoch 0, Batch: 723, Loss: 0.587814
Train - Epoch 0, Batch: 724, Loss: 0.594898
Train - Epoch 0, Batch: 725, Loss: 0.602101
Train - Epoch 0, Batch: 726, Loss: 0.602544
Train - Epoch 0, Batch: 727, Loss: 0.598288
Train - Epoch 0, Batch: 728, Loss: 0.590103
Train - Epoch 0, Batch: 729, Loss: 0.588395
Train - Epoch 0, Batch: 730, Loss: 0.601366
Train - Epoch 0, Batch: 731, Loss: 0.595654
Train - Epoch 0, Batch: 732, Loss: 0.594462
Train - Epoch 0, Batch: 733, Loss: 0.596950
Train - Epoch 0, Batch: 734, Loss: 0.594630
Train - Epoch 0, Batch: 735, Loss: 0.595875
Train - Epoch 0, Batch: 736, Loss: 0.598383
Train - Epoch 0, Batch: 737, Loss: 0.600120
Train - Epoch 0, Batch: 738, Loss: 0.593149
Train - Epoch 0, Batch: 739, Loss: 0.598318
Train - Epoch 0, Batch: 740, Loss: 0.593349
Train - Epoch 0, Batch: 741, Loss: 0.591002
Train - Epoch 0, Batch: 742, Loss: 0.591880
Train - Epoch 0, Batch: 743, Loss: 0.593442
Train - Epoch 0, Batch: 744, Loss: 0.597415
Train - Epoch 0, Batch: 745, Loss: 0.592959
Train - Epoch 0, Batch: 746, Loss: 0.587050
Train - Epoch 0, Batch: 747, Loss: 0.585860
Train - Epoch 0, Batch: 748, Loss: 0.587063
Train - Epoch 0, Batch: 749, Loss: 0.592955
Train - Epoch 0, Batch: 750, Loss: 0.599571
Train - Epoch 0, Batch: 751, Loss: 0.599434
Train - Epoch 0, Batch: 752, Loss: 0.590205
Train - Epoch 0, Batch: 753, Loss: 0.592414
Train - Epoch 0, Batch: 754, Loss: 0.592910
Train - Epoch 0, Batch: 755, Loss: 0.585805
Train - Epoch 0, Batch: 756, Loss: 0.589981
Train - Epoch 0, Batch: 757, Loss: 0.600432
Train - Epoch 0, Batch: 758, Loss: 0.599629
Train - Epoch 0, Batch: 759, Loss: 0.595813
Train - Epoch 0, Batch: 760, Loss: 0.604698
Train - Epoch 0, Batch: 761, Loss: 0.596574
Train - Epoch 0, Batch: 762, Loss: 0.598080
Train - Epoch 0, Batch: 763, Loss: 0.587441
Train - Epoch 0, Batch: 764, Loss: 0.603237
Train - Epoch 0, Batch: 765, Loss: 0.586666
Train - Epoch 0, Batch: 766, Loss: 0.583488
Train - Epoch 0, Batch: 767, Loss: 0.581566
Train - Epoch 0, Batch: 768, Loss: 0.604352
Train - Epoch 0, Batch: 769, Loss: 0.602530
Train - Epoch 0, Batch: 770, Loss: 0.584535
Train - Epoch 0, Batch: 771, Loss: 0.586408
Train - Epoch 0, Batch: 772, Loss: 0.605106
Train - Epoch 0, Batch: 773, Loss: 0.590286
Train - Epoch 0, Batch: 774, Loss: 0.596582
Train - Epoch 0, Batch: 775, Loss: 0.593447
Train - Epoch 0, Batch: 776, Loss: 0.587783
Train - Epoch 0, Batch: 777, Loss: 0.600448
Train - Epoch 0, Batch: 778, Loss: 0.588377
Train - Epoch 0, Batch: 779, Loss: 0.591378
Train - Epoch 0, Batch: 780, Loss: 0.602592
Train - Epoch 0, Batch: 781, Loss: 0.581902
training_time:: 53.1413950920105
training time full:: 53.14155554771423
provenance prepare time:: 2.384185791015625e-07
here
Test Avg. Loss: 0.000776, Accuracy: 0.878010, F1 Score: 0.879140
</training>
torch.Size([400000, 2001])
tensor([287294, 125390, 163970,  ..., 146081, 181869,  15816])
<results lr="1" epochs="60" bz="512" remove_ratio="0.1" sampling_type="targeted_informed">
<baseline>
data dimension:: [400000, 2001]
tensor([393216,  24579,     19, 395284,  75810,  47141, 149544, 249897, 356398,
        374835, 106553, 198717,  71743,  63558, 139349, 358497,  84073, 378989,
        309358, 264311, 192633, 170106,  82044, 372861, 120958,  39039,   8320,
        137348, 338056, 133258, 114827, 278673,  32914, 159891, 336044,  65709,
        161968, 358577, 155830, 245947, 303291, 127167, 350401,  61637, 352457,
        350413, 372944, 205009, 108763, 344285])
Epoch:0 Batch: 781 Baseline Loss 0.5826473212188078
training time is 56.6064989566803
overhead:: 0
overhead2:: 9.02458381652832
overhead3:: 0
memory usage:: 17504387072
time_baseline:: 56.65967154502869
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.9871, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.864920, F1 Score: 0.872511
Remove Test Avg. Loss: 0.000921, Accuracy: 0.805050, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867027, F1 Score: 0.874562
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.6636, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864520, F1 Score: 0.872165
Remove Test Avg. Loss: 0.000927, Accuracy: 0.803425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.865780, F1 Score: 0.873420
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6947, dtype=torch.float64)
Test Avg. Loss: 0.000925, Accuracy: 0.786050, F1 Score: 0.798447
Remove Test Avg. Loss: 0.001114, Accuracy: 0.723925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000918, Accuracy: 0.787447, F1 Score: 0.799722
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7287, dtype=torch.float64)
Test Avg. Loss: 0.005957, Accuracy: 0.550060, F1 Score: 0.558449
Remove Test Avg. Loss: 0.007027, Accuracy: 0.530475, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005936, Accuracy: 0.552105, F1 Score: 0.559823
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1417, dtype=torch.float64)
Test Avg. Loss: 0.065438, Accuracy: 0.510660, F1 Score: 0.514794
Remove Test Avg. Loss: 0.075209, Accuracy: 0.500775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065216, Accuracy: 0.512068, F1 Score: 0.515703
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.9775, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.865360, F1 Score: 0.872847
Remove Test Avg. Loss: 0.000920, Accuracy: 0.806050, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867360, F1 Score: 0.874791
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.6644, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.867350, F1 Score: 0.873788
Remove Test Avg. Loss: 0.000909, Accuracy: 0.814625, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.869147, F1 Score: 0.875703
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9090, dtype=torch.float64)
Test Avg. Loss: 0.000912, Accuracy: 0.788620, F1 Score: 0.785653
Remove Test Avg. Loss: 0.000920, Accuracy: 0.801425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000902, Accuracy: 0.791637, F1 Score: 0.789051
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.0783, dtype=torch.float64)
Test Avg. Loss: 0.006203, Accuracy: 0.529540, F1 Score: 0.467565
Remove Test Avg. Loss: 0.004698, Accuracy: 0.644550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006095, Accuracy: 0.534168, F1 Score: 0.474151
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8442, dtype=torch.float64)
Test Avg. Loss: 0.069170, Accuracy: 0.491120, F1 Score: 0.418144
Remove Test Avg. Loss: 0.051075, Accuracy: 0.615150, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068022, Accuracy: 0.495470, F1 Score: 0.424214
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.9677, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.867550, F1 Score: 0.874263
Remove Test Avg. Loss: 0.000908, Accuracy: 0.812575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.869062, F1 Score: 0.875856
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.5443, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877230, F1 Score: 0.877689
Remove Test Avg. Loss: 0.000801, Accuracy: 0.870275, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878262, F1 Score: 0.878977
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7770, dtype=torch.float64)
Test Avg. Loss: 0.001357, Accuracy: 0.639630, F1 Score: 0.457184
Remove Test Avg. Loss: 0.000263, Accuracy: 0.975625, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001348, Accuracy: 0.641338, F1 Score: 0.460664
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8479, dtype=torch.float64)
Test Avg. Loss: 0.017014, Accuracy: 0.510690, F1 Score: 0.097830
Remove Test Avg. Loss: 0.000284, Accuracy: 0.967575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016970, Accuracy: 0.509827, F1 Score: 0.094215
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6309, dtype=torch.float64)
Test Avg. Loss: 0.179949, Accuracy: 0.501950, F1 Score: 0.075580
Remove Test Avg. Loss: 0.003164, Accuracy: 0.961800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179605, Accuracy: 0.501070, F1 Score: 0.072578
</noise>
</baseline>
<deltagrad period="2">
data dimension:: [400000, 2001]
overhead2:: 11.801492691040039
overhead3:: 18.331586122512817
overhead4:: 11.550246715545654
overhead5:: 0
memory usage:: 20384194560
time_deltagrad:: 84.57597374916077
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.4651, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.864950, F1 Score: 0.872497
Remove Test Avg. Loss: 0.000920, Accuracy: 0.805950, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867247, F1 Score: 0.874712
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.5701, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864720, F1 Score: 0.872293
Remove Test Avg. Loss: 0.000926, Accuracy: 0.803925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.865950, F1 Score: 0.873535
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6737, dtype=torch.float64)
Test Avg. Loss: 0.000925, Accuracy: 0.786120, F1 Score: 0.798409
Remove Test Avg. Loss: 0.001114, Accuracy: 0.724300, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000918, Accuracy: 0.787438, F1 Score: 0.799648
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7150, dtype=torch.float64)
Test Avg. Loss: 0.005957, Accuracy: 0.550070, F1 Score: 0.558446
Remove Test Avg. Loss: 0.007026, Accuracy: 0.530575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005936, Accuracy: 0.552107, F1 Score: 0.559790
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1287, dtype=torch.float64)
Test Avg. Loss: 0.065438, Accuracy: 0.510670, F1 Score: 0.514799
Remove Test Avg. Loss: 0.075208, Accuracy: 0.500775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065217, Accuracy: 0.512065, F1 Score: 0.515697
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.4623, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.865580, F1 Score: 0.872992
Remove Test Avg. Loss: 0.000919, Accuracy: 0.806625, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867520, F1 Score: 0.874897
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.5886, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.867540, F1 Score: 0.873912
Remove Test Avg. Loss: 0.000908, Accuracy: 0.815275, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.869295, F1 Score: 0.875795
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9057, dtype=torch.float64)
Test Avg. Loss: 0.000912, Accuracy: 0.788520, F1 Score: 0.785452
Remove Test Avg. Loss: 0.000919, Accuracy: 0.802025, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000902, Accuracy: 0.791620, F1 Score: 0.788935
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.0823, dtype=torch.float64)
Test Avg. Loss: 0.006203, Accuracy: 0.529550, F1 Score: 0.467546
Remove Test Avg. Loss: 0.004697, Accuracy: 0.644550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006095, Accuracy: 0.534175, F1 Score: 0.474137
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8490, dtype=torch.float64)
Test Avg. Loss: 0.069170, Accuracy: 0.491110, F1 Score: 0.418125
Remove Test Avg. Loss: 0.051074, Accuracy: 0.615175, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068023, Accuracy: 0.495472, F1 Score: 0.424215
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.4529, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.867720, F1 Score: 0.874376
Remove Test Avg. Loss: 0.000907, Accuracy: 0.813300, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.869267, F1 Score: 0.876002
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.4782, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877290, F1 Score: 0.877690
Remove Test Avg. Loss: 0.000801, Accuracy: 0.870600, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878305, F1 Score: 0.878971
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7854, dtype=torch.float64)
Test Avg. Loss: 0.001357, Accuracy: 0.639550, F1 Score: 0.457015
Remove Test Avg. Loss: 0.000263, Accuracy: 0.975650, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001348, Accuracy: 0.641277, F1 Score: 0.460501
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8638, dtype=torch.float64)
Test Avg. Loss: 0.017015, Accuracy: 0.510700, F1 Score: 0.097865
Remove Test Avg. Loss: 0.000284, Accuracy: 0.967575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016971, Accuracy: 0.509820, F1 Score: 0.094197
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6476, dtype=torch.float64)
Test Avg. Loss: 0.179950, Accuracy: 0.501950, F1 Score: 0.075580
Remove Test Avg. Loss: 0.003164, Accuracy: 0.961800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179606, Accuracy: 0.501067, F1 Score: 0.072577
</noise>
</deltagrad>
<deltagrad period="5">
data dimension:: [400000, 2001]
overhead2:: 4.761078119277954
overhead3:: 8.50335168838501
overhead4:: 4.4534547328948975
overhead5:: 0
memory usage:: 20386267136
time_deltagrad:: 67.030104637146
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.4809, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.864360, F1 Score: 0.872105
Remove Test Avg. Loss: 0.000923, Accuracy: 0.803725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.866648, F1 Score: 0.874311
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.5696, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864040, F1 Score: 0.871820
Remove Test Avg. Loss: 0.000928, Accuracy: 0.801900, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.865425, F1 Score: 0.873193
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6715, dtype=torch.float64)
Test Avg. Loss: 0.000925, Accuracy: 0.785860, F1 Score: 0.798464
Remove Test Avg. Loss: 0.001116, Accuracy: 0.723125, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000918, Accuracy: 0.787480, F1 Score: 0.799918
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7126, dtype=torch.float64)
Test Avg. Loss: 0.005957, Accuracy: 0.549940, F1 Score: 0.558401
Remove Test Avg. Loss: 0.007028, Accuracy: 0.530350, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005935, Accuracy: 0.552100, F1 Score: 0.559877
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1264, dtype=torch.float64)
Test Avg. Loss: 0.065437, Accuracy: 0.510640, F1 Score: 0.514774
Remove Test Avg. Loss: 0.075211, Accuracy: 0.500775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065216, Accuracy: 0.512070, F1 Score: 0.515717
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.4888, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.864680, F1 Score: 0.872335
Remove Test Avg. Loss: 0.000921, Accuracy: 0.804750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.866915, F1 Score: 0.874479
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.5993, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.867070, F1 Score: 0.873618
Remove Test Avg. Loss: 0.000910, Accuracy: 0.813150, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.868687, F1 Score: 0.875376
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9147, dtype=torch.float64)
Test Avg. Loss: 0.000911, Accuracy: 0.789120, F1 Score: 0.786355
Remove Test Avg. Loss: 0.000921, Accuracy: 0.801200, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000901, Accuracy: 0.791990, F1 Score: 0.789598
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.0912, dtype=torch.float64)
Test Avg. Loss: 0.006201, Accuracy: 0.529520, F1 Score: 0.467614
Remove Test Avg. Loss: 0.004698, Accuracy: 0.644300, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006093, Accuracy: 0.534177, F1 Score: 0.474251
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8578, dtype=torch.float64)
Test Avg. Loss: 0.069168, Accuracy: 0.491110, F1 Score: 0.418125
Remove Test Avg. Loss: 0.051075, Accuracy: 0.615100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068021, Accuracy: 0.495465, F1 Score: 0.424218
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.4774, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.867050, F1 Score: 0.873896
Remove Test Avg. Loss: 0.000910, Accuracy: 0.811525, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.868665, F1 Score: 0.875584
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.4863, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877340, F1 Score: 0.877894
Remove Test Avg. Loss: 0.000803, Accuracy: 0.869650, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878293, F1 Score: 0.879117
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7919, dtype=torch.float64)
Test Avg. Loss: 0.001355, Accuracy: 0.639770, F1 Score: 0.457509
Remove Test Avg. Loss: 0.000263, Accuracy: 0.975725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001346, Accuracy: 0.641460, F1 Score: 0.460895
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8700, dtype=torch.float64)
Test Avg. Loss: 0.017012, Accuracy: 0.510690, F1 Score: 0.097830
Remove Test Avg. Loss: 0.000284, Accuracy: 0.967575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016968, Accuracy: 0.509810, F1 Score: 0.094136
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6538, dtype=torch.float64)
Test Avg. Loss: 0.179947, Accuracy: 0.501950, F1 Score: 0.075580
Remove Test Avg. Loss: 0.003164, Accuracy: 0.961775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179603, Accuracy: 0.501065, F1 Score: 0.072577
</noise>
</deltagrad>
<deltagrad period="10">
data dimension:: [400000, 2001]
overhead2:: 2.4056625366210938
overhead3:: 5.253556251525879
overhead4:: 2.2729201316833496
overhead5:: 0
memory usage:: 20384182272
time_deltagrad:: 61.85607123374939
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.5088, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.864810, F1 Score: 0.872403
Remove Test Avg. Loss: 0.000920, Accuracy: 0.805450, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867023, F1 Score: 0.874549
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.5768, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864510, F1 Score: 0.872159
Remove Test Avg. Loss: 0.000926, Accuracy: 0.803400, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.865730, F1 Score: 0.873364
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6764, dtype=torch.float64)
Test Avg. Loss: 0.000925, Accuracy: 0.786150, F1 Score: 0.798530
Remove Test Avg. Loss: 0.001113, Accuracy: 0.724225, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000918, Accuracy: 0.787660, F1 Score: 0.799911
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7174, dtype=torch.float64)
Test Avg. Loss: 0.005957, Accuracy: 0.550060, F1 Score: 0.558423
Remove Test Avg. Loss: 0.007025, Accuracy: 0.530550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005935, Accuracy: 0.552107, F1 Score: 0.559812
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1310, dtype=torch.float64)
Test Avg. Loss: 0.065437, Accuracy: 0.510640, F1 Score: 0.514774
Remove Test Avg. Loss: 0.075207, Accuracy: 0.500775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065216, Accuracy: 0.512065, F1 Score: 0.515704
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.5067, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.865160, F1 Score: 0.872634
Remove Test Avg. Loss: 0.000919, Accuracy: 0.806225, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867320, F1 Score: 0.874746
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.5958, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.867470, F1 Score: 0.873893
Remove Test Avg. Loss: 0.000908, Accuracy: 0.814750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.869073, F1 Score: 0.875617
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9089, dtype=torch.float64)
Test Avg. Loss: 0.000911, Accuracy: 0.788900, F1 Score: 0.785907
Remove Test Avg. Loss: 0.000919, Accuracy: 0.802225, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000901, Accuracy: 0.791890, F1 Score: 0.789274
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.0851, dtype=torch.float64)
Test Avg. Loss: 0.006202, Accuracy: 0.529540, F1 Score: 0.467504
Remove Test Avg. Loss: 0.004695, Accuracy: 0.644475, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006094, Accuracy: 0.534180, F1 Score: 0.474161
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8518, dtype=torch.float64)
Test Avg. Loss: 0.069169, Accuracy: 0.491100, F1 Score: 0.418107
Remove Test Avg. Loss: 0.051073, Accuracy: 0.615125, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068021, Accuracy: 0.495465, F1 Score: 0.424205
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.4949, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.867440, F1 Score: 0.874136
Remove Test Avg. Loss: 0.000907, Accuracy: 0.812900, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.869012, F1 Score: 0.875799
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.4818, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877450, F1 Score: 0.877879
Remove Test Avg. Loss: 0.000800, Accuracy: 0.870750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878455, F1 Score: 0.879151
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7850, dtype=torch.float64)
Test Avg. Loss: 0.001357, Accuracy: 0.639340, F1 Score: 0.456461
Remove Test Avg. Loss: 0.000262, Accuracy: 0.975825, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001348, Accuracy: 0.641097, F1 Score: 0.459950
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8629, dtype=torch.float64)
Test Avg. Loss: 0.017016, Accuracy: 0.510690, F1 Score: 0.097797
Remove Test Avg. Loss: 0.000283, Accuracy: 0.967600, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016972, Accuracy: 0.509800, F1 Score: 0.094059
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6467, dtype=torch.float64)
Test Avg. Loss: 0.179950, Accuracy: 0.501950, F1 Score: 0.075580
Remove Test Avg. Loss: 0.003164, Accuracy: 0.961800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179606, Accuracy: 0.501065, F1 Score: 0.072568
</noise>
</deltagrad>
<deltagrad period="20">
data dimension:: [400000, 2001]
overhead2:: 1.2083392143249512
overhead3:: 3.6431562900543213
overhead4:: 1.133439540863037
overhead5:: 0
memory usage:: 20386246656
time_deltagrad:: 58.760091066360474
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.5398, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.865140, F1 Score: 0.872718
Remove Test Avg. Loss: 0.000921, Accuracy: 0.805950, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867315, F1 Score: 0.874809
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.5721, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864830, F1 Score: 0.872444
Remove Test Avg. Loss: 0.000927, Accuracy: 0.803950, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.865995, F1 Score: 0.873611
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6677, dtype=torch.float64)
Test Avg. Loss: 0.000925, Accuracy: 0.786060, F1 Score: 0.798440
Remove Test Avg. Loss: 0.001115, Accuracy: 0.724100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000919, Accuracy: 0.787473, F1 Score: 0.799725
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7082, dtype=torch.float64)
Test Avg. Loss: 0.005958, Accuracy: 0.550100, F1 Score: 0.558497
Remove Test Avg. Loss: 0.007028, Accuracy: 0.530425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005937, Accuracy: 0.552098, F1 Score: 0.559804
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1218, dtype=torch.float64)
Test Avg. Loss: 0.065439, Accuracy: 0.510650, F1 Score: 0.514798
Remove Test Avg. Loss: 0.075211, Accuracy: 0.500750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065217, Accuracy: 0.512075, F1 Score: 0.515710
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.5462, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.865730, F1 Score: 0.873154
Remove Test Avg. Loss: 0.000920, Accuracy: 0.806575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867570, F1 Score: 0.874970
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.6009, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.867720, F1 Score: 0.874132
Remove Test Avg. Loss: 0.000909, Accuracy: 0.815175, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.869340, F1 Score: 0.875860
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9100, dtype=torch.float64)
Test Avg. Loss: 0.000912, Accuracy: 0.788530, F1 Score: 0.785551
Remove Test Avg. Loss: 0.000921, Accuracy: 0.801550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000902, Accuracy: 0.791508, F1 Score: 0.788888
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.0859, dtype=torch.float64)
Test Avg. Loss: 0.006203, Accuracy: 0.529580, F1 Score: 0.467598
Remove Test Avg. Loss: 0.004699, Accuracy: 0.644550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006095, Accuracy: 0.534188, F1 Score: 0.474206
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8525, dtype=torch.float64)
Test Avg. Loss: 0.069171, Accuracy: 0.491100, F1 Score: 0.418134
Remove Test Avg. Loss: 0.051077, Accuracy: 0.615100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068023, Accuracy: 0.495460, F1 Score: 0.424199
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.5352, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.868000, F1 Score: 0.874672
Remove Test Avg. Loss: 0.000908, Accuracy: 0.813125, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.869372, F1 Score: 0.876123
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.4871, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877180, F1 Score: 0.877599
Remove Test Avg. Loss: 0.000801, Accuracy: 0.870400, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878332, F1 Score: 0.879034
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7862, dtype=torch.float64)
Test Avg. Loss: 0.001357, Accuracy: 0.639730, F1 Score: 0.457449
Remove Test Avg. Loss: 0.000263, Accuracy: 0.975425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001348, Accuracy: 0.641455, F1 Score: 0.461037
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8637, dtype=torch.float64)
Test Avg. Loss: 0.017014, Accuracy: 0.510700, F1 Score: 0.097931
Remove Test Avg. Loss: 0.000284, Accuracy: 0.967450, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016970, Accuracy: 0.509820, F1 Score: 0.094280
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6474, dtype=torch.float64)
Test Avg. Loss: 0.179948, Accuracy: 0.501950, F1 Score: 0.075580
Remove Test Avg. Loss: 0.003165, Accuracy: 0.961800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179605, Accuracy: 0.501067, F1 Score: 0.072577
</noise>
</deltagrad>
<deltagrad period="50">
data dimension:: [400000, 2001]
overhead2:: 0.4671792984008789
overhead3:: 2.742401361465454
overhead4:: 0.4926156997680664
overhead5:: 0
memory usage:: 20384178176
time_deltagrad:: 55.729822635650635
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.6533, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.865770, F1 Score: 0.873283
Remove Test Avg. Loss: 0.000923, Accuracy: 0.805600, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867355, F1 Score: 0.874882
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.6059, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.864860, F1 Score: 0.872459
Remove Test Avg. Loss: 0.000929, Accuracy: 0.804100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000784, Accuracy: 0.866217, F1 Score: 0.873843
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6902, dtype=torch.float64)
Test Avg. Loss: 0.000926, Accuracy: 0.785830, F1 Score: 0.798198
Remove Test Avg. Loss: 0.001118, Accuracy: 0.723575, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000920, Accuracy: 0.786960, F1 Score: 0.799278
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7295, dtype=torch.float64)
Test Avg. Loss: 0.005961, Accuracy: 0.550060, F1 Score: 0.558458
Remove Test Avg. Loss: 0.007034, Accuracy: 0.530425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005939, Accuracy: 0.552042, F1 Score: 0.559772
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1431, dtype=torch.float64)
Test Avg. Loss: 0.065441, Accuracy: 0.510640, F1 Score: 0.514793
Remove Test Avg. Loss: 0.075217, Accuracy: 0.500750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065220, Accuracy: 0.512070, F1 Score: 0.515700
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.6578, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.865990, F1 Score: 0.873393
Remove Test Avg. Loss: 0.000922, Accuracy: 0.806750, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.867710, F1 Score: 0.875116
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.6335, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.867850, F1 Score: 0.874271
Remove Test Avg. Loss: 0.000911, Accuracy: 0.815050, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.869470, F1 Score: 0.876009
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9315, dtype=torch.float64)
Test Avg. Loss: 0.000913, Accuracy: 0.787720, F1 Score: 0.784811
Remove Test Avg. Loss: 0.000924, Accuracy: 0.800625, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000903, Accuracy: 0.790905, F1 Score: 0.788338
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.1062, dtype=torch.float64)
Test Avg. Loss: 0.006206, Accuracy: 0.529530, F1 Score: 0.467607
Remove Test Avg. Loss: 0.004705, Accuracy: 0.644425, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006098, Accuracy: 0.534145, F1 Score: 0.474204
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8726, dtype=torch.float64)
Test Avg. Loss: 0.069173, Accuracy: 0.491110, F1 Score: 0.418179
Remove Test Avg. Loss: 0.051083, Accuracy: 0.615150, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068026, Accuracy: 0.495457, F1 Score: 0.424204
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.6372, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.868210, F1 Score: 0.874902
Remove Test Avg. Loss: 0.000910, Accuracy: 0.813525, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000780, Accuracy: 0.869565, F1 Score: 0.876317
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.5040, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.876810, F1 Score: 0.877273
Remove Test Avg. Loss: 0.000803, Accuracy: 0.869675, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000774, Accuracy: 0.878228, F1 Score: 0.878950
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7915, dtype=torch.float64)
Test Avg. Loss: 0.001357, Accuracy: 0.640170, F1 Score: 0.458975
Remove Test Avg. Loss: 0.000265, Accuracy: 0.974925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001348, Accuracy: 0.641948, F1 Score: 0.462503
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8679, dtype=torch.float64)
Test Avg. Loss: 0.017012, Accuracy: 0.510680, F1 Score: 0.098127
Remove Test Avg. Loss: 0.000285, Accuracy: 0.967375, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016968, Accuracy: 0.509833, F1 Score: 0.094492
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6515, dtype=torch.float64)
Test Avg. Loss: 0.179946, Accuracy: 0.501940, F1 Score: 0.075578
Remove Test Avg. Loss: 0.003166, Accuracy: 0.961800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179603, Accuracy: 0.501073, F1 Score: 0.072604
</noise>
</deltagrad>
<deltagrad period="75">
data dimension:: [400000, 2001]
overhead2:: 0.31990528106689453
overhead3:: 2.4858384132385254
overhead4:: 0.3746509552001953
overhead5:: 0
memory usage:: 20386279424
time_deltagrad:: 56.16527318954468
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(1.1920, dtype=torch.float64)
Test Avg. Loss: 0.000789, Accuracy: 0.860190, F1 Score: 0.869296
Remove Test Avg. Loss: 0.000948, Accuracy: 0.789525, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000785, Accuracy: 0.862313, F1 Score: 0.871350
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.7299, dtype=torch.float64)
Test Avg. Loss: 0.000791, Accuracy: 0.859580, F1 Score: 0.868759
Remove Test Avg. Loss: 0.000953, Accuracy: 0.788725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000787, Accuracy: 0.861605, F1 Score: 0.870724
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.7208, dtype=torch.float64)
Test Avg. Loss: 0.000929, Accuracy: 0.784070, F1 Score: 0.798417
Remove Test Avg. Loss: 0.001143, Accuracy: 0.712300, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000923, Accuracy: 0.785590, F1 Score: 0.799732
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7506, dtype=torch.float64)
Test Avg. Loss: 0.005960, Accuracy: 0.550040, F1 Score: 0.559208
Remove Test Avg. Loss: 0.007062, Accuracy: 0.528650, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005938, Accuracy: 0.552055, F1 Score: 0.560573
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1631, dtype=torch.float64)
Test Avg. Loss: 0.065439, Accuracy: 0.510610, F1 Score: 0.514826
Remove Test Avg. Loss: 0.075247, Accuracy: 0.500550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065218, Accuracy: 0.512055, F1 Score: 0.515776
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(1.1847, dtype=torch.float64)
Test Avg. Loss: 0.000789, Accuracy: 0.860640, F1 Score: 0.869596
Remove Test Avg. Loss: 0.000946, Accuracy: 0.790275, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000785, Accuracy: 0.862710, F1 Score: 0.871635
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.7323, dtype=torch.float64)
Test Avg. Loss: 0.000790, Accuracy: 0.863230, F1 Score: 0.871189
Remove Test Avg. Loss: 0.000935, Accuracy: 0.799350, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000785, Accuracy: 0.864885, F1 Score: 0.872851
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9366, dtype=torch.float64)
Test Avg. Loss: 0.000912, Accuracy: 0.789800, F1 Score: 0.788935
Remove Test Avg. Loss: 0.000945, Accuracy: 0.792375, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000902, Accuracy: 0.792550, F1 Score: 0.792061
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.1017, dtype=torch.float64)
Test Avg. Loss: 0.006195, Accuracy: 0.529450, F1 Score: 0.468431
Remove Test Avg. Loss: 0.004724, Accuracy: 0.642950, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006087, Accuracy: 0.534273, F1 Score: 0.475214
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8672, dtype=torch.float64)
Test Avg. Loss: 0.069161, Accuracy: 0.491130, F1 Score: 0.418268
Remove Test Avg. Loss: 0.051103, Accuracy: 0.614850, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068013, Accuracy: 0.495430, F1 Score: 0.424266
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(1.1702, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.862850, F1 Score: 0.871081
Remove Test Avg. Loss: 0.000934, Accuracy: 0.797300, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.864888, F1 Score: 0.873097
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.5976, dtype=torch.float64)
Test Avg. Loss: 0.000780, Accuracy: 0.876150, F1 Score: 0.877794
Remove Test Avg. Loss: 0.000825, Accuracy: 0.858825, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000775, Accuracy: 0.877398, F1 Score: 0.879298
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7886, dtype=torch.float64)
Test Avg. Loss: 0.001339, Accuracy: 0.642670, F1 Score: 0.465259
Remove Test Avg. Loss: 0.000271, Accuracy: 0.974100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001330, Accuracy: 0.644430, F1 Score: 0.468755
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8552, dtype=torch.float64)
Test Avg. Loss: 0.016983, Accuracy: 0.510720, F1 Score: 0.098401
Remove Test Avg. Loss: 0.000286, Accuracy: 0.967350, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016939, Accuracy: 0.509880, F1 Score: 0.094759
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6378, dtype=torch.float64)
Test Avg. Loss: 0.179917, Accuracy: 0.501930, F1 Score: 0.075577
Remove Test Avg. Loss: 0.003167, Accuracy: 0.961775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179574, Accuracy: 0.501073, F1 Score: 0.072638
</noise>
</deltagrad>
<deltagrad period="100">
data dimension:: [400000, 2001]
overhead2:: 0.23884844779968262
overhead3:: 2.336221694946289
overhead4:: 0.3129613399505615
overhead5:: 0
memory usage:: 20384190464
time_deltagrad:: 56.35835599899292
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(1.8439, dtype=torch.float64)
Test Avg. Loss: 0.000791, Accuracy: 0.867010, F1 Score: 0.874513
Remove Test Avg. Loss: 0.000944, Accuracy: 0.803650, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000786, Accuracy: 0.867973, F1 Score: 0.875583
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.9493, dtype=torch.float64)
Test Avg. Loss: 0.000794, Accuracy: 0.864720, F1 Score: 0.872370
Remove Test Avg. Loss: 0.000951, Accuracy: 0.800525, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000789, Accuracy: 0.865880, F1 Score: 0.873648
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.7598, dtype=torch.float64)
Test Avg. Loss: 0.000939, Accuracy: 0.781560, F1 Score: 0.794328
Remove Test Avg. Loss: 0.001150, Accuracy: 0.717100, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000931, Accuracy: 0.782887, F1 Score: 0.795567
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7703, dtype=torch.float64)
Test Avg. Loss: 0.005984, Accuracy: 0.549540, F1 Score: 0.558000
Remove Test Avg. Loss: 0.007089, Accuracy: 0.529800, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005962, Accuracy: 0.551735, F1 Score: 0.559553
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1809, dtype=torch.float64)
Test Avg. Loss: 0.065466, Accuracy: 0.510590, F1 Score: 0.514739
Remove Test Avg. Loss: 0.075276, Accuracy: 0.500725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065243, Accuracy: 0.512068, F1 Score: 0.515725
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(1.8417, dtype=torch.float64)
Test Avg. Loss: 0.000790, Accuracy: 0.867520, F1 Score: 0.874894
Remove Test Avg. Loss: 0.000942, Accuracy: 0.804550, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000786, Accuracy: 0.868340, F1 Score: 0.875847
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.9610, dtype=torch.float64)
Test Avg. Loss: 0.000792, Accuracy: 0.868200, F1 Score: 0.874712
Remove Test Avg. Loss: 0.000933, Accuracy: 0.811000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000787, Accuracy: 0.868672, F1 Score: 0.875389
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9855, dtype=torch.float64)
Test Avg. Loss: 0.000924, Accuracy: 0.781250, F1 Score: 0.778456
Remove Test Avg. Loss: 0.000952, Accuracy: 0.792625, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000913, Accuracy: 0.784310, F1 Score: 0.781946
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.1315, dtype=torch.float64)
Test Avg. Loss: 0.006229, Accuracy: 0.529190, F1 Score: 0.467825
Remove Test Avg. Loss: 0.004757, Accuracy: 0.643025, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006120, Accuracy: 0.534020, F1 Score: 0.474695
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8951, dtype=torch.float64)
Test Avg. Loss: 0.069196, Accuracy: 0.491160, F1 Score: 0.418243
Remove Test Avg. Loss: 0.051139, Accuracy: 0.614925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068048, Accuracy: 0.495418, F1 Score: 0.424228
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(1.8220, dtype=torch.float64)
Test Avg. Loss: 0.000789, Accuracy: 0.868650, F1 Score: 0.875371
Remove Test Avg. Loss: 0.000931, Accuracy: 0.811450, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000784, Accuracy: 0.869975, F1 Score: 0.876818
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.7931, dtype=torch.float64)
Test Avg. Loss: 0.000784, Accuracy: 0.872220, F1 Score: 0.872894
Remove Test Avg. Loss: 0.000824, Accuracy: 0.864450, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000779, Accuracy: 0.874150, F1 Score: 0.874948
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7963, dtype=torch.float64)
Test Avg. Loss: 0.001359, Accuracy: 0.644190, F1 Score: 0.472162
Remove Test Avg. Loss: 0.000281, Accuracy: 0.970075, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001350, Accuracy: 0.645868, F1 Score: 0.475190
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8431, dtype=torch.float64)
Test Avg. Loss: 0.016996, Accuracy: 0.510820, F1 Score: 0.100077
Remove Test Avg. Loss: 0.000294, Accuracy: 0.966600, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016951, Accuracy: 0.510073, F1 Score: 0.096671
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6238, dtype=torch.float64)
Test Avg. Loss: 0.179928, Accuracy: 0.501970, F1 Score: 0.075788
Remove Test Avg. Loss: 0.003176, Accuracy: 0.961700, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179584, Accuracy: 0.501100, F1 Score: 0.072814
</noise>
</deltagrad>
<deltagrad period="200">
data dimension:: [400000, 2001]
overhead2:: 0.12894511222839355
overhead3:: 2.3112382888793945
overhead4:: 0.1710972785949707
overhead5:: 0
memory usage:: 20384153600
time_deltagrad:: 56.31750774383545
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(1.1048, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.861960, F1 Score: 0.870029
Remove Test Avg. Loss: 0.000922, Accuracy: 0.798775, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.863812, F1 Score: 0.871886
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(4.6874, dtype=torch.float64)
Test Avg. Loss: 0.000788, Accuracy: 0.861200, F1 Score: 0.869375
Remove Test Avg. Loss: 0.000927, Accuracy: 0.797725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000784, Accuracy: 0.863125, F1 Score: 0.871249
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(45.6945, dtype=torch.float64)
Test Avg. Loss: 0.000923, Accuracy: 0.785900, F1 Score: 0.798888
Remove Test Avg. Loss: 0.001111, Accuracy: 0.722675, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000916, Accuracy: 0.787870, F1 Score: 0.800570
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(456.7260, dtype=torch.float64)
Test Avg. Loss: 0.005949, Accuracy: 0.549920, F1 Score: 0.558485
Remove Test Avg. Loss: 0.007013, Accuracy: 0.529700, F1 Score: 0.000000
Remain Test Avg. Loss: 0.005928, Accuracy: 0.552092, F1 Score: 0.560031
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(4567.1388, dtype=torch.float64)
Test Avg. Loss: 0.065430, Accuracy: 0.510630, F1 Score: 0.514779
Remove Test Avg. Loss: 0.075195, Accuracy: 0.500825, F1 Score: 0.000000
Remain Test Avg. Loss: 0.065208, Accuracy: 0.512055, F1 Score: 0.515711
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(1.1106, dtype=torch.float64)
Test Avg. Loss: 0.000786, Accuracy: 0.862240, F1 Score: 0.870202
Remove Test Avg. Loss: 0.000920, Accuracy: 0.799850, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000782, Accuracy: 0.864070, F1 Score: 0.872036
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(4.7220, dtype=torch.float64)
Test Avg. Loss: 0.000787, Accuracy: 0.864310, F1 Score: 0.871290
Remove Test Avg. Loss: 0.000909, Accuracy: 0.808000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000783, Accuracy: 0.866278, F1 Score: 0.873338
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(45.9435, dtype=torch.float64)
Test Avg. Loss: 0.000908, Accuracy: 0.791720, F1 Score: 0.789352
Remove Test Avg. Loss: 0.000916, Accuracy: 0.802725, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000899, Accuracy: 0.794327, F1 Score: 0.792285
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(459.1103, dtype=torch.float64)
Test Avg. Loss: 0.006191, Accuracy: 0.529450, F1 Score: 0.467360
Remove Test Avg. Loss: 0.004681, Accuracy: 0.644675, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006083, Accuracy: 0.534262, F1 Score: 0.474166
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(4590.8760, dtype=torch.float64)
Test Avg. Loss: 0.069158, Accuracy: 0.491090, F1 Score: 0.418116
Remove Test Avg. Loss: 0.051057, Accuracy: 0.614975, F1 Score: 0.000000
Remain Test Avg. Loss: 0.068010, Accuracy: 0.495425, F1 Score: 0.424185
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(1.0862, dtype=torch.float64)
Test Avg. Loss: 0.000785, Accuracy: 0.864000, F1 Score: 0.871258
Remove Test Avg. Loss: 0.000909, Accuracy: 0.806900, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000781, Accuracy: 0.866245, F1 Score: 0.873496
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(4.5656, dtype=torch.float64)
Test Avg. Loss: 0.000779, Accuracy: 0.877040, F1 Score: 0.877790
Remove Test Avg. Loss: 0.000801, Accuracy: 0.868075, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000775, Accuracy: 0.878302, F1 Score: 0.879287
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(44.7737, dtype=torch.float64)
Test Avg. Loss: 0.001353, Accuracy: 0.638140, F1 Score: 0.452391
Remove Test Avg. Loss: 0.000259, Accuracy: 0.977375, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001345, Accuracy: 0.639490, F1 Score: 0.454785
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(447.8421, dtype=torch.float64)
Test Avg. Loss: 0.017018, Accuracy: 0.510660, F1 Score: 0.097025
Remove Test Avg. Loss: 0.000280, Accuracy: 0.967925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.016974, Accuracy: 0.509715, F1 Score: 0.093198
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(4478.6249, dtype=torch.float64)
Test Avg. Loss: 0.179953, Accuracy: 0.501920, F1 Score: 0.075438
Remove Test Avg. Loss: 0.003160, Accuracy: 0.961850, F1 Score: 0.000000
Remain Test Avg. Loss: 0.179609, Accuracy: 0.501073, F1 Score: 0.072526
</noise>
</deltagrad>
</results>
</data>
