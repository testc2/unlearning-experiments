<data>
<generate>
</generate>
<training>
Train - Epoch 0, Batch: 0, Loss: 0.695010
Train - Epoch 0, Batch: 1, Loss: 0.691716
Train - Epoch 0, Batch: 2, Loss: 0.698302
Train - Epoch 0, Batch: 3, Loss: 0.693168
Train - Epoch 0, Batch: 4, Loss: 0.691853
Train - Epoch 0, Batch: 5, Loss: 0.690012
Train - Epoch 0, Batch: 6, Loss: 0.693955
Train - Epoch 0, Batch: 7, Loss: 0.691763
Train - Epoch 0, Batch: 8, Loss: 0.690955
Train - Epoch 0, Batch: 9, Loss: 0.690886
Train - Epoch 0, Batch: 10, Loss: 0.689377
Train - Epoch 0, Batch: 11, Loss: 0.687271
Train - Epoch 0, Batch: 12, Loss: 0.683330
Train - Epoch 0, Batch: 13, Loss: 0.686153
Train - Epoch 0, Batch: 14, Loss: 0.685538
Train - Epoch 0, Batch: 15, Loss: 0.684668
Train - Epoch 0, Batch: 16, Loss: 0.683772
Train - Epoch 0, Batch: 17, Loss: 0.688985
Train - Epoch 0, Batch: 18, Loss: 0.684070
Train - Epoch 0, Batch: 19, Loss: 0.682562
Train - Epoch 0, Batch: 20, Loss: 0.683542
Train - Epoch 0, Batch: 21, Loss: 0.684023
Train - Epoch 0, Batch: 22, Loss: 0.678753
Train - Epoch 0, Batch: 23, Loss: 0.679202
Train - Epoch 0, Batch: 24, Loss: 0.683569
Train - Epoch 0, Batch: 25, Loss: 0.681252
Train - Epoch 0, Batch: 26, Loss: 0.682335
Train - Epoch 0, Batch: 27, Loss: 0.678679
Train - Epoch 0, Batch: 28, Loss: 0.681889
Train - Epoch 0, Batch: 29, Loss: 0.678093
Train - Epoch 0, Batch: 30, Loss: 0.677210
Train - Epoch 0, Batch: 31, Loss: 0.676420
Train - Epoch 0, Batch: 32, Loss: 0.681333
Train - Epoch 0, Batch: 33, Loss: 0.682154
Train - Epoch 0, Batch: 34, Loss: 0.679801
Train - Epoch 0, Batch: 35, Loss: 0.675368
Train - Epoch 0, Batch: 36, Loss: 0.678033
Train - Epoch 0, Batch: 37, Loss: 0.678001
Train - Epoch 0, Batch: 38, Loss: 0.675866
Train - Epoch 0, Batch: 39, Loss: 0.675504
Train - Epoch 0, Batch: 40, Loss: 0.671528
Train - Epoch 0, Batch: 41, Loss: 0.672723
Train - Epoch 0, Batch: 42, Loss: 0.673858
Train - Epoch 0, Batch: 43, Loss: 0.674499
Train - Epoch 0, Batch: 44, Loss: 0.674349
Train - Epoch 0, Batch: 45, Loss: 0.674527
Train - Epoch 0, Batch: 46, Loss: 0.668979
Train - Epoch 0, Batch: 47, Loss: 0.668183
Train - Epoch 0, Batch: 48, Loss: 0.671026
Train - Epoch 0, Batch: 49, Loss: 0.674491
Train - Epoch 0, Batch: 50, Loss: 0.670824
Train - Epoch 0, Batch: 51, Loss: 0.670101
Train - Epoch 0, Batch: 52, Loss: 0.670211
Train - Epoch 0, Batch: 53, Loss: 0.672382
Train - Epoch 0, Batch: 54, Loss: 0.667905
Train - Epoch 0, Batch: 55, Loss: 0.673014
Train - Epoch 0, Batch: 56, Loss: 0.665153
Train - Epoch 0, Batch: 57, Loss: 0.669382
Train - Epoch 0, Batch: 58, Loss: 0.664635
Train - Epoch 0, Batch: 59, Loss: 0.664440
Train - Epoch 0, Batch: 60, Loss: 0.667268
Train - Epoch 0, Batch: 61, Loss: 0.665725
Train - Epoch 0, Batch: 62, Loss: 0.670271
Train - Epoch 0, Batch: 63, Loss: 0.670468
Train - Epoch 0, Batch: 64, Loss: 0.667396
Train - Epoch 0, Batch: 65, Loss: 0.663636
Train - Epoch 0, Batch: 66, Loss: 0.664490
Train - Epoch 0, Batch: 67, Loss: 0.670462
Train - Epoch 0, Batch: 68, Loss: 0.662599
Train - Epoch 0, Batch: 69, Loss: 0.663282
Train - Epoch 0, Batch: 70, Loss: 0.661481
Train - Epoch 0, Batch: 71, Loss: 0.660357
Train - Epoch 0, Batch: 72, Loss: 0.659783
Train - Epoch 0, Batch: 73, Loss: 0.663727
Train - Epoch 0, Batch: 74, Loss: 0.664516
Train - Epoch 0, Batch: 75, Loss: 0.662565
Train - Epoch 0, Batch: 76, Loss: 0.661032
Train - Epoch 0, Batch: 77, Loss: 0.656561
Train - Epoch 0, Batch: 78, Loss: 0.657950
Train - Epoch 0, Batch: 79, Loss: 0.663242
Train - Epoch 0, Batch: 80, Loss: 0.660536
Train - Epoch 0, Batch: 81, Loss: 0.650823
Train - Epoch 0, Batch: 82, Loss: 0.666444
Train - Epoch 0, Batch: 83, Loss: 0.659022
Train - Epoch 0, Batch: 84, Loss: 0.657270
Train - Epoch 0, Batch: 85, Loss: 0.661599
Train - Epoch 0, Batch: 86, Loss: 0.658665
Train - Epoch 0, Batch: 87, Loss: 0.659222
Train - Epoch 0, Batch: 88, Loss: 0.650277
Train - Epoch 0, Batch: 89, Loss: 0.659423
Train - Epoch 0, Batch: 90, Loss: 0.664819
Train - Epoch 0, Batch: 91, Loss: 0.658398
Train - Epoch 0, Batch: 92, Loss: 0.654987
Train - Epoch 0, Batch: 93, Loss: 0.662314
Train - Epoch 0, Batch: 94, Loss: 0.659162
Train - Epoch 0, Batch: 95, Loss: 0.657818
Train - Epoch 0, Batch: 96, Loss: 0.653172
Train - Epoch 0, Batch: 97, Loss: 0.656179
Train - Epoch 0, Batch: 98, Loss: 0.656277
Train - Epoch 0, Batch: 99, Loss: 0.663259
Train - Epoch 0, Batch: 100, Loss: 0.658705
Train - Epoch 0, Batch: 101, Loss: 0.654295
Train - Epoch 0, Batch: 102, Loss: 0.651543
Train - Epoch 0, Batch: 103, Loss: 0.656346
Train - Epoch 0, Batch: 104, Loss: 0.653368
Train - Epoch 0, Batch: 105, Loss: 0.649814
Train - Epoch 0, Batch: 106, Loss: 0.652155
Train - Epoch 0, Batch: 107, Loss: 0.659800
Train - Epoch 0, Batch: 108, Loss: 0.653236
Train - Epoch 0, Batch: 109, Loss: 0.651570
Train - Epoch 0, Batch: 110, Loss: 0.650047
Train - Epoch 0, Batch: 111, Loss: 0.658197
Train - Epoch 0, Batch: 112, Loss: 0.648463
Train - Epoch 0, Batch: 113, Loss: 0.659081
Train - Epoch 0, Batch: 114, Loss: 0.651455
Train - Epoch 0, Batch: 115, Loss: 0.654431
Train - Epoch 0, Batch: 116, Loss: 0.649280
Train - Epoch 0, Batch: 117, Loss: 0.648523
Train - Epoch 0, Batch: 118, Loss: 0.658356
Train - Epoch 0, Batch: 119, Loss: 0.652201
Train - Epoch 0, Batch: 120, Loss: 0.653398
Train - Epoch 0, Batch: 121, Loss: 0.641397
Train - Epoch 0, Batch: 122, Loss: 0.645253
Train - Epoch 0, Batch: 123, Loss: 0.636848
Train - Epoch 0, Batch: 124, Loss: 0.646567
Train - Epoch 0, Batch: 125, Loss: 0.644465
Train - Epoch 0, Batch: 126, Loss: 0.652616
Train - Epoch 0, Batch: 127, Loss: 0.650626
Train - Epoch 0, Batch: 128, Loss: 0.655014
Train - Epoch 0, Batch: 129, Loss: 0.645164
Train - Epoch 0, Batch: 130, Loss: 0.649377
Train - Epoch 0, Batch: 131, Loss: 0.642908
Train - Epoch 0, Batch: 132, Loss: 0.651180
Train - Epoch 0, Batch: 133, Loss: 0.654369
Train - Epoch 0, Batch: 134, Loss: 0.656658
Train - Epoch 0, Batch: 135, Loss: 0.639388
Train - Epoch 0, Batch: 136, Loss: 0.653243
Train - Epoch 0, Batch: 137, Loss: 0.628773
Train - Epoch 0, Batch: 138, Loss: 0.641383
Train - Epoch 0, Batch: 139, Loss: 0.652193
Train - Epoch 0, Batch: 140, Loss: 0.649105
Train - Epoch 0, Batch: 141, Loss: 0.645788
Train - Epoch 0, Batch: 142, Loss: 0.656222
Train - Epoch 0, Batch: 143, Loss: 0.652292
Train - Epoch 0, Batch: 144, Loss: 0.639930
Train - Epoch 0, Batch: 145, Loss: 0.643566
Train - Epoch 0, Batch: 146, Loss: 0.653040
Train - Epoch 0, Batch: 147, Loss: 0.642653
Train - Epoch 0, Batch: 148, Loss: 0.652678
Train - Epoch 0, Batch: 149, Loss: 0.640211
Train - Epoch 0, Batch: 150, Loss: 0.644563
Train - Epoch 0, Batch: 151, Loss: 0.650714
Train - Epoch 0, Batch: 152, Loss: 0.643356
Train - Epoch 0, Batch: 153, Loss: 0.648411
Train - Epoch 0, Batch: 154, Loss: 0.636715
Train - Epoch 0, Batch: 155, Loss: 0.657522
Train - Epoch 0, Batch: 156, Loss: 0.635734
Train - Epoch 0, Batch: 157, Loss: 0.649585
Train - Epoch 0, Batch: 158, Loss: 0.642365
Train - Epoch 0, Batch: 159, Loss: 0.640020
Train - Epoch 0, Batch: 160, Loss: 0.643794
Train - Epoch 0, Batch: 161, Loss: 0.642947
Train - Epoch 0, Batch: 162, Loss: 0.644925
Train - Epoch 0, Batch: 163, Loss: 0.641981
Train - Epoch 0, Batch: 164, Loss: 0.640464
Train - Epoch 0, Batch: 165, Loss: 0.640387
Train - Epoch 0, Batch: 166, Loss: 0.647269
Train - Epoch 0, Batch: 167, Loss: 0.644878
Train - Epoch 0, Batch: 168, Loss: 0.644890
Train - Epoch 0, Batch: 169, Loss: 0.621873
Train - Epoch 0, Batch: 170, Loss: 0.633747
Train - Epoch 0, Batch: 171, Loss: 0.636635
Train - Epoch 0, Batch: 172, Loss: 0.652661
Train - Epoch 0, Batch: 173, Loss: 0.641039
Train - Epoch 0, Batch: 174, Loss: 0.643678
Train - Epoch 0, Batch: 175, Loss: 0.635016
Train - Epoch 0, Batch: 176, Loss: 0.647464
Train - Epoch 0, Batch: 177, Loss: 0.636222
Train - Epoch 0, Batch: 178, Loss: 0.641480
Train - Epoch 0, Batch: 179, Loss: 0.630200
Train - Epoch 0, Batch: 180, Loss: 0.622944
Train - Epoch 0, Batch: 181, Loss: 0.639988
Train - Epoch 0, Batch: 182, Loss: 0.645520
Train - Epoch 0, Batch: 183, Loss: 0.636981
Train - Epoch 0, Batch: 184, Loss: 0.646951
Train - Epoch 0, Batch: 185, Loss: 0.633980
Train - Epoch 0, Batch: 186, Loss: 0.637850
Train - Epoch 0, Batch: 187, Loss: 0.632948
Train - Epoch 0, Batch: 188, Loss: 0.640462
Train - Epoch 0, Batch: 189, Loss: 0.629091
Train - Epoch 0, Batch: 190, Loss: 0.646797
Train - Epoch 0, Batch: 191, Loss: 0.639788
Train - Epoch 0, Batch: 192, Loss: 0.645688
Train - Epoch 0, Batch: 193, Loss: 0.627415
Train - Epoch 0, Batch: 194, Loss: 0.637097
Train - Epoch 0, Batch: 195, Loss: 0.638392
Train - Epoch 0, Batch: 196, Loss: 0.625803
Train - Epoch 0, Batch: 197, Loss: 0.626545
Train - Epoch 0, Batch: 198, Loss: 0.634409
Train - Epoch 0, Batch: 199, Loss: 0.647503
Train - Epoch 0, Batch: 200, Loss: 0.639800
Train - Epoch 0, Batch: 201, Loss: 0.639068
Train - Epoch 0, Batch: 202, Loss: 0.622676
Train - Epoch 0, Batch: 203, Loss: 0.638962
Train - Epoch 0, Batch: 204, Loss: 0.626968
Train - Epoch 0, Batch: 205, Loss: 0.623116
Train - Epoch 0, Batch: 206, Loss: 0.630804
Train - Epoch 0, Batch: 207, Loss: 0.632836
Train - Epoch 0, Batch: 208, Loss: 0.644933
Train - Epoch 0, Batch: 209, Loss: 0.622611
Train - Epoch 0, Batch: 210, Loss: 0.634245
Train - Epoch 0, Batch: 211, Loss: 0.620047
Train - Epoch 0, Batch: 212, Loss: 0.646474
Train - Epoch 0, Batch: 213, Loss: 0.622954
Train - Epoch 0, Batch: 214, Loss: 0.630761
Train - Epoch 0, Batch: 215, Loss: 0.636946
Train - Epoch 0, Batch: 216, Loss: 0.631595
Train - Epoch 0, Batch: 217, Loss: 0.635816
Train - Epoch 0, Batch: 218, Loss: 0.620395
Train - Epoch 0, Batch: 219, Loss: 0.631676
Train - Epoch 0, Batch: 220, Loss: 0.632932
Train - Epoch 0, Batch: 221, Loss: 0.637755
Train - Epoch 0, Batch: 222, Loss: 0.627185
Train - Epoch 0, Batch: 223, Loss: 0.637237
Train - Epoch 0, Batch: 224, Loss: 0.631833
Train - Epoch 0, Batch: 225, Loss: 0.631928
Train - Epoch 0, Batch: 226, Loss: 0.631968
Train - Epoch 0, Batch: 227, Loss: 0.624824
Train - Epoch 0, Batch: 228, Loss: 0.641426
Train - Epoch 0, Batch: 229, Loss: 0.613111
Train - Epoch 0, Batch: 230, Loss: 0.645549
Train - Epoch 0, Batch: 231, Loss: 0.628760
Train - Epoch 0, Batch: 232, Loss: 0.643135
Train - Epoch 0, Batch: 233, Loss: 0.638180
Train - Epoch 0, Batch: 234, Loss: 0.634949
Train - Epoch 0, Batch: 235, Loss: 0.629800
Train - Epoch 0, Batch: 236, Loss: 0.634744
Train - Epoch 0, Batch: 237, Loss: 0.623439
Train - Epoch 0, Batch: 238, Loss: 0.634372
Train - Epoch 0, Batch: 239, Loss: 0.633972
Train - Epoch 0, Batch: 240, Loss: 0.637134
Train - Epoch 0, Batch: 241, Loss: 0.622537
Train - Epoch 0, Batch: 242, Loss: 0.632849
Train - Epoch 0, Batch: 243, Loss: 0.619791
Train - Epoch 0, Batch: 244, Loss: 0.623512
Train - Epoch 0, Batch: 245, Loss: 0.632576
Train - Epoch 0, Batch: 246, Loss: 0.625513
Train - Epoch 0, Batch: 247, Loss: 0.611328
Train - Epoch 0, Batch: 248, Loss: 0.629039
Train - Epoch 0, Batch: 249, Loss: 0.630918
Train - Epoch 0, Batch: 250, Loss: 0.622656
Train - Epoch 0, Batch: 251, Loss: 0.646014
Train - Epoch 0, Batch: 252, Loss: 0.623136
Train - Epoch 0, Batch: 253, Loss: 0.620947
Train - Epoch 0, Batch: 254, Loss: 0.634553
Train - Epoch 0, Batch: 255, Loss: 0.625354
Train - Epoch 0, Batch: 256, Loss: 0.627200
Train - Epoch 0, Batch: 257, Loss: 0.630781
Train - Epoch 0, Batch: 258, Loss: 0.627716
Train - Epoch 0, Batch: 259, Loss: 0.626347
Train - Epoch 0, Batch: 260, Loss: 0.627199
Train - Epoch 0, Batch: 261, Loss: 0.628832
Train - Epoch 0, Batch: 262, Loss: 0.643742
Train - Epoch 0, Batch: 263, Loss: 0.619357
Train - Epoch 0, Batch: 264, Loss: 0.620714
Train - Epoch 0, Batch: 265, Loss: 0.638776
Train - Epoch 0, Batch: 266, Loss: 0.623938
Train - Epoch 0, Batch: 267, Loss: 0.631664
Train - Epoch 0, Batch: 268, Loss: 0.631931
Train - Epoch 0, Batch: 269, Loss: 0.625362
Train - Epoch 0, Batch: 270, Loss: 0.624726
Train - Epoch 0, Batch: 271, Loss: 0.618845
Train - Epoch 0, Batch: 272, Loss: 0.631466
Train - Epoch 0, Batch: 273, Loss: 0.629596
Train - Epoch 0, Batch: 274, Loss: 0.635303
Train - Epoch 0, Batch: 275, Loss: 0.617593
Train - Epoch 0, Batch: 276, Loss: 0.621476
Train - Epoch 0, Batch: 277, Loss: 0.637317
Train - Epoch 0, Batch: 278, Loss: 0.629989
Train - Epoch 0, Batch: 279, Loss: 0.631352
Train - Epoch 0, Batch: 280, Loss: 0.630607
Train - Epoch 0, Batch: 281, Loss: 0.629226
Train - Epoch 0, Batch: 282, Loss: 0.626539
Train - Epoch 0, Batch: 283, Loss: 0.612735
Train - Epoch 0, Batch: 284, Loss: 0.618949
Train - Epoch 0, Batch: 285, Loss: 0.619268
Train - Epoch 0, Batch: 286, Loss: 0.634213
Train - Epoch 0, Batch: 287, Loss: 0.626670
Train - Epoch 0, Batch: 288, Loss: 0.600967
Train - Epoch 0, Batch: 289, Loss: 0.623752
Train - Epoch 0, Batch: 290, Loss: 0.614087
Train - Epoch 0, Batch: 291, Loss: 0.622238
Train - Epoch 0, Batch: 292, Loss: 0.615773
Train - Epoch 0, Batch: 293, Loss: 0.616133
Train - Epoch 0, Batch: 294, Loss: 0.623523
Train - Epoch 0, Batch: 295, Loss: 0.628931
Train - Epoch 0, Batch: 296, Loss: 0.618238
Train - Epoch 0, Batch: 297, Loss: 0.609050
Train - Epoch 0, Batch: 298, Loss: 0.618639
Train - Epoch 0, Batch: 299, Loss: 0.631975
Train - Epoch 0, Batch: 300, Loss: 0.626985
Train - Epoch 0, Batch: 301, Loss: 0.627996
Train - Epoch 0, Batch: 302, Loss: 0.616759
Train - Epoch 0, Batch: 303, Loss: 0.622599
Train - Epoch 0, Batch: 304, Loss: 0.615279
Train - Epoch 0, Batch: 305, Loss: 0.623449
Train - Epoch 0, Batch: 306, Loss: 0.615365
Train - Epoch 0, Batch: 307, Loss: 0.607265
Train - Epoch 0, Batch: 308, Loss: 0.625033
Train - Epoch 0, Batch: 309, Loss: 0.625939
Train - Epoch 0, Batch: 310, Loss: 0.623911
Train - Epoch 0, Batch: 311, Loss: 0.608769
Train - Epoch 0, Batch: 312, Loss: 0.610730
Train - Epoch 0, Batch: 313, Loss: 0.608385
Train - Epoch 0, Batch: 314, Loss: 0.619624
Train - Epoch 0, Batch: 315, Loss: 0.616194
Train - Epoch 0, Batch: 316, Loss: 0.615317
Train - Epoch 0, Batch: 317, Loss: 0.613224
Train - Epoch 0, Batch: 318, Loss: 0.630274
Train - Epoch 0, Batch: 319, Loss: 0.612205
Train - Epoch 0, Batch: 320, Loss: 0.610023
Train - Epoch 0, Batch: 321, Loss: 0.615586
Train - Epoch 0, Batch: 322, Loss: 0.618797
Train - Epoch 0, Batch: 323, Loss: 0.613735
Train - Epoch 0, Batch: 324, Loss: 0.612163
Train - Epoch 0, Batch: 325, Loss: 0.617741
Train - Epoch 0, Batch: 326, Loss: 0.610869
Train - Epoch 0, Batch: 327, Loss: 0.617613
Train - Epoch 0, Batch: 328, Loss: 0.619169
Train - Epoch 0, Batch: 329, Loss: 0.607992
Train - Epoch 0, Batch: 330, Loss: 0.604818
Train - Epoch 0, Batch: 331, Loss: 0.615788
Train - Epoch 0, Batch: 332, Loss: 0.622870
Train - Epoch 0, Batch: 333, Loss: 0.623623
Train - Epoch 0, Batch: 334, Loss: 0.615430
Train - Epoch 0, Batch: 335, Loss: 0.630818
Train - Epoch 0, Batch: 336, Loss: 0.624272
Train - Epoch 0, Batch: 337, Loss: 0.613448
Train - Epoch 0, Batch: 338, Loss: 0.621539
Train - Epoch 0, Batch: 339, Loss: 0.611154
Train - Epoch 0, Batch: 340, Loss: 0.610614
Train - Epoch 0, Batch: 341, Loss: 0.610616
Train - Epoch 0, Batch: 342, Loss: 0.606614
Train - Epoch 0, Batch: 343, Loss: 0.615537
Train - Epoch 0, Batch: 344, Loss: 0.618909
Train - Epoch 0, Batch: 345, Loss: 0.603539
Train - Epoch 0, Batch: 346, Loss: 0.640189
Train - Epoch 0, Batch: 347, Loss: 0.611945
Train - Epoch 0, Batch: 348, Loss: 0.606285
Train - Epoch 0, Batch: 349, Loss: 0.626788
Train - Epoch 0, Batch: 350, Loss: 0.619301
Train - Epoch 0, Batch: 351, Loss: 0.619080
Train - Epoch 0, Batch: 352, Loss: 0.603114
Train - Epoch 0, Batch: 353, Loss: 0.617252
Train - Epoch 0, Batch: 354, Loss: 0.620366
Train - Epoch 0, Batch: 355, Loss: 0.605719
Train - Epoch 0, Batch: 356, Loss: 0.607103
Train - Epoch 0, Batch: 357, Loss: 0.612118
Train - Epoch 0, Batch: 358, Loss: 0.608834
Train - Epoch 0, Batch: 359, Loss: 0.614846
Train - Epoch 0, Batch: 360, Loss: 0.608948
Train - Epoch 0, Batch: 361, Loss: 0.608881
Train - Epoch 0, Batch: 362, Loss: 0.619295
Train - Epoch 0, Batch: 363, Loss: 0.602903
Train - Epoch 0, Batch: 364, Loss: 0.603151
Train - Epoch 0, Batch: 365, Loss: 0.605213
Train - Epoch 0, Batch: 366, Loss: 0.611627
Train - Epoch 0, Batch: 367, Loss: 0.610051
Train - Epoch 0, Batch: 368, Loss: 0.615792
Train - Epoch 0, Batch: 369, Loss: 0.616705
Train - Epoch 0, Batch: 370, Loss: 0.615845
Train - Epoch 0, Batch: 371, Loss: 0.619968
Train - Epoch 0, Batch: 372, Loss: 0.608468
Train - Epoch 0, Batch: 373, Loss: 0.602297
Train - Epoch 0, Batch: 374, Loss: 0.596083
Train - Epoch 0, Batch: 375, Loss: 0.600529
Train - Epoch 0, Batch: 376, Loss: 0.614569
Train - Epoch 0, Batch: 377, Loss: 0.617750
Train - Epoch 0, Batch: 378, Loss: 0.626244
Train - Epoch 0, Batch: 379, Loss: 0.606859
Train - Epoch 0, Batch: 380, Loss: 0.606856
Train - Epoch 0, Batch: 381, Loss: 0.618647
Train - Epoch 0, Batch: 382, Loss: 0.596566
Train - Epoch 0, Batch: 383, Loss: 0.619697
Train - Epoch 0, Batch: 384, Loss: 0.611580
Train - Epoch 0, Batch: 385, Loss: 0.608757
Train - Epoch 0, Batch: 386, Loss: 0.610257
Train - Epoch 0, Batch: 387, Loss: 0.600032
Train - Epoch 0, Batch: 388, Loss: 0.630321
Train - Epoch 0, Batch: 389, Loss: 0.625589
Train - Epoch 0, Batch: 390, Loss: 0.610510
Train - Epoch 0, Batch: 391, Loss: 0.613758
Train - Epoch 0, Batch: 392, Loss: 0.615350
Train - Epoch 0, Batch: 393, Loss: 0.605828
Train - Epoch 0, Batch: 394, Loss: 0.606167
Train - Epoch 0, Batch: 395, Loss: 0.609449
Train - Epoch 0, Batch: 396, Loss: 0.614139
Train - Epoch 0, Batch: 397, Loss: 0.630367
Train - Epoch 0, Batch: 398, Loss: 0.607775
Train - Epoch 0, Batch: 399, Loss: 0.610896
Train - Epoch 0, Batch: 400, Loss: 0.599913
Train - Epoch 0, Batch: 401, Loss: 0.613245
Train - Epoch 0, Batch: 402, Loss: 0.614908
Train - Epoch 0, Batch: 403, Loss: 0.596080
Train - Epoch 0, Batch: 404, Loss: 0.609839
Train - Epoch 0, Batch: 405, Loss: 0.613782
Train - Epoch 0, Batch: 406, Loss: 0.633616
Train - Epoch 0, Batch: 407, Loss: 0.617161
Train - Epoch 0, Batch: 408, Loss: 0.613814
Train - Epoch 0, Batch: 409, Loss: 0.621864
Train - Epoch 0, Batch: 410, Loss: 0.611585
Train - Epoch 0, Batch: 411, Loss: 0.606351
Train - Epoch 0, Batch: 412, Loss: 0.602174
Train - Epoch 0, Batch: 413, Loss: 0.616033
Train - Epoch 0, Batch: 414, Loss: 0.615893
Train - Epoch 0, Batch: 415, Loss: 0.611390
Train - Epoch 0, Batch: 416, Loss: 0.613286
Train - Epoch 0, Batch: 417, Loss: 0.605887
Train - Epoch 0, Batch: 418, Loss: 0.610760
Train - Epoch 0, Batch: 419, Loss: 0.597770
Train - Epoch 0, Batch: 420, Loss: 0.613123
Train - Epoch 0, Batch: 421, Loss: 0.605278
Train - Epoch 0, Batch: 422, Loss: 0.613827
Train - Epoch 0, Batch: 423, Loss: 0.608083
Train - Epoch 0, Batch: 424, Loss: 0.587444
Train - Epoch 0, Batch: 425, Loss: 0.620053
Train - Epoch 0, Batch: 426, Loss: 0.613821
Train - Epoch 0, Batch: 427, Loss: 0.614099
Train - Epoch 0, Batch: 428, Loss: 0.601692
Train - Epoch 0, Batch: 429, Loss: 0.584165
Train - Epoch 0, Batch: 430, Loss: 0.605260
Train - Epoch 0, Batch: 431, Loss: 0.606948
Train - Epoch 0, Batch: 432, Loss: 0.612445
Train - Epoch 0, Batch: 433, Loss: 0.606945
Train - Epoch 0, Batch: 434, Loss: 0.605455
Train - Epoch 0, Batch: 435, Loss: 0.606252
Train - Epoch 0, Batch: 436, Loss: 0.598550
Train - Epoch 0, Batch: 437, Loss: 0.619913
Train - Epoch 0, Batch: 438, Loss: 0.630036
Train - Epoch 0, Batch: 439, Loss: 0.612209
Train - Epoch 0, Batch: 440, Loss: 0.610900
Train - Epoch 0, Batch: 441, Loss: 0.628879
Train - Epoch 0, Batch: 442, Loss: 0.611101
Train - Epoch 0, Batch: 443, Loss: 0.624395
Train - Epoch 0, Batch: 444, Loss: 0.591900
Train - Epoch 0, Batch: 445, Loss: 0.611316
Train - Epoch 0, Batch: 446, Loss: 0.596674
Train - Epoch 0, Batch: 447, Loss: 0.602853
Train - Epoch 0, Batch: 448, Loss: 0.609122
Train - Epoch 0, Batch: 449, Loss: 0.601352
Train - Epoch 0, Batch: 450, Loss: 0.619826
Train - Epoch 0, Batch: 451, Loss: 0.585448
Train - Epoch 0, Batch: 452, Loss: 0.607843
Train - Epoch 0, Batch: 453, Loss: 0.608028
Train - Epoch 0, Batch: 454, Loss: 0.594004
Train - Epoch 0, Batch: 455, Loss: 0.611157
Train - Epoch 0, Batch: 456, Loss: 0.597065
Train - Epoch 0, Batch: 457, Loss: 0.636051
Train - Epoch 0, Batch: 458, Loss: 0.577391
Train - Epoch 0, Batch: 459, Loss: 0.614756
Train - Epoch 0, Batch: 460, Loss: 0.595815
Train - Epoch 0, Batch: 461, Loss: 0.600604
Train - Epoch 0, Batch: 462, Loss: 0.625207
Train - Epoch 0, Batch: 463, Loss: 0.612984
Train - Epoch 0, Batch: 464, Loss: 0.590694
Train - Epoch 0, Batch: 465, Loss: 0.621071
Train - Epoch 0, Batch: 466, Loss: 0.600185
Train - Epoch 0, Batch: 467, Loss: 0.595839
Train - Epoch 0, Batch: 468, Loss: 0.611209
Train - Epoch 0, Batch: 469, Loss: 0.604116
Train - Epoch 0, Batch: 470, Loss: 0.618621
Train - Epoch 0, Batch: 471, Loss: 0.601575
Train - Epoch 0, Batch: 472, Loss: 0.588381
Train - Epoch 0, Batch: 473, Loss: 0.597615
Train - Epoch 0, Batch: 474, Loss: 0.605694
Train - Epoch 0, Batch: 475, Loss: 0.589938
Train - Epoch 0, Batch: 476, Loss: 0.593299
Train - Epoch 0, Batch: 477, Loss: 0.611222
Train - Epoch 0, Batch: 478, Loss: 0.591262
Train - Epoch 0, Batch: 479, Loss: 0.602495
Train - Epoch 0, Batch: 480, Loss: 0.612516
Train - Epoch 0, Batch: 481, Loss: 0.600573
Train - Epoch 0, Batch: 482, Loss: 0.613234
Train - Epoch 0, Batch: 483, Loss: 0.600120
Train - Epoch 0, Batch: 484, Loss: 0.612886
Train - Epoch 0, Batch: 485, Loss: 0.603880
Train - Epoch 0, Batch: 486, Loss: 0.602656
Train - Epoch 0, Batch: 487, Loss: 0.611399
Train - Epoch 0, Batch: 488, Loss: 0.606109
Train - Epoch 0, Batch: 489, Loss: 0.615659
Train - Epoch 0, Batch: 490, Loss: 0.594776
Train - Epoch 0, Batch: 491, Loss: 0.617957
Train - Epoch 0, Batch: 492, Loss: 0.601146
Train - Epoch 0, Batch: 493, Loss: 0.604640
Train - Epoch 0, Batch: 494, Loss: 0.607032
Train - Epoch 0, Batch: 495, Loss: 0.612208
Train - Epoch 0, Batch: 496, Loss: 0.620236
Train - Epoch 0, Batch: 497, Loss: 0.618572
Train - Epoch 0, Batch: 498, Loss: 0.605036
Train - Epoch 0, Batch: 499, Loss: 0.609260
Train - Epoch 0, Batch: 500, Loss: 0.616952
Train - Epoch 0, Batch: 501, Loss: 0.609950
Train - Epoch 0, Batch: 502, Loss: 0.600473
Train - Epoch 0, Batch: 503, Loss: 0.595982
Train - Epoch 0, Batch: 504, Loss: 0.598242
Train - Epoch 0, Batch: 505, Loss: 0.589352
Train - Epoch 0, Batch: 506, Loss: 0.600146
Train - Epoch 0, Batch: 507, Loss: 0.595233
Train - Epoch 0, Batch: 508, Loss: 0.597033
Train - Epoch 0, Batch: 509, Loss: 0.601437
Train - Epoch 0, Batch: 510, Loss: 0.605253
Train - Epoch 0, Batch: 511, Loss: 0.594774
Train - Epoch 0, Batch: 512, Loss: 0.590396
Train - Epoch 0, Batch: 513, Loss: 0.595430
Train - Epoch 0, Batch: 514, Loss: 0.601330
Train - Epoch 0, Batch: 515, Loss: 0.609377
Train - Epoch 0, Batch: 516, Loss: 0.601190
Train - Epoch 0, Batch: 517, Loss: 0.602025
Train - Epoch 0, Batch: 518, Loss: 0.616366
Train - Epoch 0, Batch: 519, Loss: 0.596409
Train - Epoch 0, Batch: 520, Loss: 0.600735
Train - Epoch 0, Batch: 521, Loss: 0.587165
Train - Epoch 0, Batch: 522, Loss: 0.599417
Train - Epoch 0, Batch: 523, Loss: 0.611706
Train - Epoch 0, Batch: 524, Loss: 0.593905
Train - Epoch 0, Batch: 525, Loss: 0.611629
Train - Epoch 0, Batch: 526, Loss: 0.602587
Train - Epoch 0, Batch: 527, Loss: 0.610115
Train - Epoch 0, Batch: 528, Loss: 0.590284
Train - Epoch 0, Batch: 529, Loss: 0.595511
Train - Epoch 0, Batch: 530, Loss: 0.591982
Train - Epoch 0, Batch: 531, Loss: 0.595735
Train - Epoch 0, Batch: 532, Loss: 0.598945
Train - Epoch 0, Batch: 533, Loss: 0.590240
Train - Epoch 0, Batch: 534, Loss: 0.592547
Train - Epoch 0, Batch: 535, Loss: 0.608737
Train - Epoch 0, Batch: 536, Loss: 0.597814
Train - Epoch 0, Batch: 537, Loss: 0.593133
Train - Epoch 0, Batch: 538, Loss: 0.588152
Train - Epoch 0, Batch: 539, Loss: 0.586004
Train - Epoch 0, Batch: 540, Loss: 0.596782
Train - Epoch 0, Batch: 541, Loss: 0.581768
Train - Epoch 0, Batch: 542, Loss: 0.599691
Train - Epoch 0, Batch: 543, Loss: 0.597774
Train - Epoch 0, Batch: 544, Loss: 0.601743
Train - Epoch 0, Batch: 545, Loss: 0.606864
Train - Epoch 0, Batch: 546, Loss: 0.606579
Train - Epoch 0, Batch: 547, Loss: 0.594953
Train - Epoch 0, Batch: 548, Loss: 0.580026
Train - Epoch 0, Batch: 549, Loss: 0.605717
Train - Epoch 0, Batch: 550, Loss: 0.612586
Train - Epoch 0, Batch: 551, Loss: 0.592867
Train - Epoch 0, Batch: 552, Loss: 0.606363
Train - Epoch 0, Batch: 553, Loss: 0.597228
Train - Epoch 0, Batch: 554, Loss: 0.606362
Train - Epoch 0, Batch: 555, Loss: 0.598720
Train - Epoch 0, Batch: 556, Loss: 0.593574
Train - Epoch 0, Batch: 557, Loss: 0.619537
Train - Epoch 0, Batch: 558, Loss: 0.607478
Train - Epoch 0, Batch: 559, Loss: 0.593872
Train - Epoch 0, Batch: 560, Loss: 0.596030
Train - Epoch 0, Batch: 561, Loss: 0.603556
Train - Epoch 0, Batch: 562, Loss: 0.611081
Train - Epoch 0, Batch: 563, Loss: 0.571952
Train - Epoch 0, Batch: 564, Loss: 0.580877
Train - Epoch 0, Batch: 565, Loss: 0.595182
Train - Epoch 0, Batch: 566, Loss: 0.604249
Train - Epoch 0, Batch: 567, Loss: 0.606100
Train - Epoch 0, Batch: 568, Loss: 0.587959
Train - Epoch 0, Batch: 569, Loss: 0.599453
Train - Epoch 0, Batch: 570, Loss: 0.608350
Train - Epoch 0, Batch: 571, Loss: 0.604859
Train - Epoch 0, Batch: 572, Loss: 0.576437
Train - Epoch 0, Batch: 573, Loss: 0.592516
Train - Epoch 0, Batch: 574, Loss: 0.599456
Train - Epoch 0, Batch: 575, Loss: 0.587318
Train - Epoch 0, Batch: 576, Loss: 0.602576
Train - Epoch 0, Batch: 577, Loss: 0.601005
Train - Epoch 0, Batch: 578, Loss: 0.595202
Train - Epoch 0, Batch: 579, Loss: 0.607077
Train - Epoch 0, Batch: 580, Loss: 0.611038
Train - Epoch 0, Batch: 581, Loss: 0.584984
Train - Epoch 0, Batch: 582, Loss: 0.603305
Train - Epoch 0, Batch: 583, Loss: 0.595246
Train - Epoch 0, Batch: 584, Loss: 0.601821
Train - Epoch 0, Batch: 585, Loss: 0.584369
Train - Epoch 0, Batch: 586, Loss: 0.597457
Train - Epoch 0, Batch: 587, Loss: 0.600237
Train - Epoch 0, Batch: 588, Loss: 0.599977
Train - Epoch 0, Batch: 589, Loss: 0.589894
Train - Epoch 0, Batch: 590, Loss: 0.607641
Train - Epoch 0, Batch: 591, Loss: 0.592928
Train - Epoch 0, Batch: 592, Loss: 0.604329
Train - Epoch 0, Batch: 593, Loss: 0.576837
Train - Epoch 0, Batch: 594, Loss: 0.576387
Train - Epoch 0, Batch: 595, Loss: 0.611435
Train - Epoch 0, Batch: 596, Loss: 0.591179
Train - Epoch 0, Batch: 597, Loss: 0.602557
Train - Epoch 0, Batch: 598, Loss: 0.594258
Train - Epoch 0, Batch: 599, Loss: 0.588436
Train - Epoch 0, Batch: 600, Loss: 0.585287
Train - Epoch 0, Batch: 601, Loss: 0.589568
Train - Epoch 0, Batch: 602, Loss: 0.608747
Train - Epoch 0, Batch: 603, Loss: 0.594752
Train - Epoch 0, Batch: 604, Loss: 0.575571
Train - Epoch 0, Batch: 605, Loss: 0.605236
Train - Epoch 0, Batch: 606, Loss: 0.591632
Train - Epoch 0, Batch: 607, Loss: 0.593265
Train - Epoch 0, Batch: 608, Loss: 0.622103
Train - Epoch 0, Batch: 609, Loss: 0.580810
Train - Epoch 0, Batch: 610, Loss: 0.603141
Train - Epoch 0, Batch: 611, Loss: 0.605966
Train - Epoch 0, Batch: 612, Loss: 0.601312
Train - Epoch 0, Batch: 613, Loss: 0.587743
Train - Epoch 0, Batch: 614, Loss: 0.614707
Train - Epoch 0, Batch: 615, Loss: 0.596585
Train - Epoch 0, Batch: 616, Loss: 0.628711
Train - Epoch 0, Batch: 617, Loss: 0.602001
Train - Epoch 0, Batch: 618, Loss: 0.598610
Train - Epoch 0, Batch: 619, Loss: 0.588468
Train - Epoch 0, Batch: 620, Loss: 0.611690
Train - Epoch 0, Batch: 621, Loss: 0.582554
Train - Epoch 0, Batch: 622, Loss: 0.583559
Train - Epoch 0, Batch: 623, Loss: 0.582890
Train - Epoch 0, Batch: 624, Loss: 0.601197
Train - Epoch 0, Batch: 625, Loss: 0.598951
Train - Epoch 0, Batch: 626, Loss: 0.601029
Train - Epoch 0, Batch: 627, Loss: 0.601995
Train - Epoch 0, Batch: 628, Loss: 0.582284
Train - Epoch 0, Batch: 629, Loss: 0.573430
Train - Epoch 0, Batch: 630, Loss: 0.611466
Train - Epoch 0, Batch: 631, Loss: 0.589977
Train - Epoch 0, Batch: 632, Loss: 0.603532
Train - Epoch 0, Batch: 633, Loss: 0.602616
Train - Epoch 0, Batch: 634, Loss: 0.599188
Train - Epoch 0, Batch: 635, Loss: 0.588853
Train - Epoch 0, Batch: 636, Loss: 0.583837
Train - Epoch 0, Batch: 637, Loss: 0.580937
Train - Epoch 0, Batch: 638, Loss: 0.603065
Train - Epoch 0, Batch: 639, Loss: 0.594100
Train - Epoch 0, Batch: 640, Loss: 0.591327
Train - Epoch 0, Batch: 641, Loss: 0.577796
Train - Epoch 0, Batch: 642, Loss: 0.590106
Train - Epoch 0, Batch: 643, Loss: 0.571195
Train - Epoch 0, Batch: 644, Loss: 0.575134
Train - Epoch 0, Batch: 645, Loss: 0.598482
Train - Epoch 0, Batch: 646, Loss: 0.592468
Train - Epoch 0, Batch: 647, Loss: 0.597488
Train - Epoch 0, Batch: 648, Loss: 0.608646
Train - Epoch 0, Batch: 649, Loss: 0.589599
Train - Epoch 0, Batch: 650, Loss: 0.598449
Train - Epoch 0, Batch: 651, Loss: 0.593676
Train - Epoch 0, Batch: 652, Loss: 0.577822
Train - Epoch 0, Batch: 653, Loss: 0.608436
Train - Epoch 0, Batch: 654, Loss: 0.626687
Train - Epoch 0, Batch: 655, Loss: 0.589506
Train - Epoch 0, Batch: 656, Loss: 0.598065
Train - Epoch 0, Batch: 657, Loss: 0.603964
Train - Epoch 0, Batch: 658, Loss: 0.592403
Train - Epoch 0, Batch: 659, Loss: 0.591774
Train - Epoch 0, Batch: 660, Loss: 0.598868
Train - Epoch 0, Batch: 661, Loss: 0.595732
Train - Epoch 0, Batch: 662, Loss: 0.588778
Train - Epoch 0, Batch: 663, Loss: 0.599265
Train - Epoch 0, Batch: 664, Loss: 0.582163
Train - Epoch 0, Batch: 665, Loss: 0.605045
Train - Epoch 0, Batch: 666, Loss: 0.598129
Train - Epoch 0, Batch: 667, Loss: 0.573485
Train - Epoch 0, Batch: 668, Loss: 0.596090
Train - Epoch 0, Batch: 669, Loss: 0.597583
Train - Epoch 0, Batch: 670, Loss: 0.574807
Train - Epoch 0, Batch: 671, Loss: 0.583838
Train - Epoch 0, Batch: 672, Loss: 0.579494
Train - Epoch 0, Batch: 673, Loss: 0.607726
Train - Epoch 0, Batch: 674, Loss: 0.588515
Train - Epoch 0, Batch: 675, Loss: 0.587499
Train - Epoch 0, Batch: 676, Loss: 0.595089
Train - Epoch 0, Batch: 677, Loss: 0.601636
Train - Epoch 0, Batch: 678, Loss: 0.602693
Train - Epoch 0, Batch: 679, Loss: 0.567569
Train - Epoch 0, Batch: 680, Loss: 0.597101
Train - Epoch 0, Batch: 681, Loss: 0.584318
Train - Epoch 0, Batch: 682, Loss: 0.591437
Train - Epoch 0, Batch: 683, Loss: 0.606071
Train - Epoch 0, Batch: 684, Loss: 0.575380
Train - Epoch 0, Batch: 685, Loss: 0.594217
Train - Epoch 0, Batch: 686, Loss: 0.627252
Train - Epoch 0, Batch: 687, Loss: 0.616311
Train - Epoch 0, Batch: 688, Loss: 0.612281
Train - Epoch 0, Batch: 689, Loss: 0.599761
Train - Epoch 0, Batch: 690, Loss: 0.597349
Train - Epoch 0, Batch: 691, Loss: 0.595975
Train - Epoch 0, Batch: 692, Loss: 0.581015
Train - Epoch 0, Batch: 693, Loss: 0.592312
Train - Epoch 0, Batch: 694, Loss: 0.581206
Train - Epoch 0, Batch: 695, Loss: 0.583942
Train - Epoch 0, Batch: 696, Loss: 0.575507
Train - Epoch 0, Batch: 697, Loss: 0.591239
Train - Epoch 0, Batch: 698, Loss: 0.575538
Train - Epoch 0, Batch: 699, Loss: 0.614432
Train - Epoch 0, Batch: 700, Loss: 0.591693
Train - Epoch 0, Batch: 701, Loss: 0.599285
Train - Epoch 0, Batch: 702, Loss: 0.614709
Train - Epoch 0, Batch: 703, Loss: 0.616411
Train - Epoch 0, Batch: 704, Loss: 0.598122
Train - Epoch 0, Batch: 705, Loss: 0.591414
Train - Epoch 0, Batch: 706, Loss: 0.584664
Train - Epoch 0, Batch: 707, Loss: 0.588486
Train - Epoch 0, Batch: 708, Loss: 0.612260
Train - Epoch 0, Batch: 709, Loss: 0.606966
Train - Epoch 0, Batch: 710, Loss: 0.579121
Train - Epoch 0, Batch: 711, Loss: 0.581523
Train - Epoch 0, Batch: 712, Loss: 0.574560
Train - Epoch 0, Batch: 713, Loss: 0.592794
Train - Epoch 0, Batch: 714, Loss: 0.585457
Train - Epoch 0, Batch: 715, Loss: 0.581440
Train - Epoch 0, Batch: 716, Loss: 0.580733
Train - Epoch 0, Batch: 717, Loss: 0.593589
Train - Epoch 0, Batch: 718, Loss: 0.589409
Train - Epoch 0, Batch: 719, Loss: 0.585541
Train - Epoch 0, Batch: 720, Loss: 0.591060
Train - Epoch 0, Batch: 721, Loss: 0.586309
Train - Epoch 0, Batch: 722, Loss: 0.610594
Train - Epoch 0, Batch: 723, Loss: 0.579436
Train - Epoch 0, Batch: 724, Loss: 0.599001
Train - Epoch 0, Batch: 725, Loss: 0.591569
Train - Epoch 0, Batch: 726, Loss: 0.572796
Train - Epoch 0, Batch: 727, Loss: 0.575198
Train - Epoch 0, Batch: 728, Loss: 0.588104
Train - Epoch 0, Batch: 729, Loss: 0.591346
Train - Epoch 0, Batch: 730, Loss: 0.589586
Train - Epoch 0, Batch: 731, Loss: 0.579286
Train - Epoch 0, Batch: 732, Loss: 0.600214
Train - Epoch 0, Batch: 733, Loss: 0.571272
Train - Epoch 0, Batch: 734, Loss: 0.603370
Train - Epoch 0, Batch: 735, Loss: 0.596743
Train - Epoch 0, Batch: 736, Loss: 0.596236
Train - Epoch 0, Batch: 737, Loss: 0.605040
Train - Epoch 0, Batch: 738, Loss: 0.576091
Train - Epoch 0, Batch: 739, Loss: 0.575065
Train - Epoch 0, Batch: 740, Loss: 0.579384
Train - Epoch 0, Batch: 741, Loss: 0.586832
Train - Epoch 0, Batch: 742, Loss: 0.582892
Train - Epoch 0, Batch: 743, Loss: 0.597965
Train - Epoch 0, Batch: 744, Loss: 0.584450
Train - Epoch 0, Batch: 745, Loss: 0.607869
Train - Epoch 0, Batch: 746, Loss: 0.607342
Train - Epoch 0, Batch: 747, Loss: 0.569322
Train - Epoch 0, Batch: 748, Loss: 0.586537
Train - Epoch 0, Batch: 749, Loss: 0.586399
Train - Epoch 0, Batch: 750, Loss: 0.587283
Train - Epoch 0, Batch: 751, Loss: 0.590361
Train - Epoch 0, Batch: 752, Loss: 0.597186
Train - Epoch 0, Batch: 753, Loss: 0.612679
Train - Epoch 0, Batch: 754, Loss: 0.576934
Train - Epoch 0, Batch: 755, Loss: 0.580832
Train - Epoch 0, Batch: 756, Loss: 0.597620
Train - Epoch 0, Batch: 757, Loss: 0.596755
Train - Epoch 0, Batch: 758, Loss: 0.563072
Train - Epoch 0, Batch: 759, Loss: 0.593676
Train - Epoch 0, Batch: 760, Loss: 0.599581
Train - Epoch 0, Batch: 761, Loss: 0.612614
Train - Epoch 0, Batch: 762, Loss: 0.570988
Train - Epoch 0, Batch: 763, Loss: 0.577670
Train - Epoch 0, Batch: 764, Loss: 0.592920
Train - Epoch 0, Batch: 765, Loss: 0.580695
Train - Epoch 0, Batch: 766, Loss: 0.602322
Train - Epoch 0, Batch: 767, Loss: 0.579359
Train - Epoch 0, Batch: 768, Loss: 0.577708
Train - Epoch 0, Batch: 769, Loss: 0.582748
Train - Epoch 0, Batch: 770, Loss: 0.573849
Train - Epoch 0, Batch: 771, Loss: 0.591121
Train - Epoch 0, Batch: 772, Loss: 0.588711
Train - Epoch 0, Batch: 773, Loss: 0.592942
Train - Epoch 0, Batch: 774, Loss: 0.602513
Train - Epoch 0, Batch: 775, Loss: 0.585805
Train - Epoch 0, Batch: 776, Loss: 0.578645
Train - Epoch 0, Batch: 777, Loss: 0.582159
Train - Epoch 0, Batch: 778, Loss: 0.590127
Train - Epoch 0, Batch: 779, Loss: 0.574062
Train - Epoch 0, Batch: 780, Loss: 0.594100
Train - Epoch 0, Batch: 781, Loss: 0.586054
Train - Epoch 0, Batch: 782, Loss: 0.598093
Train - Epoch 0, Batch: 783, Loss: 0.609413
Train - Epoch 0, Batch: 784, Loss: 0.605573
Train - Epoch 0, Batch: 785, Loss: 0.589673
Train - Epoch 0, Batch: 786, Loss: 0.589619
Train - Epoch 0, Batch: 787, Loss: 0.590967
Train - Epoch 0, Batch: 788, Loss: 0.575567
Train - Epoch 0, Batch: 789, Loss: 0.590456
Train - Epoch 0, Batch: 790, Loss: 0.594003
Train - Epoch 0, Batch: 791, Loss: 0.595604
Train - Epoch 0, Batch: 792, Loss: 0.586887
Train - Epoch 0, Batch: 793, Loss: 0.592918
Train - Epoch 0, Batch: 794, Loss: 0.593018
Train - Epoch 0, Batch: 795, Loss: 0.574764
Train - Epoch 0, Batch: 796, Loss: 0.566785
Train - Epoch 0, Batch: 797, Loss: 0.571664
Train - Epoch 0, Batch: 798, Loss: 0.556875
Train - Epoch 0, Batch: 799, Loss: 0.593944
Train - Epoch 0, Batch: 800, Loss: 0.575541
Train - Epoch 0, Batch: 801, Loss: 0.589784
Train - Epoch 0, Batch: 802, Loss: 0.607314
Train - Epoch 0, Batch: 803, Loss: 0.581461
Train - Epoch 0, Batch: 804, Loss: 0.587581
Train - Epoch 0, Batch: 805, Loss: 0.593825
Train - Epoch 0, Batch: 806, Loss: 0.602392
Train - Epoch 0, Batch: 807, Loss: 0.600990
Train - Epoch 0, Batch: 808, Loss: 0.569378
Train - Epoch 0, Batch: 809, Loss: 0.577816
Train - Epoch 0, Batch: 810, Loss: 0.599532
Train - Epoch 0, Batch: 811, Loss: 0.598392
Train - Epoch 0, Batch: 812, Loss: 0.583816
Train - Epoch 0, Batch: 813, Loss: 0.586931
Train - Epoch 0, Batch: 814, Loss: 0.585089
Train - Epoch 0, Batch: 815, Loss: 0.556743
Train - Epoch 0, Batch: 816, Loss: 0.595997
Train - Epoch 0, Batch: 817, Loss: 0.590578
Train - Epoch 0, Batch: 818, Loss: 0.588664
Train - Epoch 0, Batch: 819, Loss: 0.597587
Train - Epoch 0, Batch: 820, Loss: 0.573924
Train - Epoch 0, Batch: 821, Loss: 0.583735
Train - Epoch 0, Batch: 822, Loss: 0.590377
Train - Epoch 0, Batch: 823, Loss: 0.571316
Train - Epoch 0, Batch: 824, Loss: 0.594330
Train - Epoch 0, Batch: 825, Loss: 0.610948
Train - Epoch 0, Batch: 826, Loss: 0.566610
Train - Epoch 0, Batch: 827, Loss: 0.569970
Train - Epoch 0, Batch: 828, Loss: 0.601372
Train - Epoch 0, Batch: 829, Loss: 0.569518
Train - Epoch 0, Batch: 830, Loss: 0.598644
Train - Epoch 0, Batch: 831, Loss: 0.595299
Train - Epoch 0, Batch: 832, Loss: 0.594176
Train - Epoch 0, Batch: 833, Loss: 0.571343
Train - Epoch 0, Batch: 834, Loss: 0.596681
Train - Epoch 0, Batch: 835, Loss: 0.589358
Train - Epoch 0, Batch: 836, Loss: 0.582800
Train - Epoch 0, Batch: 837, Loss: 0.580303
Train - Epoch 0, Batch: 838, Loss: 0.575429
Train - Epoch 0, Batch: 839, Loss: 0.597577
Train - Epoch 0, Batch: 840, Loss: 0.587220
Train - Epoch 0, Batch: 841, Loss: 0.588783
Train - Epoch 0, Batch: 842, Loss: 0.600701
Train - Epoch 0, Batch: 843, Loss: 0.571931
Train - Epoch 0, Batch: 844, Loss: 0.579135
Train - Epoch 0, Batch: 845, Loss: 0.592075
Train - Epoch 0, Batch: 846, Loss: 0.574883
Train - Epoch 0, Batch: 847, Loss: 0.586780
Train - Epoch 0, Batch: 848, Loss: 0.580093
Train - Epoch 0, Batch: 849, Loss: 0.570148
Train - Epoch 0, Batch: 850, Loss: 0.589321
Train - Epoch 0, Batch: 851, Loss: 0.581811
Train - Epoch 0, Batch: 852, Loss: 0.589207
Train - Epoch 0, Batch: 853, Loss: 0.600608
Train - Epoch 0, Batch: 854, Loss: 0.605553
Train - Epoch 0, Batch: 855, Loss: 0.585604
Train - Epoch 0, Batch: 856, Loss: 0.585088
Train - Epoch 0, Batch: 857, Loss: 0.575566
Train - Epoch 0, Batch: 858, Loss: 0.592452
Train - Epoch 0, Batch: 859, Loss: 0.590542
Train - Epoch 0, Batch: 860, Loss: 0.588517
Train - Epoch 0, Batch: 861, Loss: 0.564461
Train - Epoch 0, Batch: 862, Loss: 0.564037
Train - Epoch 0, Batch: 863, Loss: 0.580477
Train - Epoch 0, Batch: 864, Loss: 0.591124
Train - Epoch 0, Batch: 865, Loss: 0.581314
Train - Epoch 0, Batch: 866, Loss: 0.589056
Train - Epoch 0, Batch: 867, Loss: 0.577716
Train - Epoch 0, Batch: 868, Loss: 0.575236
Train - Epoch 0, Batch: 869, Loss: 0.591492
Train - Epoch 0, Batch: 870, Loss: 0.583830
Train - Epoch 0, Batch: 871, Loss: 0.578775
Train - Epoch 0, Batch: 872, Loss: 0.607374
Train - Epoch 0, Batch: 873, Loss: 0.566021
Train - Epoch 0, Batch: 874, Loss: 0.603290
Train - Epoch 0, Batch: 875, Loss: 0.585914
Train - Epoch 0, Batch: 876, Loss: 0.609926
Train - Epoch 0, Batch: 877, Loss: 0.592984
Train - Epoch 0, Batch: 878, Loss: 0.577139
Train - Epoch 0, Batch: 879, Loss: 0.571525
Train - Epoch 0, Batch: 880, Loss: 0.587618
Train - Epoch 0, Batch: 881, Loss: 0.582144
Train - Epoch 0, Batch: 882, Loss: 0.570520
Train - Epoch 0, Batch: 883, Loss: 0.588122
Train - Epoch 0, Batch: 884, Loss: 0.600189
Train - Epoch 0, Batch: 885, Loss: 0.558608
Train - Epoch 0, Batch: 886, Loss: 0.583833
Train - Epoch 0, Batch: 887, Loss: 0.608476
Train - Epoch 0, Batch: 888, Loss: 0.594466
Train - Epoch 0, Batch: 889, Loss: 0.583793
Train - Epoch 0, Batch: 890, Loss: 0.592804
Train - Epoch 0, Batch: 891, Loss: 0.556062
Train - Epoch 0, Batch: 892, Loss: 0.588954
Train - Epoch 0, Batch: 893, Loss: 0.602204
Train - Epoch 0, Batch: 894, Loss: 0.596655
Train - Epoch 0, Batch: 895, Loss: 0.586453
Train - Epoch 0, Batch: 896, Loss: 0.572238
Train - Epoch 0, Batch: 897, Loss: 0.591237
Train - Epoch 0, Batch: 898, Loss: 0.578435
Train - Epoch 0, Batch: 899, Loss: 0.603528
Train - Epoch 0, Batch: 900, Loss: 0.557911
Train - Epoch 0, Batch: 901, Loss: 0.582033
Train - Epoch 0, Batch: 902, Loss: 0.588376
Train - Epoch 0, Batch: 903, Loss: 0.575051
Train - Epoch 0, Batch: 904, Loss: 0.585556
Train - Epoch 0, Batch: 905, Loss: 0.575902
Train - Epoch 0, Batch: 906, Loss: 0.629482
Train - Epoch 0, Batch: 907, Loss: 0.569666
Train - Epoch 0, Batch: 908, Loss: 0.585318
Train - Epoch 0, Batch: 909, Loss: 0.575736
Train - Epoch 0, Batch: 910, Loss: 0.577229
Train - Epoch 0, Batch: 911, Loss: 0.583777
Train - Epoch 0, Batch: 912, Loss: 0.563413
Train - Epoch 0, Batch: 913, Loss: 0.585664
Train - Epoch 0, Batch: 914, Loss: 0.597923
Train - Epoch 0, Batch: 915, Loss: 0.586371
Train - Epoch 0, Batch: 916, Loss: 0.588805
Train - Epoch 0, Batch: 917, Loss: 0.569321
Train - Epoch 0, Batch: 918, Loss: 0.579665
Train - Epoch 0, Batch: 919, Loss: 0.573249
Train - Epoch 0, Batch: 920, Loss: 0.599825
Train - Epoch 0, Batch: 921, Loss: 0.594043
Train - Epoch 0, Batch: 922, Loss: 0.571049
Train - Epoch 0, Batch: 923, Loss: 0.597476
Train - Epoch 0, Batch: 924, Loss: 0.583491
Train - Epoch 0, Batch: 925, Loss: 0.581404
Train - Epoch 0, Batch: 926, Loss: 0.596260
Train - Epoch 0, Batch: 927, Loss: 0.565318
Train - Epoch 0, Batch: 928, Loss: 0.560667
Train - Epoch 0, Batch: 929, Loss: 0.570660
Train - Epoch 0, Batch: 930, Loss: 0.569189
Train - Epoch 0, Batch: 931, Loss: 0.593666
Train - Epoch 0, Batch: 932, Loss: 0.579753
Train - Epoch 0, Batch: 933, Loss: 0.585374
Train - Epoch 0, Batch: 934, Loss: 0.580483
Train - Epoch 0, Batch: 935, Loss: 0.575577
Train - Epoch 0, Batch: 936, Loss: 0.582371
Train - Epoch 0, Batch: 937, Loss: 0.585901
Train - Epoch 0, Batch: 938, Loss: 0.583287
Train - Epoch 0, Batch: 939, Loss: 0.618019
Train - Epoch 0, Batch: 940, Loss: 0.561636
Train - Epoch 0, Batch: 941, Loss: 0.589730
Train - Epoch 0, Batch: 942, Loss: 0.585556
Train - Epoch 0, Batch: 943, Loss: 0.591146
Train - Epoch 0, Batch: 944, Loss: 0.564876
Train - Epoch 0, Batch: 945, Loss: 0.584955
Train - Epoch 0, Batch: 946, Loss: 0.580018
Train - Epoch 0, Batch: 947, Loss: 0.558403
Train - Epoch 0, Batch: 948, Loss: 0.573767
Train - Epoch 0, Batch: 949, Loss: 0.578747
Train - Epoch 0, Batch: 950, Loss: 0.581715
Train - Epoch 0, Batch: 951, Loss: 0.601704
Train - Epoch 0, Batch: 952, Loss: 0.574249
Train - Epoch 0, Batch: 953, Loss: 0.577300
Train - Epoch 0, Batch: 954, Loss: 0.580569
Train - Epoch 0, Batch: 955, Loss: 0.580656
Train - Epoch 0, Batch: 956, Loss: 0.597026
Train - Epoch 0, Batch: 957, Loss: 0.563171
Train - Epoch 0, Batch: 958, Loss: 0.567251
Train - Epoch 0, Batch: 959, Loss: 0.605816
Train - Epoch 0, Batch: 960, Loss: 0.577932
Train - Epoch 0, Batch: 961, Loss: 0.593169
Train - Epoch 0, Batch: 962, Loss: 0.600618
Train - Epoch 0, Batch: 963, Loss: 0.578124
Train - Epoch 0, Batch: 964, Loss: 0.598039
Train - Epoch 0, Batch: 965, Loss: 0.574876
Train - Epoch 0, Batch: 966, Loss: 0.599643
Train - Epoch 0, Batch: 967, Loss: 0.586233
Train - Epoch 0, Batch: 968, Loss: 0.585511
Train - Epoch 0, Batch: 969, Loss: 0.584014
Train - Epoch 0, Batch: 970, Loss: 0.571087
Train - Epoch 0, Batch: 971, Loss: 0.560568
Train - Epoch 0, Batch: 972, Loss: 0.579115
Train - Epoch 0, Batch: 973, Loss: 0.576595
Train - Epoch 0, Batch: 974, Loss: 0.601379
Train - Epoch 0, Batch: 975, Loss: 0.575559
Train - Epoch 0, Batch: 976, Loss: 0.592200
Train - Epoch 0, Batch: 977, Loss: 0.580742
Train - Epoch 0, Batch: 978, Loss: 0.575869
Train - Epoch 0, Batch: 979, Loss: 0.565266
Train - Epoch 0, Batch: 980, Loss: 0.589966
Train - Epoch 0, Batch: 981, Loss: 0.563269
Train - Epoch 0, Batch: 982, Loss: 0.587744
Train - Epoch 0, Batch: 983, Loss: 0.558964
Train - Epoch 0, Batch: 984, Loss: 0.605689
Train - Epoch 0, Batch: 985, Loss: 0.570343
Train - Epoch 0, Batch: 986, Loss: 0.583783
Train - Epoch 0, Batch: 987, Loss: 0.588500
Train - Epoch 0, Batch: 988, Loss: 0.586008
Train - Epoch 0, Batch: 989, Loss: 0.571429
Train - Epoch 0, Batch: 990, Loss: 0.575739
Train - Epoch 0, Batch: 991, Loss: 0.585852
Train - Epoch 0, Batch: 992, Loss: 0.590061
Train - Epoch 0, Batch: 993, Loss: 0.561979
Train - Epoch 0, Batch: 994, Loss: 0.590160
Train - Epoch 0, Batch: 995, Loss: 0.568637
Train - Epoch 0, Batch: 996, Loss: 0.576130
Train - Epoch 0, Batch: 997, Loss: 0.591853
Train - Epoch 0, Batch: 998, Loss: 0.563310
Train - Epoch 0, Batch: 999, Loss: 0.563081
Train - Epoch 0, Batch: 1000, Loss: 0.578778
Train - Epoch 0, Batch: 1001, Loss: 0.609541
Train - Epoch 0, Batch: 1002, Loss: 0.566049
Train - Epoch 0, Batch: 1003, Loss: 0.582504
Train - Epoch 0, Batch: 1004, Loss: 0.567476
Train - Epoch 0, Batch: 1005, Loss: 0.565820
Train - Epoch 0, Batch: 1006, Loss: 0.567479
Train - Epoch 0, Batch: 1007, Loss: 0.591933
Train - Epoch 0, Batch: 1008, Loss: 0.564677
Train - Epoch 0, Batch: 1009, Loss: 0.611545
Train - Epoch 0, Batch: 1010, Loss: 0.578281
Train - Epoch 0, Batch: 1011, Loss: 0.574860
Train - Epoch 0, Batch: 1012, Loss: 0.558907
Train - Epoch 0, Batch: 1013, Loss: 0.583088
Train - Epoch 0, Batch: 1014, Loss: 0.608278
Train - Epoch 0, Batch: 1015, Loss: 0.575726
Train - Epoch 0, Batch: 1016, Loss: 0.576024
Train - Epoch 0, Batch: 1017, Loss: 0.587644
Train - Epoch 0, Batch: 1018, Loss: 0.560370
Train - Epoch 0, Batch: 1019, Loss: 0.580326
Train - Epoch 0, Batch: 1020, Loss: 0.550629
Train - Epoch 0, Batch: 1021, Loss: 0.565174
Train - Epoch 100, Batch: 0, Loss: 0.528942
Train - Epoch 100, Batch: 1, Loss: 0.554043
Train - Epoch 100, Batch: 2, Loss: 0.541945
Train - Epoch 100, Batch: 3, Loss: 0.554855
Train - Epoch 100, Batch: 4, Loss: 0.539735
Train - Epoch 100, Batch: 5, Loss: 0.529322
Train - Epoch 100, Batch: 6, Loss: 0.510744
Train - Epoch 100, Batch: 7, Loss: 0.510275
Train - Epoch 100, Batch: 8, Loss: 0.520736
Train - Epoch 100, Batch: 9, Loss: 0.528515
Train - Epoch 100, Batch: 10, Loss: 0.529414
Train - Epoch 100, Batch: 11, Loss: 0.519053
Train - Epoch 100, Batch: 12, Loss: 0.528339
Train - Epoch 100, Batch: 13, Loss: 0.535772
Train - Epoch 100, Batch: 14, Loss: 0.544956
Train - Epoch 100, Batch: 15, Loss: 0.534895
Train - Epoch 100, Batch: 16, Loss: 0.507325
Train - Epoch 100, Batch: 17, Loss: 0.515850
Train - Epoch 100, Batch: 18, Loss: 0.535869
Train - Epoch 100, Batch: 19, Loss: 0.526180
Train - Epoch 100, Batch: 20, Loss: 0.524217
Train - Epoch 100, Batch: 21, Loss: 0.533109
Train - Epoch 100, Batch: 22, Loss: 0.515477
Train - Epoch 100, Batch: 23, Loss: 0.536507
Train - Epoch 100, Batch: 24, Loss: 0.540559
Train - Epoch 100, Batch: 25, Loss: 0.563377
Train - Epoch 100, Batch: 26, Loss: 0.520406
Train - Epoch 100, Batch: 27, Loss: 0.539644
Train - Epoch 100, Batch: 28, Loss: 0.546295
Train - Epoch 100, Batch: 29, Loss: 0.535511
Train - Epoch 100, Batch: 30, Loss: 0.538933
Train - Epoch 100, Batch: 31, Loss: 0.565581
Train - Epoch 100, Batch: 32, Loss: 0.539510
Train - Epoch 100, Batch: 33, Loss: 0.531143
Train - Epoch 100, Batch: 34, Loss: 0.554221
Train - Epoch 100, Batch: 35, Loss: 0.529634
Train - Epoch 100, Batch: 36, Loss: 0.524302
Train - Epoch 100, Batch: 37, Loss: 0.532296
Train - Epoch 100, Batch: 38, Loss: 0.495508
Train - Epoch 100, Batch: 39, Loss: 0.524823
Train - Epoch 100, Batch: 40, Loss: 0.553864
Train - Epoch 100, Batch: 41, Loss: 0.555075
Train - Epoch 100, Batch: 42, Loss: 0.532924
Train - Epoch 100, Batch: 43, Loss: 0.526821
Train - Epoch 100, Batch: 44, Loss: 0.549941
Train - Epoch 100, Batch: 45, Loss: 0.541699
Train - Epoch 100, Batch: 46, Loss: 0.553015
Train - Epoch 100, Batch: 47, Loss: 0.523402
Train - Epoch 100, Batch: 48, Loss: 0.571407
Train - Epoch 100, Batch: 49, Loss: 0.533353
Train - Epoch 100, Batch: 50, Loss: 0.523869
Train - Epoch 100, Batch: 51, Loss: 0.503744
Train - Epoch 100, Batch: 52, Loss: 0.552939
Train - Epoch 100, Batch: 53, Loss: 0.546874
Train - Epoch 100, Batch: 54, Loss: 0.511090
Train - Epoch 100, Batch: 55, Loss: 0.517604
Train - Epoch 100, Batch: 56, Loss: 0.511274
Train - Epoch 100, Batch: 57, Loss: 0.499055
Train - Epoch 100, Batch: 58, Loss: 0.541802
Train - Epoch 100, Batch: 59, Loss: 0.518735
Train - Epoch 100, Batch: 60, Loss: 0.537364
Train - Epoch 100, Batch: 61, Loss: 0.552473
Train - Epoch 100, Batch: 62, Loss: 0.509727
Train - Epoch 100, Batch: 63, Loss: 0.526514
Train - Epoch 100, Batch: 64, Loss: 0.531584
Train - Epoch 100, Batch: 65, Loss: 0.519610
Train - Epoch 100, Batch: 66, Loss: 0.512180
Train - Epoch 100, Batch: 67, Loss: 0.520144
Train - Epoch 100, Batch: 68, Loss: 0.571863
Train - Epoch 100, Batch: 69, Loss: 0.518121
Train - Epoch 100, Batch: 70, Loss: 0.548329
Train - Epoch 100, Batch: 71, Loss: 0.530205
Train - Epoch 100, Batch: 72, Loss: 0.530418
Train - Epoch 100, Batch: 73, Loss: 0.524380
Train - Epoch 100, Batch: 74, Loss: 0.526201
Train - Epoch 100, Batch: 75, Loss: 0.541336
Train - Epoch 100, Batch: 76, Loss: 0.543158
Train - Epoch 100, Batch: 77, Loss: 0.532712
Train - Epoch 100, Batch: 78, Loss: 0.515202
Train - Epoch 100, Batch: 79, Loss: 0.501684
Train - Epoch 100, Batch: 80, Loss: 0.533197
Train - Epoch 100, Batch: 81, Loss: 0.548383
Train - Epoch 100, Batch: 82, Loss: 0.520254
Train - Epoch 100, Batch: 83, Loss: 0.540696
Train - Epoch 100, Batch: 84, Loss: 0.519802
Train - Epoch 100, Batch: 85, Loss: 0.531174
Train - Epoch 100, Batch: 86, Loss: 0.529631
Train - Epoch 100, Batch: 87, Loss: 0.532464
Train - Epoch 100, Batch: 88, Loss: 0.549618
Train - Epoch 100, Batch: 89, Loss: 0.543355
Train - Epoch 100, Batch: 90, Loss: 0.540089
Train - Epoch 100, Batch: 91, Loss: 0.537311
Train - Epoch 100, Batch: 92, Loss: 0.542914
Train - Epoch 100, Batch: 93, Loss: 0.559418
Train - Epoch 100, Batch: 94, Loss: 0.543657
Train - Epoch 100, Batch: 95, Loss: 0.540863
Train - Epoch 100, Batch: 96, Loss: 0.534711
Train - Epoch 100, Batch: 97, Loss: 0.535069
Train - Epoch 100, Batch: 98, Loss: 0.538347
Train - Epoch 100, Batch: 99, Loss: 0.516676
Train - Epoch 100, Batch: 100, Loss: 0.546272
Train - Epoch 100, Batch: 101, Loss: 0.534642
Train - Epoch 100, Batch: 102, Loss: 0.547970
Train - Epoch 100, Batch: 103, Loss: 0.541829
Train - Epoch 100, Batch: 104, Loss: 0.556343
Train - Epoch 100, Batch: 105, Loss: 0.574395
Train - Epoch 100, Batch: 106, Loss: 0.509664
Train - Epoch 100, Batch: 107, Loss: 0.513839
Train - Epoch 100, Batch: 108, Loss: 0.545123
Train - Epoch 100, Batch: 109, Loss: 0.531198
Train - Epoch 100, Batch: 110, Loss: 0.571246
Train - Epoch 100, Batch: 111, Loss: 0.526410
Train - Epoch 100, Batch: 112, Loss: 0.560403
Train - Epoch 100, Batch: 113, Loss: 0.491920
Train - Epoch 100, Batch: 114, Loss: 0.527042
Train - Epoch 100, Batch: 115, Loss: 0.531243
Train - Epoch 100, Batch: 116, Loss: 0.522440
Train - Epoch 100, Batch: 117, Loss: 0.532293
Train - Epoch 100, Batch: 118, Loss: 0.518658
Train - Epoch 100, Batch: 119, Loss: 0.529146
Train - Epoch 100, Batch: 120, Loss: 0.536004
Train - Epoch 100, Batch: 121, Loss: 0.566683
Train - Epoch 100, Batch: 122, Loss: 0.536048
Train - Epoch 100, Batch: 123, Loss: 0.515472
Train - Epoch 100, Batch: 124, Loss: 0.516406
Train - Epoch 100, Batch: 125, Loss: 0.547918
Train - Epoch 100, Batch: 126, Loss: 0.527729
Train - Epoch 100, Batch: 127, Loss: 0.531637
Train - Epoch 100, Batch: 128, Loss: 0.527919
Train - Epoch 100, Batch: 129, Loss: 0.581139
Train - Epoch 100, Batch: 130, Loss: 0.543776
Train - Epoch 100, Batch: 131, Loss: 0.543618
Train - Epoch 100, Batch: 132, Loss: 0.537969
Train - Epoch 100, Batch: 133, Loss: 0.540700
Train - Epoch 100, Batch: 134, Loss: 0.518448
Train - Epoch 100, Batch: 135, Loss: 0.581475
Train - Epoch 100, Batch: 136, Loss: 0.517446
Train - Epoch 100, Batch: 137, Loss: 0.552572
Train - Epoch 100, Batch: 138, Loss: 0.539640
Train - Epoch 100, Batch: 139, Loss: 0.532822
Train - Epoch 100, Batch: 140, Loss: 0.548426
Train - Epoch 100, Batch: 141, Loss: 0.554814
Train - Epoch 100, Batch: 142, Loss: 0.504609
Train - Epoch 100, Batch: 143, Loss: 0.529460
Train - Epoch 100, Batch: 144, Loss: 0.539369
Train - Epoch 100, Batch: 145, Loss: 0.543464
Train - Epoch 100, Batch: 146, Loss: 0.548666
Train - Epoch 100, Batch: 147, Loss: 0.484023
Train - Epoch 100, Batch: 148, Loss: 0.535512
Train - Epoch 100, Batch: 149, Loss: 0.526381
Train - Epoch 100, Batch: 150, Loss: 0.549324
Train - Epoch 100, Batch: 151, Loss: 0.521515
Train - Epoch 100, Batch: 152, Loss: 0.519911
Train - Epoch 100, Batch: 153, Loss: 0.562812
Train - Epoch 100, Batch: 154, Loss: 0.540904
Train - Epoch 100, Batch: 155, Loss: 0.517250
Train - Epoch 100, Batch: 156, Loss: 0.504977
Train - Epoch 100, Batch: 157, Loss: 0.515585
Train - Epoch 100, Batch: 158, Loss: 0.541175
Train - Epoch 100, Batch: 159, Loss: 0.555922
Train - Epoch 100, Batch: 160, Loss: 0.544635
Train - Epoch 100, Batch: 161, Loss: 0.576196
Train - Epoch 100, Batch: 162, Loss: 0.552642
Train - Epoch 100, Batch: 163, Loss: 0.514849
Train - Epoch 100, Batch: 164, Loss: 0.557848
Train - Epoch 100, Batch: 165, Loss: 0.524359
Train - Epoch 100, Batch: 166, Loss: 0.546722
Train - Epoch 100, Batch: 167, Loss: 0.526478
Train - Epoch 100, Batch: 168, Loss: 0.527642
Train - Epoch 100, Batch: 169, Loss: 0.530599
Train - Epoch 100, Batch: 170, Loss: 0.546314
Train - Epoch 100, Batch: 171, Loss: 0.570589
Train - Epoch 100, Batch: 172, Loss: 0.503208
Train - Epoch 100, Batch: 173, Loss: 0.561728
Train - Epoch 100, Batch: 174, Loss: 0.541064
Train - Epoch 100, Batch: 175, Loss: 0.532630
Train - Epoch 100, Batch: 176, Loss: 0.549421
Train - Epoch 100, Batch: 177, Loss: 0.528807
Train - Epoch 100, Batch: 178, Loss: 0.569455
Train - Epoch 100, Batch: 179, Loss: 0.554804
Train - Epoch 100, Batch: 180, Loss: 0.524411
Train - Epoch 100, Batch: 181, Loss: 0.553727
Train - Epoch 100, Batch: 182, Loss: 0.516863
Train - Epoch 100, Batch: 183, Loss: 0.533110
Train - Epoch 100, Batch: 184, Loss: 0.504276
Train - Epoch 100, Batch: 185, Loss: 0.544469
Train - Epoch 100, Batch: 186, Loss: 0.537906
Train - Epoch 100, Batch: 187, Loss: 0.527596
Train - Epoch 100, Batch: 188, Loss: 0.507999
Train - Epoch 100, Batch: 189, Loss: 0.539126
Train - Epoch 100, Batch: 190, Loss: 0.555367
Train - Epoch 100, Batch: 191, Loss: 0.553118
Train - Epoch 100, Batch: 192, Loss: 0.549370
Train - Epoch 100, Batch: 193, Loss: 0.526742
Train - Epoch 100, Batch: 194, Loss: 0.561043
Train - Epoch 100, Batch: 195, Loss: 0.518031
Train - Epoch 100, Batch: 196, Loss: 0.546246
Train - Epoch 100, Batch: 197, Loss: 0.553653
Train - Epoch 100, Batch: 198, Loss: 0.528900
Train - Epoch 100, Batch: 199, Loss: 0.543507
Train - Epoch 100, Batch: 200, Loss: 0.500596
Train - Epoch 100, Batch: 201, Loss: 0.541651
Train - Epoch 100, Batch: 202, Loss: 0.511338
Train - Epoch 100, Batch: 203, Loss: 0.546180
Train - Epoch 100, Batch: 204, Loss: 0.537566
Train - Epoch 100, Batch: 205, Loss: 0.542680
Train - Epoch 100, Batch: 206, Loss: 0.524567
Train - Epoch 100, Batch: 207, Loss: 0.534979
Train - Epoch 100, Batch: 208, Loss: 0.539417
Train - Epoch 100, Batch: 209, Loss: 0.557650
Train - Epoch 100, Batch: 210, Loss: 0.528438
Train - Epoch 100, Batch: 211, Loss: 0.537738
Train - Epoch 100, Batch: 212, Loss: 0.523870
Train - Epoch 100, Batch: 213, Loss: 0.535112
Train - Epoch 100, Batch: 214, Loss: 0.541584
Train - Epoch 100, Batch: 215, Loss: 0.508818
Train - Epoch 100, Batch: 216, Loss: 0.536835
Train - Epoch 100, Batch: 217, Loss: 0.528881
Train - Epoch 100, Batch: 218, Loss: 0.564092
Train - Epoch 100, Batch: 219, Loss: 0.508956
Train - Epoch 100, Batch: 220, Loss: 0.528962
Train - Epoch 100, Batch: 221, Loss: 0.535775
Train - Epoch 100, Batch: 222, Loss: 0.516979
Train - Epoch 100, Batch: 223, Loss: 0.551047
Train - Epoch 100, Batch: 224, Loss: 0.514260
Train - Epoch 100, Batch: 225, Loss: 0.514202
Train - Epoch 100, Batch: 226, Loss: 0.538224
Train - Epoch 100, Batch: 227, Loss: 0.521290
Train - Epoch 100, Batch: 228, Loss: 0.549521
Train - Epoch 100, Batch: 229, Loss: 0.575882
Train - Epoch 100, Batch: 230, Loss: 0.504646
Train - Epoch 100, Batch: 231, Loss: 0.527891
Train - Epoch 100, Batch: 232, Loss: 0.517303
Train - Epoch 100, Batch: 233, Loss: 0.528247
Train - Epoch 100, Batch: 234, Loss: 0.536085
Train - Epoch 100, Batch: 235, Loss: 0.539768
Train - Epoch 100, Batch: 236, Loss: 0.547180
Train - Epoch 100, Batch: 237, Loss: 0.522908
Train - Epoch 100, Batch: 238, Loss: 0.514669
Train - Epoch 100, Batch: 239, Loss: 0.531938
Train - Epoch 100, Batch: 240, Loss: 0.538853
Train - Epoch 100, Batch: 241, Loss: 0.550431
Train - Epoch 100, Batch: 242, Loss: 0.519279
Train - Epoch 100, Batch: 243, Loss: 0.550748
Train - Epoch 100, Batch: 244, Loss: 0.520866
Train - Epoch 100, Batch: 245, Loss: 0.569470
Train - Epoch 100, Batch: 246, Loss: 0.551499
Train - Epoch 100, Batch: 247, Loss: 0.539993
Train - Epoch 100, Batch: 248, Loss: 0.548554
Train - Epoch 100, Batch: 249, Loss: 0.528334
Train - Epoch 100, Batch: 250, Loss: 0.535910
Train - Epoch 100, Batch: 251, Loss: 0.537696
Train - Epoch 100, Batch: 252, Loss: 0.557045
Train - Epoch 100, Batch: 253, Loss: 0.519869
Train - Epoch 100, Batch: 254, Loss: 0.518662
Train - Epoch 100, Batch: 255, Loss: 0.528896
Train - Epoch 100, Batch: 256, Loss: 0.536863
Train - Epoch 100, Batch: 257, Loss: 0.522241
Train - Epoch 100, Batch: 258, Loss: 0.554471
Train - Epoch 100, Batch: 259, Loss: 0.544233
Train - Epoch 100, Batch: 260, Loss: 0.541933
Train - Epoch 100, Batch: 261, Loss: 0.517391
Train - Epoch 100, Batch: 262, Loss: 0.525611
Train - Epoch 100, Batch: 263, Loss: 0.530256
Train - Epoch 100, Batch: 264, Loss: 0.511107
Train - Epoch 100, Batch: 265, Loss: 0.512380
Train - Epoch 100, Batch: 266, Loss: 0.531630
Train - Epoch 100, Batch: 267, Loss: 0.520501
Train - Epoch 100, Batch: 268, Loss: 0.544726
Train - Epoch 100, Batch: 269, Loss: 0.518320
Train - Epoch 100, Batch: 270, Loss: 0.535948
Train - Epoch 100, Batch: 271, Loss: 0.552988
Train - Epoch 100, Batch: 272, Loss: 0.522350
Train - Epoch 100, Batch: 273, Loss: 0.512557
Train - Epoch 100, Batch: 274, Loss: 0.510503
Train - Epoch 100, Batch: 275, Loss: 0.506107
Train - Epoch 100, Batch: 276, Loss: 0.559422
Train - Epoch 100, Batch: 277, Loss: 0.526391
Train - Epoch 100, Batch: 278, Loss: 0.534459
Train - Epoch 100, Batch: 279, Loss: 0.517936
Train - Epoch 100, Batch: 280, Loss: 0.542293
Train - Epoch 100, Batch: 281, Loss: 0.544958
Train - Epoch 100, Batch: 282, Loss: 0.541038
Train - Epoch 100, Batch: 283, Loss: 0.549181
Train - Epoch 100, Batch: 284, Loss: 0.524052
Train - Epoch 100, Batch: 285, Loss: 0.537181
Train - Epoch 100, Batch: 286, Loss: 0.537001
Train - Epoch 100, Batch: 287, Loss: 0.565967
Train - Epoch 100, Batch: 288, Loss: 0.553911
Train - Epoch 100, Batch: 289, Loss: 0.504478
Train - Epoch 100, Batch: 290, Loss: 0.549138
Train - Epoch 100, Batch: 291, Loss: 0.544520
Train - Epoch 100, Batch: 292, Loss: 0.516571
Train - Epoch 100, Batch: 293, Loss: 0.557239
Train - Epoch 100, Batch: 294, Loss: 0.560564
Train - Epoch 100, Batch: 295, Loss: 0.523680
Train - Epoch 100, Batch: 296, Loss: 0.551957
Train - Epoch 100, Batch: 297, Loss: 0.557418
Train - Epoch 100, Batch: 298, Loss: 0.535283
Train - Epoch 100, Batch: 299, Loss: 0.530777
Train - Epoch 100, Batch: 300, Loss: 0.555494
Train - Epoch 100, Batch: 301, Loss: 0.511163
Train - Epoch 100, Batch: 302, Loss: 0.519479
Train - Epoch 100, Batch: 303, Loss: 0.550118
Train - Epoch 100, Batch: 304, Loss: 0.532793
Train - Epoch 100, Batch: 305, Loss: 0.534829
Train - Epoch 100, Batch: 306, Loss: 0.520014
Train - Epoch 100, Batch: 307, Loss: 0.543783
Train - Epoch 100, Batch: 308, Loss: 0.558738
Train - Epoch 100, Batch: 309, Loss: 0.550229
Train - Epoch 100, Batch: 310, Loss: 0.507891
Train - Epoch 100, Batch: 311, Loss: 0.517758
Train - Epoch 100, Batch: 312, Loss: 0.551310
Train - Epoch 100, Batch: 313, Loss: 0.537001
Train - Epoch 100, Batch: 314, Loss: 0.532245
Train - Epoch 100, Batch: 315, Loss: 0.511523
Train - Epoch 100, Batch: 316, Loss: 0.519808
Train - Epoch 100, Batch: 317, Loss: 0.516044
Train - Epoch 100, Batch: 318, Loss: 0.526782
Train - Epoch 100, Batch: 319, Loss: 0.545552
Train - Epoch 100, Batch: 320, Loss: 0.527148
Train - Epoch 100, Batch: 321, Loss: 0.524948
Train - Epoch 100, Batch: 322, Loss: 0.536934
Train - Epoch 100, Batch: 323, Loss: 0.561962
Train - Epoch 100, Batch: 324, Loss: 0.539431
Train - Epoch 100, Batch: 325, Loss: 0.539045
Train - Epoch 100, Batch: 326, Loss: 0.523860
Train - Epoch 100, Batch: 327, Loss: 0.506015
Train - Epoch 100, Batch: 328, Loss: 0.522563
Train - Epoch 100, Batch: 329, Loss: 0.525075
Train - Epoch 100, Batch: 330, Loss: 0.551356
Train - Epoch 100, Batch: 331, Loss: 0.548631
Train - Epoch 100, Batch: 332, Loss: 0.528264
Train - Epoch 100, Batch: 333, Loss: 0.497093
Train - Epoch 100, Batch: 334, Loss: 0.561132
Train - Epoch 100, Batch: 335, Loss: 0.555482
Train - Epoch 100, Batch: 336, Loss: 0.527581
Train - Epoch 100, Batch: 337, Loss: 0.544874
Train - Epoch 100, Batch: 338, Loss: 0.525651
Train - Epoch 100, Batch: 339, Loss: 0.511171
Train - Epoch 100, Batch: 340, Loss: 0.550061
Train - Epoch 100, Batch: 341, Loss: 0.541198
Train - Epoch 100, Batch: 342, Loss: 0.520917
Train - Epoch 100, Batch: 343, Loss: 0.550723
Train - Epoch 100, Batch: 344, Loss: 0.490161
Train - Epoch 100, Batch: 345, Loss: 0.533949
Train - Epoch 100, Batch: 346, Loss: 0.533466
Train - Epoch 100, Batch: 347, Loss: 0.518251
Train - Epoch 100, Batch: 348, Loss: 0.530737
Train - Epoch 100, Batch: 349, Loss: 0.541906
Train - Epoch 100, Batch: 350, Loss: 0.560182
Train - Epoch 100, Batch: 351, Loss: 0.544936
Train - Epoch 100, Batch: 352, Loss: 0.554262
Train - Epoch 100, Batch: 353, Loss: 0.526239
Train - Epoch 100, Batch: 354, Loss: 0.537209
Train - Epoch 100, Batch: 355, Loss: 0.529568
Train - Epoch 100, Batch: 356, Loss: 0.531332
Train - Epoch 100, Batch: 357, Loss: 0.525467
Train - Epoch 100, Batch: 358, Loss: 0.543510
Train - Epoch 100, Batch: 359, Loss: 0.527957
Train - Epoch 100, Batch: 360, Loss: 0.521341
Train - Epoch 100, Batch: 361, Loss: 0.539311
Train - Epoch 100, Batch: 362, Loss: 0.529302
Train - Epoch 100, Batch: 363, Loss: 0.521138
Train - Epoch 100, Batch: 364, Loss: 0.532954
Train - Epoch 100, Batch: 365, Loss: 0.532304
Train - Epoch 100, Batch: 366, Loss: 0.537704
Train - Epoch 100, Batch: 367, Loss: 0.533431
Train - Epoch 100, Batch: 368, Loss: 0.548575
Train - Epoch 100, Batch: 369, Loss: 0.578166
Train - Epoch 100, Batch: 370, Loss: 0.518228
Train - Epoch 100, Batch: 371, Loss: 0.525246
Train - Epoch 100, Batch: 372, Loss: 0.552508
Train - Epoch 100, Batch: 373, Loss: 0.515893
Train - Epoch 100, Batch: 374, Loss: 0.542006
Train - Epoch 100, Batch: 375, Loss: 0.534338
Train - Epoch 100, Batch: 376, Loss: 0.564528
Train - Epoch 100, Batch: 377, Loss: 0.520913
Train - Epoch 100, Batch: 378, Loss: 0.543618
Train - Epoch 100, Batch: 379, Loss: 0.546168
Train - Epoch 100, Batch: 380, Loss: 0.524418
Train - Epoch 100, Batch: 381, Loss: 0.522802
Train - Epoch 100, Batch: 382, Loss: 0.536549
Train - Epoch 100, Batch: 383, Loss: 0.524536
Train - Epoch 100, Batch: 384, Loss: 0.553886
Train - Epoch 100, Batch: 385, Loss: 0.540037
Train - Epoch 100, Batch: 386, Loss: 0.519962
Train - Epoch 100, Batch: 387, Loss: 0.516248
Train - Epoch 100, Batch: 388, Loss: 0.535008
Train - Epoch 100, Batch: 389, Loss: 0.554953
Train - Epoch 100, Batch: 390, Loss: 0.525735
Train - Epoch 100, Batch: 391, Loss: 0.536022
Train - Epoch 100, Batch: 392, Loss: 0.563712
Train - Epoch 100, Batch: 393, Loss: 0.526699
Train - Epoch 100, Batch: 394, Loss: 0.552387
Train - Epoch 100, Batch: 395, Loss: 0.530403
Train - Epoch 100, Batch: 396, Loss: 0.544139
Train - Epoch 100, Batch: 397, Loss: 0.520608
Train - Epoch 100, Batch: 398, Loss: 0.560782
Train - Epoch 100, Batch: 399, Loss: 0.538757
Train - Epoch 100, Batch: 400, Loss: 0.509852
Train - Epoch 100, Batch: 401, Loss: 0.518630
Train - Epoch 100, Batch: 402, Loss: 0.538190
Train - Epoch 100, Batch: 403, Loss: 0.526371
Train - Epoch 100, Batch: 404, Loss: 0.529274
Train - Epoch 100, Batch: 405, Loss: 0.508258
Train - Epoch 100, Batch: 406, Loss: 0.520031
Train - Epoch 100, Batch: 407, Loss: 0.511958
Train - Epoch 100, Batch: 408, Loss: 0.528412
Train - Epoch 100, Batch: 409, Loss: 0.564447
Train - Epoch 100, Batch: 410, Loss: 0.522085
Train - Epoch 100, Batch: 411, Loss: 0.536941
Train - Epoch 100, Batch: 412, Loss: 0.574265
Train - Epoch 100, Batch: 413, Loss: 0.539934
Train - Epoch 100, Batch: 414, Loss: 0.534676
Train - Epoch 100, Batch: 415, Loss: 0.534141
Train - Epoch 100, Batch: 416, Loss: 0.523161
Train - Epoch 100, Batch: 417, Loss: 0.506997
Train - Epoch 100, Batch: 418, Loss: 0.533562
Train - Epoch 100, Batch: 419, Loss: 0.539464
Train - Epoch 100, Batch: 420, Loss: 0.563592
Train - Epoch 100, Batch: 421, Loss: 0.539373
Train - Epoch 100, Batch: 422, Loss: 0.534390
Train - Epoch 100, Batch: 423, Loss: 0.557538
Train - Epoch 100, Batch: 424, Loss: 0.534824
Train - Epoch 100, Batch: 425, Loss: 0.528057
Train - Epoch 100, Batch: 426, Loss: 0.515246
Train - Epoch 100, Batch: 427, Loss: 0.537387
Train - Epoch 100, Batch: 428, Loss: 0.534062
Train - Epoch 100, Batch: 429, Loss: 0.540872
Train - Epoch 100, Batch: 430, Loss: 0.542801
Train - Epoch 100, Batch: 431, Loss: 0.542490
Train - Epoch 100, Batch: 432, Loss: 0.533262
Train - Epoch 100, Batch: 433, Loss: 0.540309
Train - Epoch 100, Batch: 434, Loss: 0.545880
Train - Epoch 100, Batch: 435, Loss: 0.560799
Train - Epoch 100, Batch: 436, Loss: 0.550911
Train - Epoch 100, Batch: 437, Loss: 0.534200
Train - Epoch 100, Batch: 438, Loss: 0.568063
Train - Epoch 100, Batch: 439, Loss: 0.559868
Train - Epoch 100, Batch: 440, Loss: 0.539069
Train - Epoch 100, Batch: 441, Loss: 0.524825
Train - Epoch 100, Batch: 442, Loss: 0.533984
Train - Epoch 100, Batch: 443, Loss: 0.547272
Train - Epoch 100, Batch: 444, Loss: 0.574806
Train - Epoch 100, Batch: 445, Loss: 0.535256
Train - Epoch 100, Batch: 446, Loss: 0.513485
Train - Epoch 100, Batch: 447, Loss: 0.530212
Train - Epoch 100, Batch: 448, Loss: 0.535597
Train - Epoch 100, Batch: 449, Loss: 0.539750
Train - Epoch 100, Batch: 450, Loss: 0.526341
Train - Epoch 100, Batch: 451, Loss: 0.544825
Train - Epoch 100, Batch: 452, Loss: 0.562359
Train - Epoch 100, Batch: 453, Loss: 0.541483
Train - Epoch 100, Batch: 454, Loss: 0.503132
Train - Epoch 100, Batch: 455, Loss: 0.545068
Train - Epoch 100, Batch: 456, Loss: 0.558407
Train - Epoch 100, Batch: 457, Loss: 0.525816
Train - Epoch 100, Batch: 458, Loss: 0.537663
Train - Epoch 100, Batch: 459, Loss: 0.528696
Train - Epoch 100, Batch: 460, Loss: 0.550174
Train - Epoch 100, Batch: 461, Loss: 0.541066
Train - Epoch 100, Batch: 462, Loss: 0.571267
Train - Epoch 100, Batch: 463, Loss: 0.522068
Train - Epoch 100, Batch: 464, Loss: 0.498896
Train - Epoch 100, Batch: 465, Loss: 0.553491
Train - Epoch 100, Batch: 466, Loss: 0.540327
Train - Epoch 100, Batch: 467, Loss: 0.540920
Train - Epoch 100, Batch: 468, Loss: 0.544931
Train - Epoch 100, Batch: 469, Loss: 0.526978
Train - Epoch 100, Batch: 470, Loss: 0.533604
Train - Epoch 100, Batch: 471, Loss: 0.522965
Train - Epoch 100, Batch: 472, Loss: 0.561254
Train - Epoch 100, Batch: 473, Loss: 0.534040
Train - Epoch 100, Batch: 474, Loss: 0.538256
Train - Epoch 100, Batch: 475, Loss: 0.508577
Train - Epoch 100, Batch: 476, Loss: 0.522893
Train - Epoch 100, Batch: 477, Loss: 0.542049
Train - Epoch 100, Batch: 478, Loss: 0.563699
Train - Epoch 100, Batch: 479, Loss: 0.539803
Train - Epoch 100, Batch: 480, Loss: 0.542612
Train - Epoch 100, Batch: 481, Loss: 0.508843
Train - Epoch 100, Batch: 482, Loss: 0.522590
Train - Epoch 100, Batch: 483, Loss: 0.521856
Train - Epoch 100, Batch: 484, Loss: 0.539136
Train - Epoch 100, Batch: 485, Loss: 0.566156
Train - Epoch 100, Batch: 486, Loss: 0.552579
Train - Epoch 100, Batch: 487, Loss: 0.545558
Train - Epoch 100, Batch: 488, Loss: 0.526941
Train - Epoch 100, Batch: 489, Loss: 0.521495
Train - Epoch 100, Batch: 490, Loss: 0.551948
Train - Epoch 100, Batch: 491, Loss: 0.561371
Train - Epoch 100, Batch: 492, Loss: 0.515034
Train - Epoch 100, Batch: 493, Loss: 0.540617
Train - Epoch 100, Batch: 494, Loss: 0.525489
Train - Epoch 100, Batch: 495, Loss: 0.520004
Train - Epoch 100, Batch: 496, Loss: 0.558308
Train - Epoch 100, Batch: 497, Loss: 0.533342
Train - Epoch 100, Batch: 498, Loss: 0.518818
Train - Epoch 100, Batch: 499, Loss: 0.526778
Train - Epoch 100, Batch: 500, Loss: 0.553018
Train - Epoch 100, Batch: 501, Loss: 0.541552
Train - Epoch 100, Batch: 502, Loss: 0.546332
Train - Epoch 100, Batch: 503, Loss: 0.537784
Train - Epoch 100, Batch: 504, Loss: 0.547654
Train - Epoch 100, Batch: 505, Loss: 0.541664
Train - Epoch 100, Batch: 506, Loss: 0.518548
Train - Epoch 100, Batch: 507, Loss: 0.528914
Train - Epoch 100, Batch: 508, Loss: 0.532004
Train - Epoch 100, Batch: 509, Loss: 0.556742
Train - Epoch 100, Batch: 510, Loss: 0.515182
Train - Epoch 100, Batch: 511, Loss: 0.519178
Train - Epoch 100, Batch: 512, Loss: 0.528201
Train - Epoch 100, Batch: 513, Loss: 0.539180
Train - Epoch 100, Batch: 514, Loss: 0.508948
Train - Epoch 100, Batch: 515, Loss: 0.522210
Train - Epoch 100, Batch: 516, Loss: 0.525064
Train - Epoch 100, Batch: 517, Loss: 0.527727
Train - Epoch 100, Batch: 518, Loss: 0.554251
Train - Epoch 100, Batch: 519, Loss: 0.563662
Train - Epoch 100, Batch: 520, Loss: 0.500387
Train - Epoch 100, Batch: 521, Loss: 0.536743
Train - Epoch 100, Batch: 522, Loss: 0.526288
Train - Epoch 100, Batch: 523, Loss: 0.560219
Train - Epoch 100, Batch: 524, Loss: 0.530223
Train - Epoch 100, Batch: 525, Loss: 0.524018
Train - Epoch 100, Batch: 526, Loss: 0.552994
Train - Epoch 100, Batch: 527, Loss: 0.539657
Train - Epoch 100, Batch: 528, Loss: 0.547845
Train - Epoch 100, Batch: 529, Loss: 0.531406
Train - Epoch 100, Batch: 530, Loss: 0.517832
Train - Epoch 100, Batch: 531, Loss: 0.507435
Train - Epoch 100, Batch: 532, Loss: 0.545829
Train - Epoch 100, Batch: 533, Loss: 0.532748
Train - Epoch 100, Batch: 534, Loss: 0.539744
Train - Epoch 100, Batch: 535, Loss: 0.532736
Train - Epoch 100, Batch: 536, Loss: 0.531293
Train - Epoch 100, Batch: 537, Loss: 0.510837
Train - Epoch 100, Batch: 538, Loss: 0.532068
Train - Epoch 100, Batch: 539, Loss: 0.515082
Train - Epoch 100, Batch: 540, Loss: 0.520837
Train - Epoch 100, Batch: 541, Loss: 0.539902
Train - Epoch 100, Batch: 542, Loss: 0.534711
Train - Epoch 100, Batch: 543, Loss: 0.521896
Train - Epoch 100, Batch: 544, Loss: 0.552970
Train - Epoch 100, Batch: 545, Loss: 0.519402
Train - Epoch 100, Batch: 546, Loss: 0.556486
Train - Epoch 100, Batch: 547, Loss: 0.505319
Train - Epoch 100, Batch: 548, Loss: 0.546418
Train - Epoch 100, Batch: 549, Loss: 0.524887
Train - Epoch 100, Batch: 550, Loss: 0.539859
Train - Epoch 100, Batch: 551, Loss: 0.538924
Train - Epoch 100, Batch: 552, Loss: 0.537927
Train - Epoch 100, Batch: 553, Loss: 0.546844
Train - Epoch 100, Batch: 554, Loss: 0.534938
Train - Epoch 100, Batch: 555, Loss: 0.527866
Train - Epoch 100, Batch: 556, Loss: 0.536454
Train - Epoch 100, Batch: 557, Loss: 0.552329
Train - Epoch 100, Batch: 558, Loss: 0.534702
Train - Epoch 100, Batch: 559, Loss: 0.522936
Train - Epoch 100, Batch: 560, Loss: 0.532435
Train - Epoch 100, Batch: 561, Loss: 0.537063
Train - Epoch 100, Batch: 562, Loss: 0.533094
Train - Epoch 100, Batch: 563, Loss: 0.545253
Train - Epoch 100, Batch: 564, Loss: 0.533964
Train - Epoch 100, Batch: 565, Loss: 0.545319
Train - Epoch 100, Batch: 566, Loss: 0.544189
Train - Epoch 100, Batch: 567, Loss: 0.545707
Train - Epoch 100, Batch: 568, Loss: 0.534026
Train - Epoch 100, Batch: 569, Loss: 0.510849
Train - Epoch 100, Batch: 570, Loss: 0.509236
Train - Epoch 100, Batch: 571, Loss: 0.539771
Train - Epoch 100, Batch: 572, Loss: 0.534790
Train - Epoch 100, Batch: 573, Loss: 0.537801
Train - Epoch 100, Batch: 574, Loss: 0.547659
Train - Epoch 100, Batch: 575, Loss: 0.541076
Train - Epoch 100, Batch: 576, Loss: 0.515585
Train - Epoch 100, Batch: 577, Loss: 0.547117
Train - Epoch 100, Batch: 578, Loss: 0.565559
Train - Epoch 100, Batch: 579, Loss: 0.549726
Train - Epoch 100, Batch: 580, Loss: 0.533977
Train - Epoch 100, Batch: 581, Loss: 0.569550
Train - Epoch 100, Batch: 582, Loss: 0.554643
Train - Epoch 100, Batch: 583, Loss: 0.528063
Train - Epoch 100, Batch: 584, Loss: 0.550703
Train - Epoch 100, Batch: 585, Loss: 0.561059
Train - Epoch 100, Batch: 586, Loss: 0.571971
Train - Epoch 100, Batch: 587, Loss: 0.523904
Train - Epoch 100, Batch: 588, Loss: 0.523705
Train - Epoch 100, Batch: 589, Loss: 0.504669
Train - Epoch 100, Batch: 590, Loss: 0.538390
Train - Epoch 100, Batch: 591, Loss: 0.541264
Train - Epoch 100, Batch: 592, Loss: 0.553422
Train - Epoch 100, Batch: 593, Loss: 0.539837
Train - Epoch 100, Batch: 594, Loss: 0.562217
Train - Epoch 100, Batch: 595, Loss: 0.525239
Train - Epoch 100, Batch: 596, Loss: 0.538904
Train - Epoch 100, Batch: 597, Loss: 0.537726
Train - Epoch 100, Batch: 598, Loss: 0.539875
Train - Epoch 100, Batch: 599, Loss: 0.528745
Train - Epoch 100, Batch: 600, Loss: 0.526146
Train - Epoch 100, Batch: 601, Loss: 0.526656
Train - Epoch 100, Batch: 602, Loss: 0.538098
Train - Epoch 100, Batch: 603, Loss: 0.539957
Train - Epoch 100, Batch: 604, Loss: 0.552963
Train - Epoch 100, Batch: 605, Loss: 0.505912
Train - Epoch 100, Batch: 606, Loss: 0.529288
Train - Epoch 100, Batch: 607, Loss: 0.544233
Train - Epoch 100, Batch: 608, Loss: 0.542065
Train - Epoch 100, Batch: 609, Loss: 0.569373
Train - Epoch 100, Batch: 610, Loss: 0.527904
Train - Epoch 100, Batch: 611, Loss: 0.529576
Train - Epoch 100, Batch: 612, Loss: 0.527143
Train - Epoch 100, Batch: 613, Loss: 0.516352
Train - Epoch 100, Batch: 614, Loss: 0.485211
Train - Epoch 100, Batch: 615, Loss: 0.538176
Train - Epoch 100, Batch: 616, Loss: 0.503610
Train - Epoch 100, Batch: 617, Loss: 0.495933
Train - Epoch 100, Batch: 618, Loss: 0.537824
Train - Epoch 100, Batch: 619, Loss: 0.538733
Train - Epoch 100, Batch: 620, Loss: 0.530287
Train - Epoch 100, Batch: 621, Loss: 0.529972
Train - Epoch 100, Batch: 622, Loss: 0.511392
Train - Epoch 100, Batch: 623, Loss: 0.532879
Train - Epoch 100, Batch: 624, Loss: 0.523590
Train - Epoch 100, Batch: 625, Loss: 0.518806
Train - Epoch 100, Batch: 626, Loss: 0.561942
Train - Epoch 100, Batch: 627, Loss: 0.529980
Train - Epoch 100, Batch: 628, Loss: 0.513789
Train - Epoch 100, Batch: 629, Loss: 0.570324
Train - Epoch 100, Batch: 630, Loss: 0.535638
Train - Epoch 100, Batch: 631, Loss: 0.528643
Train - Epoch 100, Batch: 632, Loss: 0.537979
Train - Epoch 100, Batch: 633, Loss: 0.530761
Train - Epoch 100, Batch: 634, Loss: 0.545240
Train - Epoch 100, Batch: 635, Loss: 0.550080
Train - Epoch 100, Batch: 636, Loss: 0.578100
Train - Epoch 100, Batch: 637, Loss: 0.555066
Train - Epoch 100, Batch: 638, Loss: 0.496518
Train - Epoch 100, Batch: 639, Loss: 0.518589
Train - Epoch 100, Batch: 640, Loss: 0.526805
Train - Epoch 100, Batch: 641, Loss: 0.520470
Train - Epoch 100, Batch: 642, Loss: 0.551811
Train - Epoch 100, Batch: 643, Loss: 0.508815
Train - Epoch 100, Batch: 644, Loss: 0.532100
Train - Epoch 100, Batch: 645, Loss: 0.536105
Train - Epoch 100, Batch: 646, Loss: 0.523295
Train - Epoch 100, Batch: 647, Loss: 0.513392
Train - Epoch 100, Batch: 648, Loss: 0.503555
Train - Epoch 100, Batch: 649, Loss: 0.537815
Train - Epoch 100, Batch: 650, Loss: 0.524217
Train - Epoch 100, Batch: 651, Loss: 0.527908
Train - Epoch 100, Batch: 652, Loss: 0.519557
Train - Epoch 100, Batch: 653, Loss: 0.529923
Train - Epoch 100, Batch: 654, Loss: 0.519195
Train - Epoch 100, Batch: 655, Loss: 0.509858
Train - Epoch 100, Batch: 656, Loss: 0.538763
Train - Epoch 100, Batch: 657, Loss: 0.531813
Train - Epoch 100, Batch: 658, Loss: 0.529404
Train - Epoch 100, Batch: 659, Loss: 0.553928
Train - Epoch 100, Batch: 660, Loss: 0.543842
Train - Epoch 100, Batch: 661, Loss: 0.533547
Train - Epoch 100, Batch: 662, Loss: 0.537849
Train - Epoch 100, Batch: 663, Loss: 0.548827
Train - Epoch 100, Batch: 664, Loss: 0.554156
Train - Epoch 100, Batch: 665, Loss: 0.531373
Train - Epoch 100, Batch: 666, Loss: 0.526476
Train - Epoch 100, Batch: 667, Loss: 0.514080
Train - Epoch 100, Batch: 668, Loss: 0.544751
Train - Epoch 100, Batch: 669, Loss: 0.572797
Train - Epoch 100, Batch: 670, Loss: 0.550409
Train - Epoch 100, Batch: 671, Loss: 0.514725
Train - Epoch 100, Batch: 672, Loss: 0.534048
Train - Epoch 100, Batch: 673, Loss: 0.507863
Train - Epoch 100, Batch: 674, Loss: 0.539045
Train - Epoch 100, Batch: 675, Loss: 0.522145
Train - Epoch 100, Batch: 676, Loss: 0.549635
Train - Epoch 100, Batch: 677, Loss: 0.559464
Train - Epoch 100, Batch: 678, Loss: 0.522654
Train - Epoch 100, Batch: 679, Loss: 0.538363
Train - Epoch 100, Batch: 680, Loss: 0.552587
Train - Epoch 100, Batch: 681, Loss: 0.536259
Train - Epoch 100, Batch: 682, Loss: 0.503171
Train - Epoch 100, Batch: 683, Loss: 0.518321
Train - Epoch 100, Batch: 684, Loss: 0.518960
Train - Epoch 100, Batch: 685, Loss: 0.531445
Train - Epoch 100, Batch: 686, Loss: 0.528648
Train - Epoch 100, Batch: 687, Loss: 0.559619
Train - Epoch 100, Batch: 688, Loss: 0.551567
Train - Epoch 100, Batch: 689, Loss: 0.566947
Train - Epoch 100, Batch: 690, Loss: 0.535048
Train - Epoch 100, Batch: 691, Loss: 0.571095
Train - Epoch 100, Batch: 692, Loss: 0.493300
Train - Epoch 100, Batch: 693, Loss: 0.525947
Train - Epoch 100, Batch: 694, Loss: 0.544351
Train - Epoch 100, Batch: 695, Loss: 0.549672
Train - Epoch 100, Batch: 696, Loss: 0.543060
Train - Epoch 100, Batch: 697, Loss: 0.517003
Train - Epoch 100, Batch: 698, Loss: 0.533277
Train - Epoch 100, Batch: 699, Loss: 0.531400
Train - Epoch 100, Batch: 700, Loss: 0.538817
Train - Epoch 100, Batch: 701, Loss: 0.539948
Train - Epoch 100, Batch: 702, Loss: 0.543404
Train - Epoch 100, Batch: 703, Loss: 0.510842
Train - Epoch 100, Batch: 704, Loss: 0.520739
Train - Epoch 100, Batch: 705, Loss: 0.539711
Train - Epoch 100, Batch: 706, Loss: 0.530048
Train - Epoch 100, Batch: 707, Loss: 0.517638
Train - Epoch 100, Batch: 708, Loss: 0.549171
Train - Epoch 100, Batch: 709, Loss: 0.549136
Train - Epoch 100, Batch: 710, Loss: 0.522089
Train - Epoch 100, Batch: 711, Loss: 0.540033
Train - Epoch 100, Batch: 712, Loss: 0.528927
Train - Epoch 100, Batch: 713, Loss: 0.557533
Train - Epoch 100, Batch: 714, Loss: 0.517004
Train - Epoch 100, Batch: 715, Loss: 0.520038
Train - Epoch 100, Batch: 716, Loss: 0.539187
Train - Epoch 100, Batch: 717, Loss: 0.546428
Train - Epoch 100, Batch: 718, Loss: 0.538409
Train - Epoch 100, Batch: 719, Loss: 0.539907
Train - Epoch 100, Batch: 720, Loss: 0.543145
Train - Epoch 100, Batch: 721, Loss: 0.552983
Train - Epoch 100, Batch: 722, Loss: 0.540364
Train - Epoch 100, Batch: 723, Loss: 0.520728
Train - Epoch 100, Batch: 724, Loss: 0.534593
Train - Epoch 100, Batch: 725, Loss: 0.551746
Train - Epoch 100, Batch: 726, Loss: 0.532317
Train - Epoch 100, Batch: 727, Loss: 0.527466
Train - Epoch 100, Batch: 728, Loss: 0.524586
Train - Epoch 100, Batch: 729, Loss: 0.550159
Train - Epoch 100, Batch: 730, Loss: 0.542050
Train - Epoch 100, Batch: 731, Loss: 0.507325
Train - Epoch 100, Batch: 732, Loss: 0.529439
Train - Epoch 100, Batch: 733, Loss: 0.527843
Train - Epoch 100, Batch: 734, Loss: 0.543342
Train - Epoch 100, Batch: 735, Loss: 0.505213
Train - Epoch 100, Batch: 736, Loss: 0.520975
Train - Epoch 100, Batch: 737, Loss: 0.528254
Train - Epoch 100, Batch: 738, Loss: 0.501919
Train - Epoch 100, Batch: 739, Loss: 0.559560
Train - Epoch 100, Batch: 740, Loss: 0.554588
Train - Epoch 100, Batch: 741, Loss: 0.502716
Train - Epoch 100, Batch: 742, Loss: 0.512342
Train - Epoch 100, Batch: 743, Loss: 0.538936
Train - Epoch 100, Batch: 744, Loss: 0.560605
Train - Epoch 100, Batch: 745, Loss: 0.561909
Train - Epoch 100, Batch: 746, Loss: 0.490047
Train - Epoch 100, Batch: 747, Loss: 0.547502
Train - Epoch 100, Batch: 748, Loss: 0.541621
Train - Epoch 100, Batch: 749, Loss: 0.517131
Train - Epoch 100, Batch: 750, Loss: 0.522658
Train - Epoch 100, Batch: 751, Loss: 0.506837
Train - Epoch 100, Batch: 752, Loss: 0.510850
Train - Epoch 100, Batch: 753, Loss: 0.523142
Train - Epoch 100, Batch: 754, Loss: 0.528852
Train - Epoch 100, Batch: 755, Loss: 0.556790
Train - Epoch 100, Batch: 756, Loss: 0.538878
Train - Epoch 100, Batch: 757, Loss: 0.518199
Train - Epoch 100, Batch: 758, Loss: 0.550333
Train - Epoch 100, Batch: 759, Loss: 0.516263
Train - Epoch 100, Batch: 760, Loss: 0.533111
Train - Epoch 100, Batch: 761, Loss: 0.534838
Train - Epoch 100, Batch: 762, Loss: 0.530658
Train - Epoch 100, Batch: 763, Loss: 0.525459
Train - Epoch 100, Batch: 764, Loss: 0.550686
Train - Epoch 100, Batch: 765, Loss: 0.542918
Train - Epoch 100, Batch: 766, Loss: 0.527832
Train - Epoch 100, Batch: 767, Loss: 0.527982
Train - Epoch 100, Batch: 768, Loss: 0.552145
Train - Epoch 100, Batch: 769, Loss: 0.508422
Train - Epoch 100, Batch: 770, Loss: 0.537852
Train - Epoch 100, Batch: 771, Loss: 0.541587
Train - Epoch 100, Batch: 772, Loss: 0.530823
Train - Epoch 100, Batch: 773, Loss: 0.536422
Train - Epoch 100, Batch: 774, Loss: 0.543872
Train - Epoch 100, Batch: 775, Loss: 0.544052
Train - Epoch 100, Batch: 776, Loss: 0.539185
Train - Epoch 100, Batch: 777, Loss: 0.519282
Train - Epoch 100, Batch: 778, Loss: 0.554517
Train - Epoch 100, Batch: 779, Loss: 0.547176
Train - Epoch 100, Batch: 780, Loss: 0.544739
Train - Epoch 100, Batch: 781, Loss: 0.534676
Train - Epoch 100, Batch: 782, Loss: 0.581384
Train - Epoch 100, Batch: 783, Loss: 0.550095
Train - Epoch 100, Batch: 784, Loss: 0.505729
Train - Epoch 100, Batch: 785, Loss: 0.564475
Train - Epoch 100, Batch: 786, Loss: 0.540826
Train - Epoch 100, Batch: 787, Loss: 0.525406
Train - Epoch 100, Batch: 788, Loss: 0.531891
Train - Epoch 100, Batch: 789, Loss: 0.516632
Train - Epoch 100, Batch: 790, Loss: 0.519361
Train - Epoch 100, Batch: 791, Loss: 0.535995
Train - Epoch 100, Batch: 792, Loss: 0.547362
Train - Epoch 100, Batch: 793, Loss: 0.543651
Train - Epoch 100, Batch: 794, Loss: 0.509454
Train - Epoch 100, Batch: 795, Loss: 0.497995
Train - Epoch 100, Batch: 796, Loss: 0.508753
Train - Epoch 100, Batch: 797, Loss: 0.543905
Train - Epoch 100, Batch: 798, Loss: 0.505660
Train - Epoch 100, Batch: 799, Loss: 0.543552
Train - Epoch 100, Batch: 800, Loss: 0.517088
Train - Epoch 100, Batch: 801, Loss: 0.561934
Train - Epoch 100, Batch: 802, Loss: 0.531140
Train - Epoch 100, Batch: 803, Loss: 0.520673
Train - Epoch 100, Batch: 804, Loss: 0.526519
Train - Epoch 100, Batch: 805, Loss: 0.528477
Train - Epoch 100, Batch: 806, Loss: 0.530754
Train - Epoch 100, Batch: 807, Loss: 0.523205
Train - Epoch 100, Batch: 808, Loss: 0.530622
Train - Epoch 100, Batch: 809, Loss: 0.554294
Train - Epoch 100, Batch: 810, Loss: 0.510843
Train - Epoch 100, Batch: 811, Loss: 0.525105
Train - Epoch 100, Batch: 812, Loss: 0.530937
Train - Epoch 100, Batch: 813, Loss: 0.545409
Train - Epoch 100, Batch: 814, Loss: 0.545266
Train - Epoch 100, Batch: 815, Loss: 0.532220
Train - Epoch 100, Batch: 816, Loss: 0.531871
Train - Epoch 100, Batch: 817, Loss: 0.546988
Train - Epoch 100, Batch: 818, Loss: 0.551267
Train - Epoch 100, Batch: 819, Loss: 0.524017
Train - Epoch 100, Batch: 820, Loss: 0.532633
Train - Epoch 100, Batch: 821, Loss: 0.519699
Train - Epoch 100, Batch: 822, Loss: 0.515217
Train - Epoch 100, Batch: 823, Loss: 0.509130
Train - Epoch 100, Batch: 824, Loss: 0.525823
Train - Epoch 100, Batch: 825, Loss: 0.557877
Train - Epoch 100, Batch: 826, Loss: 0.531972
Train - Epoch 100, Batch: 827, Loss: 0.520830
Train - Epoch 100, Batch: 828, Loss: 0.526594
Train - Epoch 100, Batch: 829, Loss: 0.538416
Train - Epoch 100, Batch: 830, Loss: 0.535970
Train - Epoch 100, Batch: 831, Loss: 0.527565
Train - Epoch 100, Batch: 832, Loss: 0.530577
Train - Epoch 100, Batch: 833, Loss: 0.545620
Train - Epoch 100, Batch: 834, Loss: 0.542358
Train - Epoch 100, Batch: 835, Loss: 0.531301
Train - Epoch 100, Batch: 836, Loss: 0.537685
Train - Epoch 100, Batch: 837, Loss: 0.531502
Train - Epoch 100, Batch: 838, Loss: 0.520724
Train - Epoch 100, Batch: 839, Loss: 0.525373
Train - Epoch 100, Batch: 840, Loss: 0.542200
Train - Epoch 100, Batch: 841, Loss: 0.576050
Train - Epoch 100, Batch: 842, Loss: 0.513014
Train - Epoch 100, Batch: 843, Loss: 0.527991
Train - Epoch 100, Batch: 844, Loss: 0.560246
Train - Epoch 100, Batch: 845, Loss: 0.565406
Train - Epoch 100, Batch: 846, Loss: 0.552618
Train - Epoch 100, Batch: 847, Loss: 0.537694
Train - Epoch 100, Batch: 848, Loss: 0.527782
Train - Epoch 100, Batch: 849, Loss: 0.542840
Train - Epoch 100, Batch: 850, Loss: 0.485740
Train - Epoch 100, Batch: 851, Loss: 0.520949
Train - Epoch 100, Batch: 852, Loss: 0.550544
Train - Epoch 100, Batch: 853, Loss: 0.536726
Train - Epoch 100, Batch: 854, Loss: 0.567963
Train - Epoch 100, Batch: 855, Loss: 0.529941
Train - Epoch 100, Batch: 856, Loss: 0.514571
Train - Epoch 100, Batch: 857, Loss: 0.535431
Train - Epoch 100, Batch: 858, Loss: 0.540165
Train - Epoch 100, Batch: 859, Loss: 0.549095
Train - Epoch 100, Batch: 860, Loss: 0.542722
Train - Epoch 100, Batch: 861, Loss: 0.542898
Train - Epoch 100, Batch: 862, Loss: 0.551433
Train - Epoch 100, Batch: 863, Loss: 0.515410
Train - Epoch 100, Batch: 864, Loss: 0.527382
Train - Epoch 100, Batch: 865, Loss: 0.563579
Train - Epoch 100, Batch: 866, Loss: 0.533385
Train - Epoch 100, Batch: 867, Loss: 0.556508
Train - Epoch 100, Batch: 868, Loss: 0.514259
Train - Epoch 100, Batch: 869, Loss: 0.541931
Train - Epoch 100, Batch: 870, Loss: 0.537082
Train - Epoch 100, Batch: 871, Loss: 0.556606
Train - Epoch 100, Batch: 872, Loss: 0.520020
Train - Epoch 100, Batch: 873, Loss: 0.518095
Train - Epoch 100, Batch: 874, Loss: 0.529489
Train - Epoch 100, Batch: 875, Loss: 0.557436
Train - Epoch 100, Batch: 876, Loss: 0.529980
Train - Epoch 100, Batch: 877, Loss: 0.522319
Train - Epoch 100, Batch: 878, Loss: 0.550371
Train - Epoch 100, Batch: 879, Loss: 0.555363
Train - Epoch 100, Batch: 880, Loss: 0.523386
Train - Epoch 100, Batch: 881, Loss: 0.498018
Train - Epoch 100, Batch: 882, Loss: 0.531682
Train - Epoch 100, Batch: 883, Loss: 0.519722
Train - Epoch 100, Batch: 884, Loss: 0.541427
Train - Epoch 100, Batch: 885, Loss: 0.532982
Train - Epoch 100, Batch: 886, Loss: 0.537398
Train - Epoch 100, Batch: 887, Loss: 0.521186
Train - Epoch 100, Batch: 888, Loss: 0.530738
Train - Epoch 100, Batch: 889, Loss: 0.524738
Train - Epoch 100, Batch: 890, Loss: 0.519859
Train - Epoch 100, Batch: 891, Loss: 0.516104
Train - Epoch 100, Batch: 892, Loss: 0.578080
Train - Epoch 100, Batch: 893, Loss: 0.506768
Train - Epoch 100, Batch: 894, Loss: 0.530805
Train - Epoch 100, Batch: 895, Loss: 0.540174
Train - Epoch 100, Batch: 896, Loss: 0.518644
Train - Epoch 100, Batch: 897, Loss: 0.545878
Train - Epoch 100, Batch: 898, Loss: 0.534579
Train - Epoch 100, Batch: 899, Loss: 0.525248
Train - Epoch 100, Batch: 900, Loss: 0.534939
Train - Epoch 100, Batch: 901, Loss: 0.533377
Train - Epoch 100, Batch: 902, Loss: 0.528829
Train - Epoch 100, Batch: 903, Loss: 0.532223
Train - Epoch 100, Batch: 904, Loss: 0.524952
Train - Epoch 100, Batch: 905, Loss: 0.524384
Train - Epoch 100, Batch: 906, Loss: 0.568934
Train - Epoch 100, Batch: 907, Loss: 0.575689
Train - Epoch 100, Batch: 908, Loss: 0.520798
Train - Epoch 100, Batch: 909, Loss: 0.560102
Train - Epoch 100, Batch: 910, Loss: 0.517104
Train - Epoch 100, Batch: 911, Loss: 0.504796
Train - Epoch 100, Batch: 912, Loss: 0.537320
Train - Epoch 100, Batch: 913, Loss: 0.520362
Train - Epoch 100, Batch: 914, Loss: 0.556982
Train - Epoch 100, Batch: 915, Loss: 0.538061
Train - Epoch 100, Batch: 916, Loss: 0.536096
Train - Epoch 100, Batch: 917, Loss: 0.537764
Train - Epoch 100, Batch: 918, Loss: 0.540380
Train - Epoch 100, Batch: 919, Loss: 0.504203
Train - Epoch 100, Batch: 920, Loss: 0.507862
Train - Epoch 100, Batch: 921, Loss: 0.540951
Train - Epoch 100, Batch: 922, Loss: 0.537414
Train - Epoch 100, Batch: 923, Loss: 0.534531
Train - Epoch 100, Batch: 924, Loss: 0.541876
Train - Epoch 100, Batch: 925, Loss: 0.518654
Train - Epoch 100, Batch: 926, Loss: 0.517617
Train - Epoch 100, Batch: 927, Loss: 0.526123
Train - Epoch 100, Batch: 928, Loss: 0.536743
Train - Epoch 100, Batch: 929, Loss: 0.548569
Train - Epoch 100, Batch: 930, Loss: 0.509373
Train - Epoch 100, Batch: 931, Loss: 0.520653
Train - Epoch 100, Batch: 932, Loss: 0.501793
Train - Epoch 100, Batch: 933, Loss: 0.531799
Train - Epoch 100, Batch: 934, Loss: 0.538977
Train - Epoch 100, Batch: 935, Loss: 0.562579
Train - Epoch 100, Batch: 936, Loss: 0.528641
Train - Epoch 100, Batch: 937, Loss: 0.550220
Train - Epoch 100, Batch: 938, Loss: 0.565145
Train - Epoch 100, Batch: 939, Loss: 0.503562
Train - Epoch 100, Batch: 940, Loss: 0.533959
Train - Epoch 100, Batch: 941, Loss: 0.534070
Train - Epoch 100, Batch: 942, Loss: 0.524386
Train - Epoch 100, Batch: 943, Loss: 0.536904
Train - Epoch 100, Batch: 944, Loss: 0.534377
Train - Epoch 100, Batch: 945, Loss: 0.537798
Train - Epoch 100, Batch: 946, Loss: 0.536356
Train - Epoch 100, Batch: 947, Loss: 0.532432
Train - Epoch 100, Batch: 948, Loss: 0.545630
Train - Epoch 100, Batch: 949, Loss: 0.538899
Train - Epoch 100, Batch: 950, Loss: 0.535702
Train - Epoch 100, Batch: 951, Loss: 0.544423
Train - Epoch 100, Batch: 952, Loss: 0.534616
Train - Epoch 100, Batch: 953, Loss: 0.559297
Train - Epoch 100, Batch: 954, Loss: 0.524751
Train - Epoch 100, Batch: 955, Loss: 0.526556
Train - Epoch 100, Batch: 956, Loss: 0.527891
Train - Epoch 100, Batch: 957, Loss: 0.534875
Train - Epoch 100, Batch: 958, Loss: 0.530111
Train - Epoch 100, Batch: 959, Loss: 0.530571
Train - Epoch 100, Batch: 960, Loss: 0.533086
Train - Epoch 100, Batch: 961, Loss: 0.559950
Train - Epoch 100, Batch: 962, Loss: 0.515827
Train - Epoch 100, Batch: 963, Loss: 0.539423
Train - Epoch 100, Batch: 964, Loss: 0.531597
Train - Epoch 100, Batch: 965, Loss: 0.526158
Train - Epoch 100, Batch: 966, Loss: 0.542694
Train - Epoch 100, Batch: 967, Loss: 0.544790
Train - Epoch 100, Batch: 968, Loss: 0.522657
Train - Epoch 100, Batch: 969, Loss: 0.541005
Train - Epoch 100, Batch: 970, Loss: 0.555494
Train - Epoch 100, Batch: 971, Loss: 0.554839
Train - Epoch 100, Batch: 972, Loss: 0.542238
Train - Epoch 100, Batch: 973, Loss: 0.529277
Train - Epoch 100, Batch: 974, Loss: 0.512350
Train - Epoch 100, Batch: 975, Loss: 0.536267
Train - Epoch 100, Batch: 976, Loss: 0.533609
Train - Epoch 100, Batch: 977, Loss: 0.520184
Train - Epoch 100, Batch: 978, Loss: 0.488898
Train - Epoch 100, Batch: 979, Loss: 0.535940
Train - Epoch 100, Batch: 980, Loss: 0.519577
Train - Epoch 100, Batch: 981, Loss: 0.512063
Train - Epoch 100, Batch: 982, Loss: 0.509011
Train - Epoch 100, Batch: 983, Loss: 0.540863
Train - Epoch 100, Batch: 984, Loss: 0.504577
Train - Epoch 100, Batch: 985, Loss: 0.514667
Train - Epoch 100, Batch: 986, Loss: 0.519405
Train - Epoch 100, Batch: 987, Loss: 0.513397
Train - Epoch 100, Batch: 988, Loss: 0.527082
Train - Epoch 100, Batch: 989, Loss: 0.521045
Train - Epoch 100, Batch: 990, Loss: 0.529435
Train - Epoch 100, Batch: 991, Loss: 0.517592
Train - Epoch 100, Batch: 992, Loss: 0.554796
Train - Epoch 100, Batch: 993, Loss: 0.529492
Train - Epoch 100, Batch: 994, Loss: 0.544868
Train - Epoch 100, Batch: 995, Loss: 0.538819
Train - Epoch 100, Batch: 996, Loss: 0.554992
Train - Epoch 100, Batch: 997, Loss: 0.503506
Train - Epoch 100, Batch: 998, Loss: 0.557653
Train - Epoch 100, Batch: 999, Loss: 0.546116
Train - Epoch 100, Batch: 1000, Loss: 0.516121
Train - Epoch 100, Batch: 1001, Loss: 0.545113
Train - Epoch 100, Batch: 1002, Loss: 0.549024
Train - Epoch 100, Batch: 1003, Loss: 0.549876
Train - Epoch 100, Batch: 1004, Loss: 0.568293
Train - Epoch 100, Batch: 1005, Loss: 0.542808
Train - Epoch 100, Batch: 1006, Loss: 0.507910
Train - Epoch 100, Batch: 1007, Loss: 0.547999
Train - Epoch 100, Batch: 1008, Loss: 0.530356
Train - Epoch 100, Batch: 1009, Loss: 0.538771
Train - Epoch 100, Batch: 1010, Loss: 0.508103
Train - Epoch 100, Batch: 1011, Loss: 0.559302
Train - Epoch 100, Batch: 1012, Loss: 0.533032
Train - Epoch 100, Batch: 1013, Loss: 0.546947
Train - Epoch 100, Batch: 1014, Loss: 0.553853
Train - Epoch 100, Batch: 1015, Loss: 0.511964
Train - Epoch 100, Batch: 1016, Loss: 0.519101
Train - Epoch 100, Batch: 1017, Loss: 0.547880
Train - Epoch 100, Batch: 1018, Loss: 0.562345
Train - Epoch 100, Batch: 1019, Loss: 0.527645
Train - Epoch 100, Batch: 1020, Loss: 0.545319
Train - Epoch 100, Batch: 1021, Loss: 0.580691
training_time:: 123.49195599555969
training time full:: 123.49217200279236
provenance prepare time:: 0.0
here
Test Avg. Loss: 0.001047, Accuracy: 0.744914, F1 Score: 0.751213
</training>
torch.Size([522910, 54])
tensor([336724, 387882, 257850,  ..., 159235, 350726, 465268])
<results lr="1" epochs="200" bz="512" remove_ratio="0.05" sampling_type="targeted_informed">
<baseline>
data dimension:: [522910, 54]
tensor([174084, 309253, 149514, 112659, 110612, 112665, 360477, 167965, 499742,
        333861, 434216,  94249, 432171, 114734, 489541, 409671, 479306, 471115,
        157772,   4171, 135254, 501855, 313443, 143468, 120944, 346226, 120954,
        239739,    124, 272520, 141450, 145550, 493712, 161941, 440470, 307352,
        467102, 411807, 118951, 221358, 477373, 186558, 389324,  45265, 110801,
        329940, 329942,  22749, 516320, 366816])
Epoch:0 Batch: 1021 Baseline Loss 0.5608118811103464
Epoch:100 Batch: 1021 Baseline Loss 0.5519492163775762
training time is 140.40421199798584
overhead:: 0
overhead2:: 29.621323585510254
overhead3:: 0
memory usage:: 4040126464
time_baseline:: 140.72179460525513
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(2.4745, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729940, F1 Score: 0.744067
Remove Test Avg. Loss: 0.001357, Accuracy: 0.538000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728777, F1 Score: 0.742669
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(2.5308, dtype=torch.float64)
Test Avg. Loss: 0.001055, Accuracy: 0.734742, F1 Score: 0.742024
Remove Test Avg. Loss: 0.001269, Accuracy: 0.560910, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001051, Accuracy: 0.734195, F1 Score: 0.741115
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(7.0406, dtype=torch.float64)
Test Avg. Loss: 0.001201, Accuracy: 0.630684, F1 Score: 0.457282
Remove Test Avg. Loss: 0.000610, Accuracy: 0.928055, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001197, Accuracy: 0.631034, F1 Score: 0.458158
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2008, dtype=torch.float64)
Test Avg. Loss: 0.008594, Accuracy: 0.502255, F1 Score: 0.009385
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008555, Accuracy: 0.502322, F1 Score: 0.008572
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.7947, dtype=torch.float64)
Test Avg. Loss: 0.091502, Accuracy: 0.488021, F1 Score: 0.009325
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091075, Accuracy: 0.487550, F1 Score: 0.008716
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(2.4730, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.731059, F1 Score: 0.744096
Remove Test Avg. Loss: 0.001347, Accuracy: 0.541786, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.729743, F1 Score: 0.742515
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(2.5423, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.738099, F1 Score: 0.731258
Remove Test Avg. Loss: 0.001178, Accuracy: 0.606540, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.736993, F1 Score: 0.729941
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.9709, dtype=torch.float64)
Test Avg. Loss: 0.001768, Accuracy: 0.554559, F1 Score: 0.183899
Remove Test Avg. Loss: 0.000245, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001761, Accuracy: 0.555790, F1 Score: 0.186477
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.4216, dtype=torch.float64)
Test Avg. Loss: 0.018964, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018885, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.4254, dtype=torch.float64)
Test Avg. Loss: 0.194935, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194123, Accuracy: 0.512245, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(2.4848, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729631, F1 Score: 0.743790
Remove Test Avg. Loss: 0.001360, Accuracy: 0.537541, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728611, F1 Score: 0.742520
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(2.6436, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.734243, F1 Score: 0.741742
Remove Test Avg. Loss: 0.001305, Accuracy: 0.555862, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.733474, F1 Score: 0.740699
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.9107, dtype=torch.float64)
Test Avg. Loss: 0.001166, Accuracy: 0.674710, F1 Score: 0.585908
Remove Test Avg. Loss: 0.000852, Accuracy: 0.794072, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001163, Accuracy: 0.674135, F1 Score: 0.585797
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(73.2347, dtype=torch.float64)
Test Avg. Loss: 0.006702, Accuracy: 0.533510, F1 Score: 0.152205
Remove Test Avg. Loss: 0.000184, Accuracy: 0.958807, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006672, Accuracy: 0.532692, F1 Score: 0.150472
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.9692, dtype=torch.float64)
Test Avg. Loss: 0.070818, Accuracy: 0.522891, F1 Score: 0.118653
Remove Test Avg. Loss: 0.001643, Accuracy: 0.938956, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070509, Accuracy: 0.522671, F1 Score: 0.118239
</noise>
</baseline>
<deltagrad period="2">
data dimension:: [522910, 54]
overhead2:: 34.619757413864136
overhead3:: 56.4801242351532
overhead4:: 28.85869836807251
overhead5:: 0
memory usage:: 4088410112
time_deltagrad:: 238.96940994262695
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.0713, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729613, F1 Score: 0.744120
Remove Test Avg. Loss: 0.001361, Accuracy: 0.536623, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728559, F1 Score: 0.742888
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.6745, dtype=torch.float64)
Test Avg. Loss: 0.001055, Accuracy: 0.734639, F1 Score: 0.742424
Remove Test Avg. Loss: 0.001273, Accuracy: 0.559151, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001051, Accuracy: 0.733964, F1 Score: 0.741367
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7305, dtype=torch.float64)
Test Avg. Loss: 0.001200, Accuracy: 0.631751, F1 Score: 0.460487
Remove Test Avg. Loss: 0.000613, Accuracy: 0.926869, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001196, Accuracy: 0.631897, F1 Score: 0.460952
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2940, dtype=torch.float64)
Test Avg. Loss: 0.008590, Accuracy: 0.502255, F1 Score: 0.009453
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008550, Accuracy: 0.502310, F1 Score: 0.008602
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9288, dtype=torch.float64)
Test Avg. Loss: 0.091497, Accuracy: 0.488021, F1 Score: 0.009325
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091070, Accuracy: 0.487549, F1 Score: 0.008723
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.0758, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.730698, F1 Score: 0.744267
Remove Test Avg. Loss: 0.001351, Accuracy: 0.539912, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.729487, F1 Score: 0.742727
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.7716, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.738047, F1 Score: 0.731749
Remove Test Avg. Loss: 0.001182, Accuracy: 0.603519, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.736991, F1 Score: 0.730550
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7518, dtype=torch.float64)
Test Avg. Loss: 0.001765, Accuracy: 0.554783, F1 Score: 0.184644
Remove Test Avg. Loss: 0.000246, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001758, Accuracy: 0.555935, F1 Score: 0.187142
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5557, dtype=torch.float64)
Test Avg. Loss: 0.018960, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018881, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.5950, dtype=torch.float64)
Test Avg. Loss: 0.194931, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194119, Accuracy: 0.512245, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.0741, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729304, F1 Score: 0.743910
Remove Test Avg. Loss: 0.001365, Accuracy: 0.535858, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728328, F1 Score: 0.742694
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.7284, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.733865, F1 Score: 0.741832
Remove Test Avg. Loss: 0.001309, Accuracy: 0.553873, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.733245, F1 Score: 0.740986
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.2959, dtype=torch.float64)
Test Avg. Loss: 0.001165, Accuracy: 0.675140, F1 Score: 0.587206
Remove Test Avg. Loss: 0.000856, Accuracy: 0.791279, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001162, Accuracy: 0.674669, F1 Score: 0.587249
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(72.9731, dtype=torch.float64)
Test Avg. Loss: 0.006698, Accuracy: 0.533544, F1 Score: 0.152427
Remove Test Avg. Loss: 0.000184, Accuracy: 0.958768, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006668, Accuracy: 0.532721, F1 Score: 0.150680
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7450, dtype=torch.float64)
Test Avg. Loss: 0.070815, Accuracy: 0.522891, F1 Score: 0.118653
Remove Test Avg. Loss: 0.001644, Accuracy: 0.938956, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070505, Accuracy: 0.522677, F1 Score: 0.118272
</noise>
</deltagrad>
<deltagrad period="5">
data dimension:: [522910, 54]
overhead2:: 14.136326313018799
overhead3:: 30.357767581939697
overhead4:: 11.158569097518921
overhead5:: 0
memory usage:: 4088406016
time_deltagrad:: 190.72522640228271
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.0773, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.730027, F1 Score: 0.744428
Remove Test Avg. Loss: 0.001356, Accuracy: 0.537043, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728978, F1 Score: 0.743154
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.6733, dtype=torch.float64)
Test Avg. Loss: 0.001055, Accuracy: 0.734966, F1 Score: 0.742548
Remove Test Avg. Loss: 0.001269, Accuracy: 0.560184, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001051, Accuracy: 0.734325, F1 Score: 0.741518
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7286, dtype=torch.float64)
Test Avg. Loss: 0.001201, Accuracy: 0.630977, F1 Score: 0.458602
Remove Test Avg. Loss: 0.000610, Accuracy: 0.928208, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001197, Accuracy: 0.631254, F1 Score: 0.459311
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2920, dtype=torch.float64)
Test Avg. Loss: 0.008593, Accuracy: 0.502255, F1 Score: 0.009453
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008553, Accuracy: 0.502324, F1 Score: 0.008632
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9268, dtype=torch.float64)
Test Avg. Loss: 0.091500, Accuracy: 0.488021, F1 Score: 0.009325
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091073, Accuracy: 0.487550, F1 Score: 0.008730
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.0924, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.731145, F1 Score: 0.744517
Remove Test Avg. Loss: 0.001347, Accuracy: 0.540524, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.729917, F1 Score: 0.742999
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.7828, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.738253, F1 Score: 0.731601
Remove Test Avg. Loss: 0.001178, Accuracy: 0.604972, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.737374, F1 Score: 0.730667
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7623, dtype=torch.float64)
Test Avg. Loss: 0.001767, Accuracy: 0.554645, F1 Score: 0.184186
Remove Test Avg. Loss: 0.000245, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001760, Accuracy: 0.555816, F1 Score: 0.186748
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5662, dtype=torch.float64)
Test Avg. Loss: 0.018963, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018883, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.6055, dtype=torch.float64)
Test Avg. Loss: 0.194933, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194122, Accuracy: 0.512245, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.0800, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729665, F1 Score: 0.744148
Remove Test Avg. Loss: 0.001360, Accuracy: 0.536546, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728766, F1 Score: 0.742981
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.7275, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.734209, F1 Score: 0.741993
Remove Test Avg. Loss: 0.001305, Accuracy: 0.554791, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.733535, F1 Score: 0.741088
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.2942, dtype=torch.float64)
Test Avg. Loss: 0.001166, Accuracy: 0.674624, F1 Score: 0.586206
Remove Test Avg. Loss: 0.000852, Accuracy: 0.794072, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001162, Accuracy: 0.674265, F1 Score: 0.586284
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(72.9713, dtype=torch.float64)
Test Avg. Loss: 0.006700, Accuracy: 0.533544, F1 Score: 0.152374
Remove Test Avg. Loss: 0.000184, Accuracy: 0.958807, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006670, Accuracy: 0.532694, F1 Score: 0.150561
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7432, dtype=torch.float64)
Test Avg. Loss: 0.070817, Accuracy: 0.522891, F1 Score: 0.118653
Remove Test Avg. Loss: 0.001644, Accuracy: 0.938956, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070507, Accuracy: 0.522667, F1 Score: 0.118245
</noise>
</deltagrad>
<deltagrad period="10">
data dimension:: [522910, 54]
overhead2:: 6.884674072265625
overhead3:: 18.83046007156372
overhead4:: 5.2271881103515625
overhead5:: 0
memory usage:: 4088582144
time_deltagrad:: 161.76726508140564
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.0854, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729786, F1 Score: 0.744183
Remove Test Avg. Loss: 0.001356, Accuracy: 0.537541, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728867, F1 Score: 0.743012
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.6795, dtype=torch.float64)
Test Avg. Loss: 0.001055, Accuracy: 0.735001, F1 Score: 0.742555
Remove Test Avg. Loss: 0.001269, Accuracy: 0.560260, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001051, Accuracy: 0.734199, F1 Score: 0.741384
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7345, dtype=torch.float64)
Test Avg. Loss: 0.001201, Accuracy: 0.630908, F1 Score: 0.457981
Remove Test Avg. Loss: 0.000610, Accuracy: 0.928552, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001197, Accuracy: 0.631172, F1 Score: 0.458685
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2978, dtype=torch.float64)
Test Avg. Loss: 0.008592, Accuracy: 0.502255, F1 Score: 0.009453
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008553, Accuracy: 0.502320, F1 Score: 0.008602
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9327, dtype=torch.float64)
Test Avg. Loss: 0.091500, Accuracy: 0.488021, F1 Score: 0.009325
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091073, Accuracy: 0.487547, F1 Score: 0.008723
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.0954, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.731145, F1 Score: 0.744500
Remove Test Avg. Loss: 0.001347, Accuracy: 0.541480, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.729825, F1 Score: 0.742869
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.7834, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.738322, F1 Score: 0.731700
Remove Test Avg. Loss: 0.001177, Accuracy: 0.605852, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.737180, F1 Score: 0.730419
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7626, dtype=torch.float64)
Test Avg. Loss: 0.001767, Accuracy: 0.554680, F1 Score: 0.184249
Remove Test Avg. Loss: 0.000245, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001760, Accuracy: 0.555901, F1 Score: 0.186936
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5664, dtype=torch.float64)
Test Avg. Loss: 0.018962, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018883, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.6057, dtype=torch.float64)
Test Avg. Loss: 0.194933, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194122, Accuracy: 0.512245, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.0788, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.729665, F1 Score: 0.744073
Remove Test Avg. Loss: 0.001360, Accuracy: 0.536967, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.728728, F1 Score: 0.742895
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.7229, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.734157, F1 Score: 0.741896
Remove Test Avg. Loss: 0.001305, Accuracy: 0.555364, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001052, Accuracy: 0.733533, F1 Score: 0.741029
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.2894, dtype=torch.float64)
Test Avg. Loss: 0.001165, Accuracy: 0.674744, F1 Score: 0.586079
Remove Test Avg. Loss: 0.000852, Accuracy: 0.794798, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001162, Accuracy: 0.674171, F1 Score: 0.585941
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(72.9664, dtype=torch.float64)
Test Avg. Loss: 0.006700, Accuracy: 0.533562, F1 Score: 0.152379
Remove Test Avg. Loss: 0.000184, Accuracy: 0.958845, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006670, Accuracy: 0.532719, F1 Score: 0.150603
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7383, dtype=torch.float64)
Test Avg. Loss: 0.070817, Accuracy: 0.522891, F1 Score: 0.118653
Remove Test Avg. Loss: 0.001643, Accuracy: 0.938956, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070507, Accuracy: 0.522681, F1 Score: 0.118279
</noise>
</deltagrad>
<deltagrad period="20">
data dimension:: [522910, 54]
overhead2:: 4.178167819976807
overhead3:: 14.05489730834961
overhead4:: 2.5797886848449707
overhead5:: 0
memory usage:: 4088590336
time_deltagrad:: 148.2718906402588
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.1040, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.730526, F1 Score: 0.744396
Remove Test Avg. Loss: 0.001351, Accuracy: 0.539415, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.729145, F1 Score: 0.742791
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.6909, dtype=torch.float64)
Test Avg. Loss: 0.001055, Accuracy: 0.735224, F1 Score: 0.742104
Remove Test Avg. Loss: 0.001263, Accuracy: 0.562708, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001051, Accuracy: 0.734453, F1 Score: 0.741041
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7447, dtype=torch.float64)
Test Avg. Loss: 0.001203, Accuracy: 0.629755, F1 Score: 0.454619
Remove Test Avg. Loss: 0.000607, Accuracy: 0.929776, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001199, Accuracy: 0.630001, F1 Score: 0.455356
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.3079, dtype=torch.float64)
Test Avg. Loss: 0.008598, Accuracy: 0.502220, F1 Score: 0.009249
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008558, Accuracy: 0.502346, F1 Score: 0.008580
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9427, dtype=torch.float64)
Test Avg. Loss: 0.091505, Accuracy: 0.488021, F1 Score: 0.009325
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091078, Accuracy: 0.487560, F1 Score: 0.008716
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.1012, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.731111, F1 Score: 0.743873
Remove Test Avg. Loss: 0.001341, Accuracy: 0.542628, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.730147, F1 Score: 0.742684
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.7781, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.738288, F1 Score: 0.730972
Remove Test Avg. Loss: 0.001172, Accuracy: 0.608759, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.737356, F1 Score: 0.729849
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7560, dtype=torch.float64)
Test Avg. Loss: 0.001771, Accuracy: 0.554353, F1 Score: 0.183057
Remove Test Avg. Loss: 0.000243, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001764, Accuracy: 0.555625, F1 Score: 0.185816
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5597, dtype=torch.float64)
Test Avg. Loss: 0.018968, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018889, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.5990, dtype=torch.float64)
Test Avg. Loss: 0.194939, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194127, Accuracy: 0.512245, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.0818, dtype=torch.float64)
Test Avg. Loss: 0.001057, Accuracy: 0.730233, F1 Score: 0.744140
Remove Test Avg. Loss: 0.001354, Accuracy: 0.538650, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001053, Accuracy: 0.728980, F1 Score: 0.742644
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.7126, dtype=torch.float64)
Test Avg. Loss: 0.001056, Accuracy: 0.734209, F1 Score: 0.741423
Remove Test Avg. Loss: 0.001299, Accuracy: 0.556818, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001052, Accuracy: 0.733702, F1 Score: 0.740633
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.2779, dtype=torch.float64)
Test Avg. Loss: 0.001167, Accuracy: 0.673849, F1 Score: 0.583937
Remove Test Avg. Loss: 0.000848, Accuracy: 0.797246, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001163, Accuracy: 0.673439, F1 Score: 0.584096
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(72.9548, dtype=torch.float64)
Test Avg. Loss: 0.006705, Accuracy: 0.533527, F1 Score: 0.152157
Remove Test Avg. Loss: 0.000183, Accuracy: 0.958960, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006675, Accuracy: 0.532684, F1 Score: 0.150281
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7267, dtype=torch.float64)
Test Avg. Loss: 0.070822, Accuracy: 0.522925, F1 Score: 0.118661
Remove Test Avg. Loss: 0.001642, Accuracy: 0.938994, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070512, Accuracy: 0.522671, F1 Score: 0.118227
</noise>
</deltagrad>
<deltagrad period="50">
data dimension:: [522910, 54]
overhead2:: 1.370741367340088
overhead3:: 10.376867532730103
overhead4:: 1.043271541595459
overhead5:: 0
memory usage:: 4088340480
time_deltagrad:: 135.4324014186859
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.4786, dtype=torch.float64)
Test Avg. Loss: 0.001062, Accuracy: 0.725328, F1 Score: 0.743321
Remove Test Avg. Loss: 0.001457, Accuracy: 0.516160, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001059, Accuracy: 0.724597, F1 Score: 0.742463
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.8177, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.731851, F1 Score: 0.743404
Remove Test Avg. Loss: 0.001364, Accuracy: 0.533716, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.730713, F1 Score: 0.741985
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7388, dtype=torch.float64)
Test Avg. Loss: 0.001187, Accuracy: 0.640030, F1 Score: 0.486383
Remove Test Avg. Loss: 0.000666, Accuracy: 0.897151, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001183, Accuracy: 0.640776, F1 Score: 0.488200
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2872, dtype=torch.float64)
Test Avg. Loss: 0.008546, Accuracy: 0.502134, F1 Score: 0.009451
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008506, Accuracy: 0.502193, F1 Score: 0.008705
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9206, dtype=torch.float64)
Test Avg. Loss: 0.091453, Accuracy: 0.487987, F1 Score: 0.009324
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091026, Accuracy: 0.487535, F1 Score: 0.008745
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.4712, dtype=torch.float64)
Test Avg. Loss: 0.001062, Accuracy: 0.726188, F1 Score: 0.743151
Remove Test Avg. Loss: 0.001446, Accuracy: 0.519143, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001058, Accuracy: 0.725381, F1 Score: 0.742220
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.8553, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.736016, F1 Score: 0.734416
Remove Test Avg. Loss: 0.001269, Accuracy: 0.566877, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001056, Accuracy: 0.734646, F1 Score: 0.733017
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7088, dtype=torch.float64)
Test Avg. Loss: 0.001736, Accuracy: 0.557175, F1 Score: 0.193271
Remove Test Avg. Loss: 0.000270, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001730, Accuracy: 0.557912, F1 Score: 0.194607
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.4997, dtype=torch.float64)
Test Avg. Loss: 0.018915, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018836, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.5378, dtype=torch.float64)
Test Avg. Loss: 0.194886, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194074, Accuracy: 0.512247, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.4839, dtype=torch.float64)
Test Avg. Loss: 0.001062, Accuracy: 0.725190, F1 Score: 0.743201
Remove Test Avg. Loss: 0.001461, Accuracy: 0.515624, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001059, Accuracy: 0.724402, F1 Score: 0.742281
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.8895, dtype=torch.float64)
Test Avg. Loss: 0.001060, Accuracy: 0.730199, F1 Score: 0.742171
Remove Test Avg. Loss: 0.001403, Accuracy: 0.529317, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001056, Accuracy: 0.729347, F1 Score: 0.740932
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.3356, dtype=torch.float64)
Test Avg. Loss: 0.001158, Accuracy: 0.681457, F1 Score: 0.603598
Remove Test Avg. Loss: 0.000925, Accuracy: 0.754026, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001154, Accuracy: 0.681224, F1 Score: 0.603866
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(72.9989, dtype=torch.float64)
Test Avg. Loss: 0.006660, Accuracy: 0.533596, F1 Score: 0.154187
Remove Test Avg. Loss: 0.000197, Accuracy: 0.956588, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006630, Accuracy: 0.532837, F1 Score: 0.152533
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7694, dtype=torch.float64)
Test Avg. Loss: 0.070775, Accuracy: 0.522856, F1 Score: 0.118814
Remove Test Avg. Loss: 0.001659, Accuracy: 0.938688, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070466, Accuracy: 0.522688, F1 Score: 0.118461
</noise>
</deltagrad>
<deltagrad period="75">
data dimension:: [522910, 54]
overhead2:: 1.025824785232544
overhead3:: 10.114334344863892
overhead4:: 0.7119379043579102
overhead5:: 0
memory usage:: 4088418304
time_deltagrad:: 136.31660890579224
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.6582, dtype=torch.float64)
Test Avg. Loss: 0.001063, Accuracy: 0.725380, F1 Score: 0.743233
Remove Test Avg. Loss: 0.001471, Accuracy: 0.510652, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001059, Accuracy: 0.724804, F1 Score: 0.742512
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.9359, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.731300, F1 Score: 0.742368
Remove Test Avg. Loss: 0.001378, Accuracy: 0.528476, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001056, Accuracy: 0.730868, F1 Score: 0.741739
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7565, dtype=torch.float64)
Test Avg. Loss: 0.001189, Accuracy: 0.639548, F1 Score: 0.486930
Remove Test Avg. Loss: 0.000673, Accuracy: 0.893517, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001185, Accuracy: 0.640349, F1 Score: 0.488675
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2913, dtype=torch.float64)
Test Avg. Loss: 0.008548, Accuracy: 0.502151, F1 Score: 0.009451
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008509, Accuracy: 0.502218, F1 Score: 0.008653
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9233, dtype=torch.float64)
Test Avg. Loss: 0.091456, Accuracy: 0.487969, F1 Score: 0.009324
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091028, Accuracy: 0.487537, F1 Score: 0.008745
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.6504, dtype=torch.float64)
Test Avg. Loss: 0.001063, Accuracy: 0.726481, F1 Score: 0.743288
Remove Test Avg. Loss: 0.001461, Accuracy: 0.513597, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001059, Accuracy: 0.725830, F1 Score: 0.742460
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.9522, dtype=torch.float64)
Test Avg. Loss: 0.001060, Accuracy: 0.735861, F1 Score: 0.733656
Remove Test Avg. Loss: 0.001282, Accuracy: 0.560528, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001057, Accuracy: 0.734702, F1 Score: 0.732464
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7034, dtype=torch.float64)
Test Avg. Loss: 0.001739, Accuracy: 0.557365, F1 Score: 0.194147
Remove Test Avg. Loss: 0.000274, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001732, Accuracy: 0.558050, F1 Score: 0.195502
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.4825, dtype=torch.float64)
Test Avg. Loss: 0.018918, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018839, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.5194, dtype=torch.float64)
Test Avg. Loss: 0.194889, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194077, Accuracy: 0.512247, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.6697, dtype=torch.float64)
Test Avg. Loss: 0.001063, Accuracy: 0.725259, F1 Score: 0.743133
Remove Test Avg. Loss: 0.001475, Accuracy: 0.510231, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001060, Accuracy: 0.724624, F1 Score: 0.742351
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(1.0489, dtype=torch.float64)
Test Avg. Loss: 0.001061, Accuracy: 0.729682, F1 Score: 0.741329
Remove Test Avg. Loss: 0.001417, Accuracy: 0.523580, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001058, Accuracy: 0.729055, F1 Score: 0.740296
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.4206, dtype=torch.float64)
Test Avg. Loss: 0.001161, Accuracy: 0.680114, F1 Score: 0.601979
Remove Test Avg. Loss: 0.000936, Accuracy: 0.744540, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001157, Accuracy: 0.679991, F1 Score: 0.602457
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(73.0723, dtype=torch.float64)
Test Avg. Loss: 0.006664, Accuracy: 0.533544, F1 Score: 0.154225
Remove Test Avg. Loss: 0.000200, Accuracy: 0.955976, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006634, Accuracy: 0.532799, F1 Score: 0.152758
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.8415, dtype=torch.float64)
Test Avg. Loss: 0.070779, Accuracy: 0.522839, F1 Score: 0.118866
Remove Test Avg. Loss: 0.001664, Accuracy: 0.938688, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070469, Accuracy: 0.522687, F1 Score: 0.118467
</noise>
</deltagrad>
<deltagrad period="100">
data dimension:: [522910, 54]
overhead2:: 0.8733527660369873
overhead3:: 9.845674753189087
overhead4:: 0.5335090160369873
overhead5:: 0
memory usage:: 4088283136
time_deltagrad:: 136.74937510490417
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.6369, dtype=torch.float64)
Test Avg. Loss: 0.001061, Accuracy: 0.729114, F1 Score: 0.743175
Remove Test Avg. Loss: 0.001425, Accuracy: 0.521821, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001057, Accuracy: 0.727919, F1 Score: 0.741766
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.9249, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.733624, F1 Score: 0.740201
Remove Test Avg. Loss: 0.001334, Accuracy: 0.539147, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.732878, F1 Score: 0.739485
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7601, dtype=torch.float64)
Test Avg. Loss: 0.001203, Accuracy: 0.630150, F1 Score: 0.461118
Remove Test Avg. Loss: 0.000648, Accuracy: 0.909199, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001199, Accuracy: 0.630097, F1 Score: 0.460897
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2969, dtype=torch.float64)
Test Avg. Loss: 0.008589, Accuracy: 0.502255, F1 Score: 0.008978
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008549, Accuracy: 0.502358, F1 Score: 0.008172
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9290, dtype=torch.float64)
Test Avg. Loss: 0.091496, Accuracy: 0.488055, F1 Score: 0.009326
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091069, Accuracy: 0.487550, F1 Score: 0.008657
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.6392, dtype=torch.float64)
Test Avg. Loss: 0.001061, Accuracy: 0.730354, F1 Score: 0.743328
Remove Test Avg. Loss: 0.001415, Accuracy: 0.524536, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001057, Accuracy: 0.729003, F1 Score: 0.741700
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(1.0092, dtype=torch.float64)
Test Avg. Loss: 0.001062, Accuracy: 0.736446, F1 Score: 0.729103
Remove Test Avg. Loss: 0.001240, Accuracy: 0.576516, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001058, Accuracy: 0.735010, F1 Score: 0.727573
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7920, dtype=torch.float64)
Test Avg. Loss: 0.001768, Accuracy: 0.555403, F1 Score: 0.186906
Remove Test Avg. Loss: 0.000263, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001761, Accuracy: 0.556346, F1 Score: 0.188460
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5727, dtype=torch.float64)
Test Avg. Loss: 0.018961, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018881, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.6097, dtype=torch.float64)
Test Avg. Loss: 0.194931, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194119, Accuracy: 0.512247, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.6446, dtype=torch.float64)
Test Avg. Loss: 0.001061, Accuracy: 0.728925, F1 Score: 0.743075
Remove Test Avg. Loss: 0.001429, Accuracy: 0.521132, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001058, Accuracy: 0.727770, F1 Score: 0.741654
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(1.0124, dtype=torch.float64)
Test Avg. Loss: 0.001060, Accuracy: 0.732970, F1 Score: 0.740113
Remove Test Avg. Loss: 0.001371, Accuracy: 0.535934, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001056, Accuracy: 0.731631, F1 Score: 0.738736
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.3870, dtype=torch.float64)
Test Avg. Loss: 0.001171, Accuracy: 0.675226, F1 Score: 0.589426
Remove Test Avg. Loss: 0.000903, Accuracy: 0.764161, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001167, Accuracy: 0.675181, F1 Score: 0.590145
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(73.0400, dtype=torch.float64)
Test Avg. Loss: 0.006698, Accuracy: 0.533304, F1 Score: 0.152042
Remove Test Avg. Loss: 0.000194, Accuracy: 0.957124, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006668, Accuracy: 0.532608, F1 Score: 0.150443
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.8094, dtype=torch.float64)
Test Avg. Loss: 0.070815, Accuracy: 0.522856, F1 Score: 0.118646
Remove Test Avg. Loss: 0.001656, Accuracy: 0.938726, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070505, Accuracy: 0.522675, F1 Score: 0.118215
</noise>
</deltagrad>
<deltagrad period="200">
data dimension:: [522910, 54]
overhead2:: 0.3555867671966553
overhead3:: 8.652737140655518
overhead4:: 0.2792940139770508
overhead5:: 0
memory usage:: 4087828480
time_deltagrad:: 131.39707255363464
<noise sigma="0.01" seed="0">
model difference (l2 norm): tensor(0.4813, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.730285, F1 Score: 0.741125
Remove Test Avg. Loss: 0.001378, Accuracy: 0.532071, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.729454, F1 Score: 0.740230
</noise>
<noise sigma="0.1" seed="0">
model difference (l2 norm): tensor(0.8143, dtype=torch.float64)
Test Avg. Loss: 0.001058, Accuracy: 0.733555, F1 Score: 0.736803
Remove Test Avg. Loss: 0.001289, Accuracy: 0.552649, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001054, Accuracy: 0.733212, F1 Score: 0.736639
</noise>
<noise sigma="1" seed="0">
model difference (l2 norm): tensor(6.7323, dtype=torch.float64)
Test Avg. Loss: 0.001214, Accuracy: 0.624471, F1 Score: 0.441126
Remove Test Avg. Loss: 0.000623, Accuracy: 0.922700, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001209, Accuracy: 0.624046, F1 Score: 0.440412
</noise>
<noise sigma="10" seed="0">
model difference (l2 norm): tensor(67.2805, dtype=torch.float64)
Test Avg. Loss: 0.008620, Accuracy: 0.502633, F1 Score: 0.008237
Remove Test Avg. Loss: 0.000001, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.008580, Accuracy: 0.502624, F1 Score: 0.007548
</noise>
<noise sigma="100" seed="0">
model difference (l2 norm): tensor(672.9138, dtype=torch.float64)
Test Avg. Loss: 0.091527, Accuracy: 0.488073, F1 Score: 0.009194
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.091100, Accuracy: 0.487604, F1 Score: 0.008614
</noise>
<noise sigma="0.01" seed="1">
model difference (l2 norm): tensor(0.4865, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.731128, F1 Score: 0.740791
Remove Test Avg. Loss: 0.001368, Accuracy: 0.534940, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.730390, F1 Score: 0.739966
</noise>
<noise sigma="0.1" seed="1">
model difference (l2 norm): tensor(0.9205, dtype=torch.float64)
Test Avg. Loss: 0.001062, Accuracy: 0.735930, F1 Score: 0.724755
Remove Test Avg. Loss: 0.001197, Accuracy: 0.594607, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001059, Accuracy: 0.734826, F1 Score: 0.723498
</noise>
<noise sigma="1" seed="1">
model difference (l2 norm): tensor(7.7815, dtype=torch.float64)
Test Avg. Loss: 0.001789, Accuracy: 0.553251, F1 Score: 0.178394
Remove Test Avg. Loss: 0.000251, Accuracy: 0.981411, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001782, Accuracy: 0.554354, F1 Score: 0.180451
</noise>
<noise sigma="10" seed="1">
model difference (l2 norm): tensor(77.5722, dtype=torch.float64)
Test Avg. Loss: 0.018994, Accuracy: 0.512409, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.018914, Accuracy: 0.512400, F1 Score: 0.000000
</noise>
<noise sigma="100" seed="1">
model difference (l2 norm): tensor(775.6102, dtype=torch.float64)
Test Avg. Loss: 0.194965, Accuracy: 0.512289, F1 Score: 0.000000
Remove Test Avg. Loss: 0.000000, Accuracy: 1.000000, F1 Score: 0.000000
Remain Test Avg. Loss: 0.194153, Accuracy: 0.512249, F1 Score: 0.000000
</noise>
<noise sigma="0.01" seed="2">
model difference (l2 norm): tensor(0.4898, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.729975, F1 Score: 0.740956
Remove Test Avg. Loss: 0.001381, Accuracy: 0.531497, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001056, Accuracy: 0.729288, F1 Score: 0.740111
</noise>
<noise sigma="0.1" seed="2">
model difference (l2 norm): tensor(0.9042, dtype=torch.float64)
Test Avg. Loss: 0.001059, Accuracy: 0.733452, F1 Score: 0.737495
Remove Test Avg. Loss: 0.001326, Accuracy: 0.548212, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001055, Accuracy: 0.732466, F1 Score: 0.736374
</noise>
<noise sigma="1" seed="2">
model difference (l2 norm): tensor(7.3513, dtype=torch.float64)
Test Avg. Loss: 0.001177, Accuracy: 0.670889, F1 Score: 0.578197
Remove Test Avg. Loss: 0.000869, Accuracy: 0.781947, F1 Score: 0.000000
Remain Test Avg. Loss: 0.001174, Accuracy: 0.671077, F1 Score: 0.578989
</noise>
<noise sigma="10" seed="2">
model difference (l2 norm): tensor(73.0146, dtype=torch.float64)
Test Avg. Loss: 0.006726, Accuracy: 0.533080, F1 Score: 0.149748
Remove Test Avg. Loss: 0.000187, Accuracy: 0.958156, F1 Score: 0.000000
Remain Test Avg. Loss: 0.006696, Accuracy: 0.532350, F1 Score: 0.148105
</noise>
<noise sigma="100" seed="2">
model difference (l2 norm): tensor(729.7851, dtype=torch.float64)
Test Avg. Loss: 0.070845, Accuracy: 0.522856, F1 Score: 0.118365
Remove Test Avg. Loss: 0.001648, Accuracy: 0.938918, F1 Score: 0.000000
Remain Test Avg. Loss: 0.070535, Accuracy: 0.522662, F1 Score: 0.117982
</noise>
</deltagrad>
</results>
</data>
