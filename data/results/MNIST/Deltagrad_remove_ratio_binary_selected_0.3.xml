<data>
<Generate_Dataset/>
<Training>
Train - Epoch 0, Batch: 0, Loss: 0.690668
Train - Epoch 0, Batch: 1, Loss: 0.686035
Train - Epoch 0, Batch: 2, Loss: 0.682603
Train - Epoch 0, Batch: 3, Loss: 0.679206
Train - Epoch 0, Batch: 4, Loss: 0.676549
Train - Epoch 0, Batch: 5, Loss: 0.672753
Train - Epoch 0, Batch: 6, Loss: 0.669738
Train - Epoch 0, Batch: 7, Loss: 0.664814
Train - Epoch 0, Batch: 8, Loss: 0.661506
Train - Epoch 0, Batch: 9, Loss: 0.659799
Train - Epoch 0, Batch: 10, Loss: 0.656385
Train - Epoch 0, Batch: 11, Loss: 0.652731
Train - Epoch 100, Batch: 0, Loss: 0.214887
Train - Epoch 100, Batch: 1, Loss: 0.230650
Train - Epoch 100, Batch: 2, Loss: 0.196124
Train - Epoch 100, Batch: 3, Loss: 0.185781
Train - Epoch 100, Batch: 4, Loss: 0.212984
Train - Epoch 100, Batch: 5, Loss: 0.218490
Train - Epoch 100, Batch: 6, Loss: 0.224942
Train - Epoch 100, Batch: 7, Loss: 0.207742
Train - Epoch 100, Batch: 8, Loss: 0.207214
Train - Epoch 100, Batch: 9, Loss: 0.190116
Train - Epoch 100, Batch: 10, Loss: 0.223480
Train - Epoch 100, Batch: 11, Loss: 0.220981
Train - Epoch 200, Batch: 0, Loss: 0.186070
Train - Epoch 200, Batch: 1, Loss: 0.173805
Train - Epoch 200, Batch: 2, Loss: 0.160556
Train - Epoch 200, Batch: 3, Loss: 0.187147
Train - Epoch 200, Batch: 4, Loss: 0.180125
Train - Epoch 200, Batch: 5, Loss: 0.173751
Train - Epoch 200, Batch: 6, Loss: 0.191680
Train - Epoch 200, Batch: 7, Loss: 0.174594
Train - Epoch 200, Batch: 8, Loss: 0.179576
Train - Epoch 200, Batch: 9, Loss: 0.154289
Train - Epoch 200, Batch: 10, Loss: 0.180450
Train - Epoch 200, Batch: 11, Loss: 0.165928
Train - Epoch 300, Batch: 0, Loss: 0.163994
Train - Epoch 300, Batch: 1, Loss: 0.157898
Train - Epoch 300, Batch: 2, Loss: 0.157557
Train - Epoch 300, Batch: 3, Loss: 0.163049
Train - Epoch 300, Batch: 4, Loss: 0.166890
Train - Epoch 300, Batch: 5, Loss: 0.150189
Train - Epoch 300, Batch: 6, Loss: 0.171357
Train - Epoch 300, Batch: 7, Loss: 0.172147
Train - Epoch 300, Batch: 8, Loss: 0.163332
Train - Epoch 300, Batch: 9, Loss: 0.148903
Train - Epoch 300, Batch: 10, Loss: 0.167355
Train - Epoch 300, Batch: 11, Loss: 0.164314
Train - Epoch 400, Batch: 0, Loss: 0.173244
Train - Epoch 400, Batch: 1, Loss: 0.143577
Train - Epoch 400, Batch: 2, Loss: 0.160296
Train - Epoch 400, Batch: 3, Loss: 0.143836
Train - Epoch 400, Batch: 4, Loss: 0.147188
Train - Epoch 400, Batch: 5, Loss: 0.164635
Train - Epoch 400, Batch: 6, Loss: 0.173777
Train - Epoch 400, Batch: 7, Loss: 0.152646
Train - Epoch 400, Batch: 8, Loss: 0.148964
Train - Epoch 400, Batch: 9, Loss: 0.147931
Train - Epoch 400, Batch: 10, Loss: 0.156793
Train - Epoch 400, Batch: 11, Loss: 0.144083
Train - Epoch 500, Batch: 0, Loss: 0.143951
Train - Epoch 500, Batch: 1, Loss: 0.135777
Train - Epoch 500, Batch: 2, Loss: 0.150165
Train - Epoch 500, Batch: 3, Loss: 0.149075
Train - Epoch 500, Batch: 4, Loss: 0.160102
Train - Epoch 500, Batch: 5, Loss: 0.154553
Train - Epoch 500, Batch: 6, Loss: 0.147439
Train - Epoch 500, Batch: 7, Loss: 0.163022
Train - Epoch 500, Batch: 8, Loss: 0.130619
Train - Epoch 500, Batch: 9, Loss: 0.166710
Train - Epoch 500, Batch: 10, Loss: 0.157058
Train - Epoch 500, Batch: 11, Loss: 0.150301
Train - Epoch 600, Batch: 0, Loss: 0.154499
Train - Epoch 600, Batch: 1, Loss: 0.149178
Train - Epoch 600, Batch: 2, Loss: 0.142031
Train - Epoch 600, Batch: 3, Loss: 0.162711
Train - Epoch 600, Batch: 4, Loss: 0.157243
Train - Epoch 600, Batch: 5, Loss: 0.139721
Train - Epoch 600, Batch: 6, Loss: 0.157663
Train - Epoch 600, Batch: 7, Loss: 0.149176
Train - Epoch 600, Batch: 8, Loss: 0.135162
Train - Epoch 600, Batch: 9, Loss: 0.132825
Train - Epoch 600, Batch: 10, Loss: 0.141485
Train - Epoch 600, Batch: 11, Loss: 0.156921
Train - Epoch 700, Batch: 0, Loss: 0.140386
Train - Epoch 700, Batch: 1, Loss: 0.150804
Train - Epoch 700, Batch: 2, Loss: 0.152736
Train - Epoch 700, Batch: 3, Loss: 0.148117
Train - Epoch 700, Batch: 4, Loss: 0.122409
Train - Epoch 700, Batch: 5, Loss: 0.155529
Train - Epoch 700, Batch: 6, Loss: 0.128902
Train - Epoch 700, Batch: 7, Loss: 0.148450
Train - Epoch 700, Batch: 8, Loss: 0.155879
Train - Epoch 700, Batch: 9, Loss: 0.150703
Train - Epoch 700, Batch: 10, Loss: 0.150223
Train - Epoch 700, Batch: 11, Loss: 0.151145
Train - Epoch 800, Batch: 0, Loss: 0.157707
Train - Epoch 800, Batch: 1, Loss: 0.146611
Train - Epoch 800, Batch: 2, Loss: 0.144243
Train - Epoch 800, Batch: 3, Loss: 0.124582
Train - Epoch 800, Batch: 4, Loss: 0.139440
Train - Epoch 800, Batch: 5, Loss: 0.152976
Train - Epoch 800, Batch: 6, Loss: 0.159948
Train - Epoch 800, Batch: 7, Loss: 0.127193
Train - Epoch 800, Batch: 8, Loss: 0.134529
Train - Epoch 800, Batch: 9, Loss: 0.166191
Train - Epoch 800, Batch: 10, Loss: 0.141160
Train - Epoch 800, Batch: 11, Loss: 0.143228
Train - Epoch 900, Batch: 0, Loss: 0.144855
Train - Epoch 900, Batch: 1, Loss: 0.145902
Train - Epoch 900, Batch: 2, Loss: 0.144487
Train - Epoch 900, Batch: 3, Loss: 0.137028
Train - Epoch 900, Batch: 4, Loss: 0.166014
Train - Epoch 900, Batch: 5, Loss: 0.141738
Train - Epoch 900, Batch: 6, Loss: 0.141998
Train - Epoch 900, Batch: 7, Loss: 0.140236
Train - Epoch 900, Batch: 8, Loss: 0.124310
Train - Epoch 900, Batch: 9, Loss: 0.149152
Train - Epoch 900, Batch: 10, Loss: 0.144212
Train - Epoch 900, Batch: 11, Loss: 0.148799
training_time:: 11.627674579620361
training time full:: 11.62783670425415
provenance prepare time:: 2.384185791015625e-07
here
Test Avg. Loss: 0.000122, Accuracy: 0.968750, F1 Score: 0.968205
</Training>
torch.Size([11982, 784])
tensor([3323, 5846, 6329,  ..., 8208, 6523, 7498])
<results lr="1" epochs="1000" bz="1024" remove_ratio="0.3" sampling_type="targeted_informed">
<Baseline>
data dimension:: [11982, 784]
tensor([ 2053,    11,  6156,  4111,  2066,  4114,  6165,    21,  6166,  2072,
         4122, 10270,  8235,  8236,  4141,  6192,  8241, 10290,  4146, 10295,
         6206,  8257, 10309, 10311,  2121, 10314,  8275,  2133, 10326,  4191,
           98,  6246,  2155,  2157,   110,   111,  6260, 10357,  4219,  4223,
        10367,   130,  2179, 10370,   134,   135, 10376,  2186,  6283,  4234])
Epoch:0 Batch: 11 Baseline Loss 0.5982121091969892
Epoch:100 Batch: 11 Baseline Loss 0.19622490941827173
Epoch:200 Batch: 11 Baseline Loss 0.15088239362359343
Epoch:300 Batch: 11 Baseline Loss 0.15140008424096155
Epoch:400 Batch: 11 Baseline Loss 0.12374451114639101
Epoch:500 Batch: 11 Baseline Loss 0.11460576682982584
Epoch:600 Batch: 11 Baseline Loss 0.12936070998859733
Epoch:700 Batch: 11 Baseline Loss 0.11952591651884546
Epoch:800 Batch: 11 Baseline Loss 0.10344316922556629
Epoch:900 Batch: 11 Baseline Loss 0.1012153754373768
training time is 16.596888065338135
overhead:: 0
overhead2:: 3.324366331100464
overhead3:: 0
memory usage:: 821563392
time_baseline:: 16.619852542877197
Test Avg. Loss: 0.000209, Accuracy: 0.926915, F1 Score: 0.929918
model difference (l2 norm): tensor(10.8381, dtype=torch.float64)
Test Avg. Loss: 0.000209, Accuracy: 0.926915, F1 Score: 0.929918
Remove Test Avg. Loss: 0.000537, Accuracy: 0.794101, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000230, Accuracy: 0.918044, F1 Score: 0.921565
</Baseline>
<Deltagrad period="2">
data dimension:: [11982, 784]
overhead2:: 3.2130229473114014
overhead3:: 4.395706653594971
overhead4:: 3.4171321392059326
overhead5:: 0
memory usage:: 1084739584
time_deltagrad:: 22.20836567878723
model difference (l2 norm): tensor(0.7825, dtype=torch.float64)
Test Avg. Loss: 0.000202, Accuracy: 0.929435, F1 Score: 0.932105
Remove Test Avg. Loss: 0.000515, Accuracy: 0.803840, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000223, Accuracy: 0.921215, F1 Score: 0.924420
</Deltagrad>
<Deltagrad period="5">
data dimension:: [11982, 784]
overhead2:: 1.2624425888061523
overhead3:: 2.0984673500061035
overhead4:: 1.38688063621521
overhead5:: 0
memory usage:: 1084755968
time_deltagrad:: 17.357566118240356
model difference (l2 norm): tensor(1.3864, dtype=torch.float64)
Test Avg. Loss: 0.000196, Accuracy: 0.931452, F1 Score: 0.933916
Remove Test Avg. Loss: 0.000496, Accuracy: 0.812743, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000218, Accuracy: 0.924136, F1 Score: 0.927052
</Deltagrad>
<Deltagrad period="10">
data dimension:: [11982, 784]
overhead2:: 0.6334547996520996
overhead3:: 1.3567442893981934
overhead4:: 0.8226971626281738
overhead5:: 0
memory usage:: 1084764160
time_deltagrad:: 15.958029508590698
model difference (l2 norm): tensor(1.5763, dtype=torch.float64)
Test Avg. Loss: 0.000191, Accuracy: 0.933468, F1 Score: 0.935610
Remove Test Avg. Loss: 0.000479, Accuracy: 0.821647, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000213, Accuracy: 0.926807, F1 Score: 0.929405
</Deltagrad>
<Deltagrad period="20">
data dimension:: [11982, 784]
overhead2:: 0.31498098373413086
overhead3:: 1.005617618560791
overhead4:: 0.47582340240478516
overhead5:: 0
memory usage:: 1084772352
time_deltagrad:: 14.972392797470093
model difference (l2 norm): tensor(1.5925, dtype=torch.float64)
Test Avg. Loss: 0.000191, Accuracy: 0.932964, F1 Score: 0.935217
Remove Test Avg. Loss: 0.000479, Accuracy: 0.821925, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000213, Accuracy: 0.926974, F1 Score: 0.929555
</Deltagrad>
<Deltagrad period="50">
data dimension:: [11982, 784]
overhead2:: 0.13657426834106445
overhead3:: 0.8260109424591064
overhead4:: 0.20810484886169434
overhead5:: 0
memory usage:: 1086824448
time_deltagrad:: 14.836566686630249
model difference (l2 norm): tensor(1.7777, dtype=torch.float64)
Test Avg. Loss: 0.000189, Accuracy: 0.933468, F1 Score: 0.935673
Remove Test Avg. Loss: 0.000472, Accuracy: 0.824430, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000211, Accuracy: 0.927391, F1 Score: 0.929861
</Deltagrad>
<Deltagrad period="75">
data dimension:: [11982, 784]
overhead2:: 0.08854007720947266
overhead3:: 0.7442739009857178
overhead4:: 0.1416938304901123
overhead5:: 0
memory usage:: 1086828544
time_deltagrad:: 14.309239149093628
model difference (l2 norm): tensor(1.9707, dtype=torch.float64)
Test Avg. Loss: 0.000198, Accuracy: 0.932460, F1 Score: 0.934825
Remove Test Avg. Loss: 0.000500, Accuracy: 0.810518, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000218, Accuracy: 0.923051, F1 Score: 0.926074
</Deltagrad>
<Deltagrad period="100">
data dimension:: [11982, 784]
overhead2:: 0.0690007209777832
overhead3:: 0.7171549797058105
overhead4:: 0.10949873924255371
overhead5:: 0
memory usage:: 1084743680
time_deltagrad:: 14.563864707946777
model difference (l2 norm): tensor(1.8546, dtype=torch.float64)
Test Avg. Loss: 0.000191, Accuracy: 0.932460, F1 Score: 0.934634
Remove Test Avg. Loss: 0.000480, Accuracy: 0.821647, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000214, Accuracy: 0.926640, F1 Score: 0.929187
</Deltagrad>
<Deltagrad period="200">
data dimension:: [11982, 784]
overhead2:: 0.036777496337890625
overhead3:: 0.6888751983642578
overhead4:: 0.058289527893066406
overhead5:: 0
memory usage:: 1091018752
time_deltagrad:: 14.368991613388062
model difference (l2 norm): tensor(2.1174, dtype=torch.float64)
Test Avg. Loss: 0.000196, Accuracy: 0.931452, F1 Score: 0.933981
Remove Test Avg. Loss: 0.000496, Accuracy: 0.813022, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000218, Accuracy: 0.923802, F1 Score: 0.926708
</Deltagrad>
</results>
</data>
