<data>
<Generate_Dataset/>
<Training>
Train - Epoch 0, Batch: 0, Loss: 0.690668
Train - Epoch 0, Batch: 1, Loss: 0.686035
Train - Epoch 0, Batch: 2, Loss: 0.682603
Train - Epoch 0, Batch: 3, Loss: 0.679206
Train - Epoch 0, Batch: 4, Loss: 0.676549
Train - Epoch 0, Batch: 5, Loss: 0.672753
Train - Epoch 0, Batch: 6, Loss: 0.669738
Train - Epoch 0, Batch: 7, Loss: 0.664814
Train - Epoch 0, Batch: 8, Loss: 0.661506
Train - Epoch 0, Batch: 9, Loss: 0.659799
Train - Epoch 0, Batch: 10, Loss: 0.656385
Train - Epoch 0, Batch: 11, Loss: 0.652731
Train - Epoch 100, Batch: 0, Loss: 0.214887
Train - Epoch 100, Batch: 1, Loss: 0.230650
Train - Epoch 100, Batch: 2, Loss: 0.196124
Train - Epoch 100, Batch: 3, Loss: 0.185781
Train - Epoch 100, Batch: 4, Loss: 0.212984
Train - Epoch 100, Batch: 5, Loss: 0.218490
Train - Epoch 100, Batch: 6, Loss: 0.224942
Train - Epoch 100, Batch: 7, Loss: 0.207742
Train - Epoch 100, Batch: 8, Loss: 0.207214
Train - Epoch 100, Batch: 9, Loss: 0.190116
Train - Epoch 100, Batch: 10, Loss: 0.223480
Train - Epoch 100, Batch: 11, Loss: 0.220981
Train - Epoch 200, Batch: 0, Loss: 0.186070
Train - Epoch 200, Batch: 1, Loss: 0.173805
Train - Epoch 200, Batch: 2, Loss: 0.160556
Train - Epoch 200, Batch: 3, Loss: 0.187147
Train - Epoch 200, Batch: 4, Loss: 0.180125
Train - Epoch 200, Batch: 5, Loss: 0.173751
Train - Epoch 200, Batch: 6, Loss: 0.191680
Train - Epoch 200, Batch: 7, Loss: 0.174594
Train - Epoch 200, Batch: 8, Loss: 0.179576
Train - Epoch 200, Batch: 9, Loss: 0.154289
Train - Epoch 200, Batch: 10, Loss: 0.180450
Train - Epoch 200, Batch: 11, Loss: 0.165928
Train - Epoch 300, Batch: 0, Loss: 0.163994
Train - Epoch 300, Batch: 1, Loss: 0.157898
Train - Epoch 300, Batch: 2, Loss: 0.157557
Train - Epoch 300, Batch: 3, Loss: 0.163049
Train - Epoch 300, Batch: 4, Loss: 0.166890
Train - Epoch 300, Batch: 5, Loss: 0.150189
Train - Epoch 300, Batch: 6, Loss: 0.171357
Train - Epoch 300, Batch: 7, Loss: 0.172147
Train - Epoch 300, Batch: 8, Loss: 0.163332
Train - Epoch 300, Batch: 9, Loss: 0.148903
Train - Epoch 300, Batch: 10, Loss: 0.167355
Train - Epoch 300, Batch: 11, Loss: 0.164314
Train - Epoch 400, Batch: 0, Loss: 0.173244
Train - Epoch 400, Batch: 1, Loss: 0.143577
Train - Epoch 400, Batch: 2, Loss: 0.160296
Train - Epoch 400, Batch: 3, Loss: 0.143836
Train - Epoch 400, Batch: 4, Loss: 0.147188
Train - Epoch 400, Batch: 5, Loss: 0.164635
Train - Epoch 400, Batch: 6, Loss: 0.173777
Train - Epoch 400, Batch: 7, Loss: 0.152646
Train - Epoch 400, Batch: 8, Loss: 0.148964
Train - Epoch 400, Batch: 9, Loss: 0.147931
Train - Epoch 400, Batch: 10, Loss: 0.156793
Train - Epoch 400, Batch: 11, Loss: 0.144083
Train - Epoch 500, Batch: 0, Loss: 0.143951
Train - Epoch 500, Batch: 1, Loss: 0.135777
Train - Epoch 500, Batch: 2, Loss: 0.150165
Train - Epoch 500, Batch: 3, Loss: 0.149075
Train - Epoch 500, Batch: 4, Loss: 0.160102
Train - Epoch 500, Batch: 5, Loss: 0.154553
Train - Epoch 500, Batch: 6, Loss: 0.147439
Train - Epoch 500, Batch: 7, Loss: 0.163022
Train - Epoch 500, Batch: 8, Loss: 0.130619
Train - Epoch 500, Batch: 9, Loss: 0.166710
Train - Epoch 500, Batch: 10, Loss: 0.157058
Train - Epoch 500, Batch: 11, Loss: 0.150301
Train - Epoch 600, Batch: 0, Loss: 0.154499
Train - Epoch 600, Batch: 1, Loss: 0.149178
Train - Epoch 600, Batch: 2, Loss: 0.142031
Train - Epoch 600, Batch: 3, Loss: 0.162711
Train - Epoch 600, Batch: 4, Loss: 0.157243
Train - Epoch 600, Batch: 5, Loss: 0.139721
Train - Epoch 600, Batch: 6, Loss: 0.157663
Train - Epoch 600, Batch: 7, Loss: 0.149176
Train - Epoch 600, Batch: 8, Loss: 0.135162
Train - Epoch 600, Batch: 9, Loss: 0.132825
Train - Epoch 600, Batch: 10, Loss: 0.141485
Train - Epoch 600, Batch: 11, Loss: 0.156921
Train - Epoch 700, Batch: 0, Loss: 0.140386
Train - Epoch 700, Batch: 1, Loss: 0.150804
Train - Epoch 700, Batch: 2, Loss: 0.152736
Train - Epoch 700, Batch: 3, Loss: 0.148117
Train - Epoch 700, Batch: 4, Loss: 0.122409
Train - Epoch 700, Batch: 5, Loss: 0.155529
Train - Epoch 700, Batch: 6, Loss: 0.128902
Train - Epoch 700, Batch: 7, Loss: 0.148450
Train - Epoch 700, Batch: 8, Loss: 0.155879
Train - Epoch 700, Batch: 9, Loss: 0.150703
Train - Epoch 700, Batch: 10, Loss: 0.150223
Train - Epoch 700, Batch: 11, Loss: 0.151145
Train - Epoch 800, Batch: 0, Loss: 0.157707
Train - Epoch 800, Batch: 1, Loss: 0.146611
Train - Epoch 800, Batch: 2, Loss: 0.144243
Train - Epoch 800, Batch: 3, Loss: 0.124582
Train - Epoch 800, Batch: 4, Loss: 0.139440
Train - Epoch 800, Batch: 5, Loss: 0.152976
Train - Epoch 800, Batch: 6, Loss: 0.159948
Train - Epoch 800, Batch: 7, Loss: 0.127193
Train - Epoch 800, Batch: 8, Loss: 0.134529
Train - Epoch 800, Batch: 9, Loss: 0.166191
Train - Epoch 800, Batch: 10, Loss: 0.141160
Train - Epoch 800, Batch: 11, Loss: 0.143228
Train - Epoch 900, Batch: 0, Loss: 0.144855
Train - Epoch 900, Batch: 1, Loss: 0.145902
Train - Epoch 900, Batch: 2, Loss: 0.144487
Train - Epoch 900, Batch: 3, Loss: 0.137028
Train - Epoch 900, Batch: 4, Loss: 0.166014
Train - Epoch 900, Batch: 5, Loss: 0.141738
Train - Epoch 900, Batch: 6, Loss: 0.141998
Train - Epoch 900, Batch: 7, Loss: 0.140236
Train - Epoch 900, Batch: 8, Loss: 0.124310
Train - Epoch 900, Batch: 9, Loss: 0.149152
Train - Epoch 900, Batch: 10, Loss: 0.144212
Train - Epoch 900, Batch: 11, Loss: 0.148799
training_time:: 11.650819063186646
training time full:: 11.650978565216064
provenance prepare time:: 2.384185791015625e-07
here
Test Avg. Loss: 0.000122, Accuracy: 0.968750, F1 Score: 0.968205
</Training>
torch.Size([11982, 784])
tensor([3323, 5846, 6329,  ..., 5145,  604, 4193])
<results lr="1" epochs="1000" bz="1024" remove_ratio="0.375" sampling_type="targeted_informed">
<Baseline>
data dimension:: [11982, 784]
tensor([ 2053,    11,  6156,  4111,  2066,  4114,  6165,    21,  6166,  2072,
         4122, 10270,  8235,  4141,  6192,  8241,  4146, 10295,  6206,  8257,
        10309, 10311,  2133, 10326,  4191,  6246,  2155,  2157,   110,   111,
         6260, 10357,  4219,  4223, 10367,   130,  2179, 10370,   134,   135,
        10376,  2186,  6283,  8332,  4241,  4242,   148,  6292,  6297,  4251])
Epoch:0 Batch: 11 Baseline Loss 0.5543769163933744
Epoch:100 Batch: 11 Baseline Loss 0.17908965467184104
Epoch:200 Batch: 11 Baseline Loss 0.13842891880980018
Epoch:300 Batch: 11 Baseline Loss 0.12087488700638635
Epoch:400 Batch: 11 Baseline Loss 0.10995562005088884
Epoch:500 Batch: 11 Baseline Loss 0.09975740446488818
Epoch:600 Batch: 11 Baseline Loss 0.11434941100771592
Epoch:700 Batch: 11 Baseline Loss 0.09494615640152089
Epoch:800 Batch: 11 Baseline Loss 0.09261465297107406
Epoch:900 Batch: 11 Baseline Loss 0.0876237751605003
training time is 19.86672353744507
overhead:: 0
overhead2:: 3.0856032371520996
overhead3:: 0
memory usage:: 822562816
time_baseline:: 19.883385181427002
Test Avg. Loss: 0.000344, Accuracy: 0.872480, F1 Score: 0.884316
model difference (l2 norm): tensor(14.2080, dtype=torch.float64)
Test Avg. Loss: 0.000344, Accuracy: 0.872480, F1 Score: 0.884316
Remove Test Avg. Loss: 0.000820, Accuracy: 0.662141, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000353, Accuracy: 0.859456, F1 Score: 0.873269
</Baseline>
<Deltagrad period="2">
data dimension:: [11982, 784]
overhead2:: 3.194705009460449
overhead3:: 4.470787048339844
overhead4:: 3.2778117656707764
overhead5:: 0
memory usage:: 1090396160
time_deltagrad:: 21.906747102737427
model difference (l2 norm): tensor(1.4327, dtype=torch.float64)
Test Avg. Loss: 0.000319, Accuracy: 0.885081, F1 Score: 0.894542
Remove Test Avg. Loss: 0.000762, Accuracy: 0.683063, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000331, Accuracy: 0.867802, F1 Score: 0.879964
</Deltagrad>
<Deltagrad period="5">
data dimension:: [11982, 784]
overhead2:: 1.37058424949646
overhead3:: 2.41843318939209
overhead4:: 1.3883590698242188
overhead5:: 0
memory usage:: 1090379776
time_deltagrad:: 18.459284782409668
model difference (l2 norm): tensor(2.5921, dtype=torch.float64)
Test Avg. Loss: 0.000295, Accuracy: 0.892137, F1 Score: 0.900372
Remove Test Avg. Loss: 0.000703, Accuracy: 0.706877, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000310, Accuracy: 0.876398, F1 Score: 0.886903
</Deltagrad>
<Deltagrad period="10">
data dimension:: [11982, 784]
overhead2:: 0.6459910869598389
overhead3:: 1.5871214866638184
overhead4:: 0.8330099582672119
overhead5:: 0
memory usage:: 1090396160
time_deltagrad:: 16.38384461402893
model difference (l2 norm): tensor(2.8996, dtype=torch.float64)
Test Avg. Loss: 0.000281, Accuracy: 0.899698, F1 Score: 0.906704
Remove Test Avg. Loss: 0.000668, Accuracy: 0.722680, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000297, Accuracy: 0.882741, F1 Score: 0.892064
</Deltagrad>
<Deltagrad period="20">
data dimension:: [11982, 784]
overhead2:: 0.33939170837402344
overhead3:: 1.2286286354064941
overhead4:: 0.4633219242095947
overhead5:: 0
memory usage:: 1090404352
time_deltagrad:: 16.071176528930664
model difference (l2 norm): tensor(2.8850, dtype=torch.float64)
Test Avg. Loss: 0.000284, Accuracy: 0.896673, F1 Score: 0.904161
Remove Test Avg. Loss: 0.000674, Accuracy: 0.719786, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000299, Accuracy: 0.881489, F1 Score: 0.891004
</Deltagrad>
<Deltagrad period="50">
data dimension:: [11982, 784]
overhead2:: 0.17508816719055176
overhead3:: 1.0221529006958008
overhead4:: 0.21024394035339355
overhead5:: 0
memory usage:: 1090416640
time_deltagrad:: 15.370920658111572
model difference (l2 norm): tensor(3.3152, dtype=torch.float64)
Test Avg. Loss: 0.000279, Accuracy: 0.899698, F1 Score: 0.906704
Remove Test Avg. Loss: 0.000661, Accuracy: 0.730024, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000294, Accuracy: 0.884994, F1 Score: 0.893853
</Deltagrad>
<Deltagrad period="75">
data dimension:: [11982, 784]
overhead2:: 0.09270906448364258
overhead3:: 0.9217612743377686
overhead4:: 0.131392240524292
overhead5:: 0
memory usage:: 1090383872
time_deltagrad:: 15.066195726394653
model difference (l2 norm): tensor(3.8620, dtype=torch.float64)
Test Avg. Loss: 0.000285, Accuracy: 0.898185, F1 Score: 0.905431
Remove Test Avg. Loss: 0.000673, Accuracy: 0.727131, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000298, Accuracy: 0.884243, F1 Score: 0.893332
</Deltagrad>
<Deltagrad period="100">
data dimension:: [11982, 784]
overhead2:: 0.07119989395141602
overhead3:: 0.894244909286499
overhead4:: 0.10753989219665527
overhead5:: 0
memory usage:: 1090400256
time_deltagrad:: 15.031922340393066
model difference (l2 norm): tensor(3.4199, dtype=torch.float64)
Test Avg. Loss: 0.000289, Accuracy: 0.893649, F1 Score: 0.901632
Remove Test Avg. Loss: 0.000686, Accuracy: 0.715558, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000304, Accuracy: 0.879236, F1 Score: 0.889042
</Deltagrad>
<Deltagrad period="200">
data dimension:: [11982, 784]
overhead2:: 0.038301706314086914
overhead3:: 0.8578011989593506
overhead4:: 0.04846334457397461
overhead5:: 0
memory usage:: 1090383872
time_deltagrad:: 14.685020208358765
model difference (l2 norm): tensor(3.8435, dtype=torch.float64)
Test Avg. Loss: 0.000297, Accuracy: 0.891129, F1 Score: 0.899535
Remove Test Avg. Loss: 0.000708, Accuracy: 0.710216, F1 Score: 0.000000
Remain Test Avg. Loss: 0.000311, Accuracy: 0.877733, F1 Score: 0.888023
</Deltagrad>
</results>
</data>
